{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM3UnzFc3UEwGeb1irsRlI0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Clustering Assignment**\n","\n","**Clustering Theory Questions: Answers\n","\n","```\n","# This is formatted as code\n","```\n","\n","**"],"metadata":{"id":"p34yAu0fzVpv"}},{"cell_type":"markdown","source":["\n","1. What is unsupervised learning in the context of machine learning?\n","   \n","   -->\n","\n","   Unsupervised learning is a branch of machine learning where the algorithm learns patterns and structures from unlabeled data. Unlike supervised learning, there are no predefined output labels or target variables. The goal is to discover inherent groupings, relationships, or representations within the data without human guidance on what the output should be. Common tasks include clustering, dimensionality reduction, and anomaly detection.\n","\n","2. How does K-Means clustering algorithm work?\n","   \n","   -->\n","\n","   K-Means clustering is an iterative, centroid-based clustering algorithm that aims to partition 'n' observations into 'k' clusters, where each observation belongs to the cluster with the nearest mean (centroid). Here's how it works:\n","\n","Initialization: Randomly select 'k' data points from the dataset as initial cluster centroids.\n","\n","Assignment Step (E-step): Each data point is assigned to the cluster whose centroid is closest to it (e.g., using Euclidean distance).\n","\n","Update Step (M-step): The centroids of the clusters are re-calculated by taking the mean of all data points assigned to that cluster.\n","\n","Iteration: Steps 2 and 3 are repeated until the cluster assignments no longer change or a maximum number of iterations is reached, indicating convergence.\n","\n","3. Explain the concept of a dendrogram in hierarchical clustering.\n","  \n","   -->\n","\n","   A dendrogram is a tree-like diagram that visually represents the sequence of merges or splits in hierarchical clustering.\n","   \n","   Agglomerative Clustering (bottom-up): A dendrogram for agglomerative clustering starts with each data point as its own cluster at the bottom. As the algorithm proceeds, individual points and then clusters are successively merged. The height of the 'U'-shaped link in the dendrogram indicates the dissimilarity (or distance) at which two clusters were merged. The longer the vertical line, the greater the dissimilarity between the merged clusters.\n","   \n","   Divisive Clustering (top-down): A dendrogram for divisive clustering starts with all data points in one large cluster at the top, which is then successively split into smaller clusters.\n","   \n","   Dendrograms are used to determine the optimal number of clusters by visually inspecting where a \"cut\" across the dendrogram would produce meaningful groups, often by looking for large vertical gaps.\n","\n","4. What is the main difference between K-Means and Hierarchical Clustering?\n","   \n","   -->\n","\n","   The main differences between K-Means and Hierarchical Clustering are: Approach:\n","   \n","   K-Means: A partitioning method that aims to partition data into a pre-defined number of 'k' clusters. It's iterative and optimizes an objective function (minimizing within-cluster sum of squares).\n","   \n","   Hierarchical Clustering: Builds a hierarchy of clusters. It can be agglomerative (bottom-up, merging clusters) or divisive (top-down, splitting clusters). It does not require 'k' to be specified beforehand, but 'k' is often chosen by cutting the dendrogram.\n","\n","   Number of Clusters (k):\n","\n","   K-Means: Requires the number of clusters 'k' to be specified before running the algorithm.\n","\n","   Hierarchical Clustering: Does not explicitly require 'k' beforehand; it produces a hierarchy, and 'k' can be chosen afterward by cutting the dendrogram at a certain dissimilarity level.\n","\n","   Output:\n","\n","   K-Means: Produces a single set of 'k' clusters.\n","\n","   Hierarchical Clustering: Produces a hierarchy (dendrogram) that shows how clusters are nested at various levels of granularity.\n","\n","   Computational Cost:\n","\n","   K-Means: Generally faster for large datasets (O(n⋅k⋅i⋅d) where i is iterations, d is dimensions).\n","\n","   Hierarchical Clustering: Can be computationally more expensive, especially for agglomerative clustering (O(n\n","2\n"," ⋅d) to O(n\n","3\n"," ) depending on linkage, where d is dimensions), making it less suitable for very large datasets.\n","\n","5. What are the advantages of DBSCAN over K-Means?\n","   \n","   -->\n","\n","   DBSCAN (Density-Based Spatial Clustering of Applications with Noise) offers several advantages over K-Means:\n","   \n","   Handles Arbitrary Shapes: DBSCAN can discover clusters of arbitrary shapes (e.g., non-spherical, complex structures) because it's based on density, unlike K-Means which assumes spherical clusters.\n","   \n","   Identifies Noise/Outliers: DBSCAN can explicitly identify \"noise\" points (outliers) that do not belong to any cluster. K-Means forces every data point into a cluster, even outliers.\n","   \n","   Does Not Require 'k': DBSCAN does not require the number of clusters 'k' to be specified beforehand. It discovers the number of clusters based on the density parameters.\n","   \n","   Robust to Noise: By explicitly modeling noise, DBSCAN is more robust to the presence of outliers in the data.\n","\n","6. When would you use Silhouette Score in clustering?\n","   \n","   -->\n","\n","   The Silhouette Score (or Silhouette Coefficient) is used to evaluate the quality of clustering results, particularly when the true labels are unknown (unsupervised evaluation). You would use it:\n","\n","To Determine Optimal 'k': When trying to find the best number of clusters (e.g., for K-Means or hierarchical clustering), you can calculate the Silhouette Score for different values of 'k' and choose the 'k' that yields the highest score.\n","\n","To Compare Different Clustering Algorithms: When comparing the performance of different clustering algorithms on the same dataset, the algorithm that produces clusters with a higher Silhouette Score is generally considered better.\n","\n","To Assess Cluster Cohesion and Separation: A high Silhouette Score indicates that objects are well-matched to their own cluster (high cohesion) and poorly matched to neighboring clusters (high separation).\n","\n","Score range: −1 to 1.\n","\n","Near +1: Data point is far away from the neighboring clusters.\n","\n","Near 0: Data point is on or very close to the decision boundary between two clusters.\n","\n","Near -1: Data point is likely assigned to the wrong cluster.\n","\n","7. What are the limitations of Hierarchical Clustering?\n","   \n","   -->\n","\n","   Despite its flexibility, Hierarchical Clustering has several limitations:\n","\n","Computational Complexity: For large datasets, its time complexity can be very high, typically O(n\n","2\n"," ⋅logn) or O(n\n","3\n"," ) depending on the linkage method, making it impractical for millions of data points.\n","\n","Space Complexity: It requires storing the dissimilarity matrix (O(n\n","2\n"," )), which can be memory-intensive for large datasets.\n","\n","Irrevocable Decisions: Once a merge or split is made, it cannot be undone. This can lead to suboptimal clusters if an early decision was poor.\n","\n","Sensitivity to Noise/Outliers: Especially with certain linkage criteria (like single linkage), hierarchical clustering can be very sensitive to noise and outliers, leading to \"chaining\" effects where clusters are incorrectly merged due to single close points.\n","\n","Difficulty with Non-Globular Shapes: Like K-Means, it can struggle with clusters that are non-globular or have varying densities, although some linkage methods can mitigate this.\n","\n","8. Why is feature scaling important in clustering algorithms like K-Means?\n","   \n","   -->\n","\n","   Feature scaling is crucial in clustering algorithms like K-Means because these algorithms typically use distance metrics (e.g., Euclidean distance) to determine the similarity or dissimilarity between data points.\n","\n","If features have different scales (e.g., one feature ranges from 0-1000 and another from 0-1), the feature with the larger range will disproportionately influence the distance calculations. This means:\n","\n","Dominance of Large-Scale Features: The clustering algorithm will implicitly give more weight to features with larger numerical ranges, even if they are not inherently more important.\n","\n","Distorted Distances: The calculated distances between points will be heavily skewed by the feature with the largest values, leading to inaccurate groupings and suboptimal clusters.\n","\n","Scaling ensures that all features contribute equally to the distance calculations, preventing features with larger numerical values from dominating the clustering process and leading to more meaningful and accurate clusters. Common scaling methods include Standardization (Z-score normalization) and Min-Max Scaling.\n","\n","9. How does DBSCAN identify noise points?\n","   \n","   -->\n","\n","   DBSCAN identifies noise points (outliers) based on its core concept of density reachability:\n","\n","Core Points: A data point is a \"core point\" if there are at least min_samples (a parameter, minimum number of points) within its eps (epsilon, a radius) neighborhood.\n","\n","Border Points: A data point is a \"border point\" if it is within the eps neighborhood of a core point but is not a core point itself (i.e., it has fewer than min_samples within its own eps neighborhood).\n","\n","Noise Points (Outliers): Any data point that is neither a core point nor a border point is considered a noise point. These are points that lie in low-density regions and are too far from any core point to be part of a cluster.\n","\n","By explicitly categorizing points as core, border, or noise, DBSCAN effectively separates outliers from dense clusters, which is a significant advantage over partition-based methods like K-Means.\n","\n","10. Define inertia in the context of K-Means.\n","    \n","   -->\n","\n","   Inertia (also known as \"within-cluster sum of squares\" or WCSS) in the context of K-Means clustering is a measure of how internally coherent clusters are. It is defined as:\n","\n","Inertia=\n","i=0\n","∑\n","n−1\n","​\n","  \n","μ\n","j\n","​\n"," ∈C\n","min\n","​\n"," (∣∣x\n","i\n","​\n"," −μ\n","j\n","​\n"," ∣∣\n","2\n"," )\n","Where:\n","\n","x\n","i\n","​\n","  is a data point.\n","\n","μ\n","j\n","​\n","  is the centroid of cluster j.\n","\n","C is the set of all cluster centroids.\n","\n","∣∣x\n","i\n","​\n"," −μ\n","j\n","​\n"," ∣∣\n","2\n","  is the squared Euclidean distance between data point x\n","i\n","​\n","  and the centroid μ\n","j\n","​\n","  of the cluster it is assigned to.\n","\n","The goal of the K-Means algorithm is to minimize inertia. A lower inertia value generally indicates better clustering, as it means data points are closer to their respective cluster centroids. However, inertia always decreases as 'k' increases, so it cannot be used as the sole metric to determine the optimal number of clusters.\n","\n","11. What is the elbow method in K-Means clustering?\n","    \n","   -->\n","\n","   The elbow method is a heuristic used to determine an optimal number of clusters (k) for K-Means clustering. It involves:\n","\n","Run K-Means for various 'k': Perform K-Means clustering for a range of k values (e.g., from 1 to 10).\n","\n","Calculate Inertia: For each k, calculate the inertia (WCSS - Within-Cluster Sum of Squares).\n","\n","Plot Inertia vs. 'k': Plot the inertia values on the y-axis against the number of clusters (k) on the x-axis.\n","\n","Find the \"Elbow\": Look for an \"elbow\" point on the plot. This is the point where the rate of decrease in inertia sharply changes, forming an \"elbow\" shape. The k value at this elbow is often considered the optimal number of clusters, as adding more clusters beyond this point provides diminishing returns in terms of reducing within-cluster variance.\n","\n","12. Describe the concept of \"density\" in DBSCAN.\n","    \n","   -->\n","\n","   In DBSCAN, \"density\" is a fundamental concept used to define clusters and identify noise. It's not a global measure but is defined locally around each data point using two parameters:\n","\n","ϵ (epsilon): This defines the maximum radius of the neighborhood around a data point. If eps is too small, many points might be labeled as noise; if too large, distinct clusters might merge.\n","\n","MinPts (Minimum Points): This defines the minimum number of data points required within the $\\epsilon$-neighborhood of a point for that point to be considered a \"dense\" region (a core point). If MinPts is too small, noise points might form small clusters; if too large, sparse clusters might be missed.\n","\n","A cluster in DBSCAN is then defined as a dense region in the data space that is reachable from a core point through a chain of other core points. Points that are not part of any such dense region are classified as noise.\n","\n","13. Can hierarchical clustering be used on categorical data?\n","    \n","   -->\n","\n","   Yes, hierarchical clustering can be used on categorical data, but it requires appropriate handling and distance metrics. Standard distance metrics like Euclidean distance are not suitable for categorical data.\n","\n","To use hierarchical clustering with categorical data:\n","\n","Encoding: Categorical data usually needs to be converted into a numerical format.\n","\n","One-Hot Encoding: Converts each category into a binary (0 or 1) feature. This can lead to very high-dimensional sparse data, which can be problematic for distance calculations and memory usage.\n","\n","Ordinal Encoding: If there's an inherent order, categories can be mapped to integers.\n","\n","Distance Metrics: Use distance metrics appropriate for categorical data:\n","\n","Hamming Distance: For binary data (like after one-hot encoding), it counts the number of positions at which the corresponding symbols are different.\n","\n","Gower Distance: A general distance metric that can handle mixed data types (numerical and categorical) by calculating a weighted average of individual attribute distances.\n","\n","While possible, the choice of encoding and distance metric significantly impacts the quality of the clustering.\n","\n","14. What does a negative Silhouette Score indicate?\n","    \n","   -->\n","\n","   A negative Silhouette Score indicates that a data point might be assigned to the wrong cluster. Specifically:\n","\n","For a data point i, the Silhouette Score s(i) is calculated as:\n","s(i)=\n","max(a(i),b(i))\n","b(i)−a(i)\n","​\n","\n","where:\n","\n","a(i) is the average distance from i to all other points in the same cluster. (Measures cohesion)\n","\n","b(i) is the minimum average distance from i to all points in a different cluster (the nearest neighboring cluster). (Measures separation)\n","\n","If s(i) is negative (b(i)−a(i)<0, meaning b(i)<a(i)): This implies that the average distance from data point i to points in its own cluster (a(i)) is greater than the minimum average distance from i to points in a neighboring cluster (b(i)). In essence, the point is, on average, closer to points in a different cluster than to points in its assigned cluster. This suggests a poor or incorrect clustering assignment for that point.\n","\n","15. Explain the term \"linkage criteria\" in hierarchical clustering.\n","    \n","   -->\n","\n","   In agglomerative hierarchical clustering, \"linkage criteria\" (also known as linkage methods or merging strategies) determine how the distance between two clusters is calculated when deciding which clusters to merge at each step. This distance is not just between individual points but between groups of points.\n","\n","Common linkage criteria include:\n","\n","Single Linkage: The distance between two clusters is the minimum distance between any single point in one cluster and any single point in the other cluster. Prone to \"chaining\" effect.\n","\n","Complete Linkage: The distance between two clusters is the maximum distance between any single point in one cluster and any single point in the other cluster. Tends to produce more compact, spherical clusters.\n","\n","Average Linkage: The distance between two clusters is the average distance between all pairs of points, where one point is from each cluster.\n","\n","Ward's Linkage: Calculates the increase in the within-cluster sum of squares (variance) when two clusters are merged. It merges clusters that result in the minimum increase in total within-cluster variance. Tends to produce well-balanced clusters of similar size.\n","\n","The choice of linkage criteria significantly influences the shape and structure of the resulting clusters and the appearance of the dendrogram.\n","\n","16. Why might K-Means clustering perform poorly on data with varying cluster sizes or densities?\n","    \n","   -->\n","\n","   K-Means clustering often performs poorly on data with varying cluster sizes or densities due to its fundamental assumptions and optimization objective:\n","\n","Assumption of Spherical/Globular Clusters: K-Means defines clusters based on centroids and assumes that clusters are convex and isotropic (i.e., spherical or globular). When clusters are elongated, crescent-shaped, or have other arbitrary forms, K-Means struggles to correctly identify them.\n","\n","Assumption of Similar Density: K-Means tries to minimize the within-cluster sum of squares (inertia). This objective implicitly pushes it to create clusters of roughly similar densities. If one cluster is very dense and another is sparse, K-Means might split the dense cluster or merge parts of the sparse one, creating suboptimal divisions.\n","\n","Sensitivity to Centroid Initialization: For varying densities, a centroid might get \"pulled\" into a denser region even if it's not the true center of the intended cluster, leading to misassignments.\n","\n","Fixed Number of Clusters (k): The need to pre-specify k means the algorithm doesn't adapt to the natural varying sizes/densities that might suggest a different number of inherent groups.\n","\n","Algorithms like DBSCAN (density-based) or hierarchical clustering with appropriate linkage can be more suitable for such data.\n","\n","17. What are the core parameters in DBSCAN, and how do they influence clustering?\n","    \n","   -->\n","\n","   The two core parameters in DBSCAN are:\n","\n","ϵ (epsilon or eps): This defines the maximum distance between two samples for one to be considered as in the neighborhood of the other. It determines the radius of the neighborhood to consider around each point.\n","\n","Influence:\n","\n","Small eps: Can lead to many points being labeled as noise and potentially splitting true clusters into smaller ones.\n","\n","Large eps: Can cause multiple distinct clusters to merge into a single large cluster.\n","\n","MinPts (Minimum Points or min_samples): This defines the minimum number of data points required within an $\\epsilon$-neighborhood for a point to be considered a \"core point\" (i.e., part of a dense region).\n","\n","Influence:\n","\n","Small MinPts: Can lead to \"noisy\" clusters, where even small, sparse groupings are considered clusters.\n","\n","Large MinPts: Can cause more points to be labeled as noise and potentially miss less dense but valid clusters.\n","\n","Choosing optimal eps and MinPts is crucial and often involves domain knowledge, visual inspection (e.g., using a k-distance graph), or trial and error.\n","\n","18. How does K-Means++ improve upon standard K-Means initialization?\n","    \n","   -->\n","\n","   K-Means++ is an initialization algorithm for K-Means that improves upon the standard (random) K-Means initialization by selecting initial centroids in a way that aims to be \"smarter\" and spread them out across the data. This helps in:\n","\n","Faster Convergence: By selecting initial centroids that are already somewhat representative and well-separated, K-Means++ often leads to fewer iterations for the K-Means algorithm to converge.\n","\n","Better Quality Clusters: It significantly reduces the chances of converging to a poor local optimum (suboptimal clustering) that can occur with purely random initialization, especially when clusters are well-separated or have varying densities.\n","\n","How K-Means++ works:\n","\n","Choose the first centroid uniformly at random from the data points.\n","\n","For each remaining data point, calculate its distance to the closest centroid already chosen.\n","\n","Choose the next centroid from the remaining data points with a probability proportional to the squared distance to the closest existing centroid. This means points far from existing centroids are more likely to be chosen as new centroids.\n","\n","Repeat steps 2 and 3 until k centroids have been chosen.\n","\n","19. What is agglomerative clustering?\n","    \n","   -->\n","\n","   Agglomerative clustering is a \"bottom-up\" approach to hierarchical clustering. It starts with each data point as its own individual cluster. Then, it iteratively merges the closest pairs of clusters until all data points are in a single large cluster, or a stopping criterion (e.g., a specific number of clusters k) is met.\n","\n","The \"closeness\" or \"distance\" between clusters is determined by a linkage criterion (e.g., single, complete, average, or Ward's linkage), which defines how the distance between two groups of points is calculated. The result of agglomerative clustering is a dendrogram, which visually represents the merging process and the hierarchy of clusters.\n","\n","20. What makes Silhouette Score a better metric than just inertia for model evaluation?\n","    \n","   -->\n","\n","   While inertia (WCSS) is the objective function that K-Means directly tries to minimize, the Silhouette Score is often considered a better metric for evaluating the quality of clustering results because:\n","\n","Considers Both Cohesion and Separation:\n","\n","Inertia: Only measures the cohesion of clusters (how close points are to their own centroid). It always decreases as the number of clusters (k) increases, even when adding more clusters doesn't make logical sense, making it a poor sole indicator for optimal k.\n","\n","Silhouette Score: Measures both the cohesion (how similar a point is to its own cluster) and the separation (how dissimilar it is to other clusters). It provides a measure of how well-defined and separated the clusters are.\n","\n","Provides Intuitive Interpretation: A high Silhouette Score (closer to 1) indicates that points are well-matched to their own cluster and well-separated from neighboring clusters. A low score (near 0) suggests overlapping clusters, and a negative score suggests misclassified points. This makes it more intuitive for interpreting cluster quality.\n","\n","Better for Optimal 'k' Selection: Because it balances cohesion and separation, the Silhouette Score often peaks at a k that represents a more natural and meaningful clustering, unlike inertia which continuously decreases. This makes it more effective for methods like the \"elbow method\" for k selection."],"metadata":{"id":"DvQzHnXe0Ug4"}},{"cell_type":"markdown","source":["**Practical Questions // Answers**"],"metadata":{"id":"MxUAysXM3frP"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1QT0Y3JrXGjQX97YfMPPZBQ2d4LQg-DGF"},"id":"1y5LxT0-zSRh","executionInfo":{"status":"ok","timestamp":1750163775966,"user_tz":-330,"elapsed":58337,"user":{"displayName":"Ram Gopal Gupta","userId":"17103427756007212633"}},"outputId":"2e935324-9f4c-47cb-b75d-994cb836e73a"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# ==============================================================================\n","# Complete Python Code for Clustering Techniques Practical Questions (21-48)\n","# This code is designed to be run in a Google Colab environment.\n","# ==============================================================================\n","\n","# --- Essential Library Imports ---\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Scikit-learn imports for various models, datasets, and metrics\n","from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n","from sklearn.datasets import (\n","    make_blobs,\n","    make_moons,\n","    make_circles,\n","    load_iris,\n","    load_wine,\n","    load_breast_cancer,\n","    load_digits\n",")\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.metrics import silhouette_score, mean_squared_error, r2_score, confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","from scipy.cluster.hierarchy import dendrogram, linkage # For hierarchical clustering visualization\n","\n","\n","# To suppress warnings that might arise during execution\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","print(\"All necessary libraries imported successfully.\")\n","\n","# ==============================================================================\n","# Q21. Generate synthetic data with 4 centers using make_blobs and apply K-Means\n","#      clustering. Visualize using a scatter plot.\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"Q21. K-Means on make_blobs with 4 centers and visualization.\")\n","print(\"=\"*80)\n","\n","# 1. Generate synthetic data with 4 centers\n","X_q21, y_q21 = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n","\n","# 2. Apply K-Means clustering\n","kmeans_q21 = KMeans(n_clusters=4, random_state=0, n_init=10) # n_init for robust initialization\n","kmeans_q21.fit(X_q21)\n","labels_q21 = kmeans_q21.labels_\n","centroids_q21 = kmeans_q21.cluster_centers_\n","\n","# 3. Visualize using a scatter plot\n","plt.figure(figsize=(10, 7))\n","plt.scatter(X_q21[:, 0], X_q21[:, 1], c=labels_q21, cmap='viridis', s=50, alpha=0.8, label='Data Points')\n","plt.scatter(centroids_q21[:, 0], centroids_q21[:, 1], c='red', s=200, alpha=0.9, marker='X', label='Centroids')\n","plt.title('K-Means Clustering on Synthetic Blobs (4 Clusters)')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","print(\"K-Means clustering visualization with 4 centers displayed above.\\n\")\n","\n","\n","# ==============================================================================\n","# Q22. Load the Iris dataset and use Agglomerative Clustering to group the data\n","#      into 3 clusters. Display the first 10 predicted labels.\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"Q22. Agglomerative Clustering on Iris dataset and first 10 labels.\")\n","print(\"=\"*80)\n","\n","# 1. Load the Iris dataset\n","iris_q22 = load_iris()\n","X_q22 = iris_q22.data\n","y_q22_true = iris_q22.target # True labels for comparison, though not used in clustering\n","\n","# 2. Apply Agglomerative Clustering to group data into 3 clusters\n","# Using 'ward' linkage which is common for general purpose clustering\n","agg_clust_q22 = AgglomerativeClustering(n_clusters=3, linkage='ward')\n","labels_q22 = agg_clust_q22.fit_predict(X_q22)\n","\n","# 3. Display the first 10 predicted labels\n","print(f\"First 10 predicted cluster labels for Iris dataset: {labels_q22[:10]}\\n\")\n","\n","\n","# ==============================================================================\n","# Q23. Generate synthetic data using make_moons and apply DBSCAN. Highlight\n","#      outliers in the plot.\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"Q23. DBSCAN on make_moons with outlier highlighting.\")\n","print(\"=\"*80)\n","\n","# 1. Generate synthetic data using make_moons\n","X_q23, y_q23 = make_moons(n_samples=200, noise=0.05, random_state=0)\n","\n","# 2. Apply DBSCAN\n","# Optimal eps and min_samples often require tuning; these are common starting points\n","dbscan_q23 = DBSCAN(eps=0.3, min_samples=5) # Tune these parameters for best results\n","labels_q23 = dbscan_q23.fit_predict(X_q23)\n","\n","# 3. Highlight outliers in the plot\n","# Noise points are labeled as -1 by DBSCAN\n","core_samples_mask_q23 = np.zeros_like(labels_q23, dtype=bool)\n","core_samples_mask_q23[dbscan_q23.core_sample_indices_] = True\n","\n","n_clusters_q23 = len(set(labels_q23)) - (1 if -1 in labels_q23 else 0)\n","n_noise_q23 = list(labels_q23).count(-1)\n","\n","print(f'Estimated number of clusters: {n_clusters_q23}')\n","print(f'Estimated number of noise points: {n_noise_q23}')\n","\n","plt.figure(figsize=(10, 7))\n","unique_labels_q23 = set(labels_q23)\n","colors_q23 = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels_q23))]\n","\n","for k, col in zip(unique_labels_q23, colors_q23):\n","    if k == -1: # Black used for noise.\n","        col = [0, 0, 0, 1]\n","\n","    class_member_mask_q23 = (labels_q23 == k)\n","\n","    # Plot core points\n","    xy_q23 = X_q23[class_member_mask_q23 & core_samples_mask_q23]\n","    plt.plot(xy_q23[:, 0], xy_q23[:, 1], 'o', markerfacecolor=tuple(col),\n","             markeredgecolor='k', markersize=8)\n","\n","    # Plot non-core points (border points or noise)\n","    xy_q23 = X_q23[class_member_mask_q23 & ~core_samples_mask_q23]\n","    plt.plot(xy_q23[:, 0], xy_q23[:, 1], 'o', markerfacecolor=tuple(col),\n","             markeredgecolor='k', markersize=4)\n","\n","plt.title(f'DBSCAN on make_moons (Clusters: {n_clusters_q23}, Noise: {n_noise_q23})')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.show()\n","print(\"DBSCAN clustering visualization with outliers highlighted displayed above.\\n\")\n","\n","\n","# ==============================================================================\n","# Q24. Load the Wine dataset and apply K-Means clustering after standardizing\n","#      the features. Print the size of each cluster.\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"Q24. K-Means on Wine dataset with standardization and cluster sizes.\")\n","print(\"=\"*80)\n","\n","# 1. Load the Wine dataset\n","wine_q24 = load_wine()\n","X_q24 = wine_q24.data\n","y_q24_true = wine_q24.target # True labels (3 classes in Wine dataset)\n","\n","# 2. Standardize the features\n","scaler_q24 = StandardScaler()\n","X_scaled_q24 = scaler_q24.fit_transform(X_q24)\n","\n","# 3. Apply K-Means clustering (using k=3 based on true classes for demonstration)\n","kmeans_q24 = KMeans(n_clusters=3, random_state=42, n_init=10)\n","kmeans_q24.fit(X_scaled_q24)\n","labels_q24 = kmeans_q24.labels_\n","\n","# 4. Print the size of each cluster\n","unique_labels_q24, counts_q24 = np.unique(labels_q24, return_counts=True)\n","cluster_sizes_q24 = dict(zip(unique_labels_q24, counts_q24))\n","\n","print(\"Cluster sizes after K-Means on standardized Wine dataset:\")\n","for cluster_id, size in cluster_sizes_q24.items():\n","    print(f\"  Cluster {cluster_id}: {size} samples\")\n","print(\"\\n\")\n","\n","\n","# ==============================================================================\n","# Q25. Use make_circles to generate synthetic data and cluster it using DBSCAN.\n","#      Plot the result.\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"Q25. DBSCAN on make_circles data and visualization.\")\n","print(\"=\"*80)\n","\n","# 1. Use make_circles to generate synthetic data\n","X_q25, y_q25 = make_circles(n_samples=400, factor=0.5, noise=0.05, random_state=0)\n","\n","# 2. Cluster it using DBSCAN\n","# DBSCAN is excellent for concentric circles\n","dbscan_q25 = DBSCAN(eps=0.1, min_samples=10) # Tune these for best results\n","labels_q25 = dbscan_q25.fit_predict(X_q25)\n","\n","# 3. Plot the result\n","plt.figure(figsize=(10, 7))\n","# Color noise points (-1) differently if present\n","unique_labels_q25 = set(labels_q25)\n","colors_q25 = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels_q25))]\n","\n","for k, col in zip(unique_labels_q25, colors_q25):\n","    if k == -1: # Black for noise\n","        col = [0, 0, 0, 1]\n","    class_member_mask_q25 = (labels_q25 == k)\n","    xy_q25 = X_q25[class_member_mask_q25]\n","    plt.plot(xy_q25[:, 0], xy_q25[:, 1], 'o', markerfacecolor=tuple(col),\n","             markeredgecolor='k', markersize=6)\n","\n","n_clusters_q25 = len(set(labels_q25)) - (1 if -1 in labels_q25 else 0)\n","n_noise_q25 = list(labels_q25).count(-1)\n","\n","plt.title(f'DBSCAN Clustering on Concentric Circles (Clusters: {n_clusters_q25}, Noise: {n_noise_q25})')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.show()\n","print(\"DBSCAN clustering visualization on make_circles displayed above.\\n\")\n","\n","\n","# ==============================================================================\n","# Q26. Load the Breast Cancer dataset, apply MinMaxScaler, and use K-Means with\n","#      2 clusters. Output the cluster centroids.\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"Q26. K-Means on Breast Cancer with MinMaxScaler and centroids.\")\n","print(\"=\"*80)\n","\n","# 1. Load the Breast Cancer dataset\n","breast_cancer_q26 = load_breast_cancer()\n","X_q26 = breast_cancer_q26.data\n","y_q26_true = breast_cancer_q26.target # True labels (2 classes)\n","feature_names_q26 = breast_cancer_q26.feature_names\n","\n","# 2. Apply MinMaxScaler\n","scaler_q26 = MinMaxScaler()\n","X_scaled_q26 = scaler_q26.fit_transform(X_q26)\n","\n","# 3. Use K-Means with 2 clusters\n","kmeans_q26 = KMeans(n_clusters=2, random_state=42, n_init=10)\n","kmeans_q26.fit(X_scaled_q26)\n","\n","# 4. Output the cluster centroids\n","centroids_q26_scaled = kmeans_q26.cluster_centers_\n","\n","# It's often useful to inverse transform centroids to original scale for interpretability\n","centroids_q26_original_scale = scaler_q26.inverse_transform(centroids_q26_scaled)\n","\n","print(\"K-Means Cluster Centroids (Original Scale) for Breast Cancer dataset:\")\n","centroid_df_q26 = pd.DataFrame(centroids_q26_original_scale, columns=feature_names_q26)\n","print(centroid_df_q26)\n","print(\"\\n\")\n","\n","\n","# ==============================================================================\n","# Q27. Generate synthetic data using make_blobs with varying cluster standard\n","#      deviations and cluster with DBSCAN.\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"Q27. DBSCAN on make_blobs with varying standard deviations.\")\n","print(\"=\"*80)\n","\n","# 1. Generate synthetic data using make_blobs with varying cluster standard deviations\n","# make_blobs can take a list for cluster_std to create varying densities\n","X_q27, y_q27 = make_blobs(n_samples=500, centers=3, cluster_std=[0.5, 1.0, 0.2], random_state=42)\n","\n","# 2. Cluster with DBSCAN\n","# DBSCAN might struggle with very varying densities or require careful tuning of eps/min_samples\n","# to capture all clusters appropriately.\n","# For illustration, let's pick parameters that might show its behavior.\n","dbscan_q27 = DBSCAN(eps=0.5, min_samples=5) # Example parameters\n","labels_q27 = dbscan_q27.fit_predict(X_q27)\n","\n","n_clusters_q27 = len(set(labels_q27)) - (1 if -1 in labels_q27 else 0)\n","n_noise_q27 = list(labels_q27).count(-1)\n","\n","print(f'Estimated number of clusters by DBSCAN: {n_clusters_q27}')\n","print(f'Estimated number of noise points by DBSCAN: {n_noise_q27}')\n","\n","# Plotting the result\n","plt.figure(figsize=(10, 7))\n","unique_labels_q27 = set(labels_q27)\n","colors_q27 = [plt.cm.viridis(each) for each in np.linspace(0, 1, len(unique_labels_q27))]\n","\n","for k, col in zip(unique_labels_q27, colors_q27):\n","    if k == -1:\n","        # Black color for noise points\n","        col = [0, 0, 0, 1]\n","\n","    class_member_mask_q27 = (labels_q27 == k)\n","    xy_q27 = X_q27[class_member_mask_q27]\n","    plt.plot(xy_q27[:, 0], xy_q27[:, 1], 'o', markerfacecolor=tuple(col),\n","             markeredgecolor='k', markersize=6)\n","\n","plt.title('DBSCAN Clustering on Blobs with Varying Standard Deviations')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.show()\n","print(\"DBSCAN clustering visualization on make_blobs with varying std deviations displayed above.\\n\")\n","\n","\n","# ==============================================================================\n","# Q28. Load the Digits dataset, reduce it to 2D using PCA, and visualize\n","#      clusters from K-Means.\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"Q28. K-Means on Digits dataset (PCA-reduced to 2D) and visualization.\")\n","print(\"=\"*80)\n","\n","# 1. Load the Digits dataset\n","digits_q28 = load_digits()\n","X_q28 = digits_q28.data\n","y_q28_true = digits_q28.target # True labels (0-9, so 10 classes)\n","\n","# 2. Reduce it to 2D using PCA\n","pca_q28 = PCA(n_components=2, random_state=42)\n","X_pca_q28 = pca_q28.fit_transform(X_q28)\n","\n","# 3. Apply K-Means (10 clusters for digits 0-9)\n","kmeans_q28 = KMeans(n_clusters=10, random_state=42, n_init=10)\n","kmeans_q28.fit(X_pca_q28)\n","labels_q28 = kmeans_q28.labels_\n","centroids_q28 = kmeans_q28.cluster_centers_\n","\n","# 4. Visualize clusters from K-Means\n","plt.figure(figsize=(10, 7))\n","plt.scatter(X_pca_q28[:, 0], X_pca_q28[:, 1], c=labels_q28, cmap='tab10', s=20, alpha=0.8)\n","plt.scatter(centroids_q28[:, 0], centroids_q28[:, 1], c='black', s=100, marker='X', label='Centroids')\n","plt.title('K-Means Clustering on Digits Dataset (PCA-reduced to 2D)')\n","plt.xlabel('Principal Component 1')\n","plt.ylabel('Principal Component 2')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","print(\"K-Means clustering visualization on Digits dataset (PCA-reduced) displayed above.\\n\")\n","\n","\n","# ==============================================================================\n","# Q29. Create synthetic data using make_blobs and evaluate silhouette scores\n","#      for k=2 to 5. Display as a bar chart.\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"Q29. Silhouette Scores for K-Means (k=2 to 5) and bar chart.\")\n","print(\"=\"*80)\n","\n","# 1. Create synthetic data using make_blobs\n","X_q29, y_q29 = make_blobs(n_samples=300, centers=4, cluster_std=0.7, random_state=42)\n","\n","# 2. Evaluate silhouette scores for k=2 to 5\n","k_values_q29 = range(2, 6) # k=2, 3, 4, 5\n","silhouette_scores_q29 = []\n","\n","for k in k_values_q29:\n","    kmeans_q29 = KMeans(n_clusters=k, random_state=42, n_init=10)\n","    labels_q29 = kmeans_q29.fit_predict(X_q29)\n","    score = silhouette_score(X_q29, labels_q29)\n","    silhouette_scores_q29.append(score)\n","    print(f\"  K = {k}, Silhouette Score: {score:.4f}\")\n","\n","# 3. Display as a bar chart\n","plt.figure(figsize=(8, 6))\n","plt.bar(k_values_q29, silhouette_scores_q29, color='lightgreen')\n","plt.xlabel(\"Number of Clusters (k)\")\n","plt.ylabel(\"Silhouette Score\")\n","plt.title(\"Silhouette Scores for K-Means at different k values\")\n","plt.xticks(k_values_q29)\n","plt.ylim(0, 1) # Silhouette score ranges from -1 to 1\n","plt.grid(axis='y', linestyle='--')\n","plt.show()\n","print(\"Silhouette scores bar chart displayed above.\\n\")\n","\n","\n","# ==============================================================================\n","# Q30. Load the Iris dataset and use hierarchical clustering to group data.\n","#      Plot a dendrogram with average linkage.\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"Q30. Hierarchical Clustering on Iris and dendrogram with average linkage.\")\n","print(\"=\"*80)\n","\n","# 1. Load the Iris dataset\n","iris_q30 = load_iris()\n","X_q30 = iris_q30.data\n","\n","# 2. Use hierarchical clustering (we need the linkage matrix for dendrogram)\n","# 'average' linkage: distance between two clusters is the average distance between all pairs of observations.\n","Z_q30 = linkage(X_q30, method='average')\n","\n","# 3. Plot a dendrogram with average linkage\n","plt.figure(figsize=(15, 8))\n","dendrogram(Z_q30, leaf_rotation=90, leaf_font_size=8, labels=iris_q30.target_names[load_iris().target])\n","plt.title('Hierarchical Clustering Dendrogram (Average Linkage) on Iris Dataset')\n","plt.xlabel('Sample Index or (Cluster Size)')\n","plt.ylabel('Distance')\n","plt.show()\n","print(\"Dendrogram with average linkage for Iris dataset displayed above.\\n\")\n","\n","\n","# ==============================================================================\n","# Q31. Generate synthetic data with overlapping clusters using make_blobs, then\n","#      apply K-Means and visualize with decision boundaries.\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"Q31. K-Means on overlapping make_blobs with decision boundaries.\")\n","print(\"=\"*80)\n","\n","# 1. Generate synthetic data with overlapping clusters\n","X_q31, y_q31 = make_blobs(n_samples=300, centers=3, cluster_std=1.5, random_state=42) # Higher std for overlap\n","\n","# 2. Apply K-Means\n","kmeans_q31 = KMeans(n_clusters=3, random_state=42, n_init=10)\n","kmeans_q31.fit(X_q31)\n","labels_q31 = kmeans_q31.labels_\n","centroids_q31 = kmeans_q31.cluster_centers_\n","\n","# 3. Visualize with decision boundaries\n","# Create a meshgrid to plot decision boundaries\n","h_q31 = 0.02 # step size in the mesh\n","x_min, x_max = X_q31[:, 0].min() - 1, X_q31[:, 0].max() + 1\n","y_min, y_max = X_q31[:, 1].min() - 1, X_q31[:, 1].max() + 1\n","xx_q31, yy_q31 = np.meshgrid(np.arange(x_min, x_max, h_q31), np.arange(y_min, y_max, h_q31))\n","\n","# Predict cluster for each point in the meshgrid\n","Z_q31 = kmeans_q31.predict(np.c_[xx_q31.ravel(), yy_q31.ravel()])\n","Z_q31 = Z_q31.reshape(xx_q31.shape)\n","\n","plt.figure(figsize=(10, 7))\n","plt.imshow(Z_q31, interpolation='nearest',\n","           extent=(xx_q31.min(), xx_q31.max(), yy_q31.min(), yy_q31.max()),\n","           cmap=plt.cm.Paired, aspect='auto', origin='lower', alpha=0.8)\n","\n","plt.scatter(X_q31[:, 0], X_q31[:, 1], c=labels_q31, cmap='viridis', s=50, edgecolors='k', label='Data Points')\n","plt.scatter(centroids_q31[:, 0], centroids_q31[:, 1], marker='X', s=200, linewidths=3,\n","            color='red', edgecolors='black', zorder=10, label='Centroids')\n","plt.title('K-Means Clustering with Decision Boundaries on Overlapping Blobs')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","print(\"K-Means clustering visualization with decision boundaries on overlapping blobs displayed above.\\n\")\n","\n","\n","# ==============================================================================\n","# Q32. Load the Digits dataset and apply DBSCAN after reducing dimensions with\n","#      t-SNE. Visualize the results.\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"Q32. DBSCAN on Digits (t-SNE reduced) and visualization.\")\n","print(\"=\"*80)\n","\n","# 1. Load the Digits dataset\n","digits_q32 = load_digits()\n","X_q32 = digits_q32.data\n","y_q32_true = digits_q32.target # True labels (10 classes)\n","\n","# 2. Reduce dimensions with t-SNE\n","print(\"  Applying t-SNE (this might take a moment)...\")\n","tsne_q32 = TSNE(n_components=2, random_state=42)\n","X_tsne_q32 = tsne_q32.fit_transform(X_q32)\n","print(\"  t-SNE dimensionality reduction complete.\")\n","\n","# 3. Apply DBSCAN\n","# DBSCAN parameters need careful tuning for t-SNE output, as densities can vary\n","dbscan_q32 = DBSCAN(eps=2.5, min_samples=10) # Example parameters; tune for optimal results\n","labels_q32 = dbscan_q32.fit_predict(X_tsne_q32)\n","\n","n_clusters_q32 = len(set(labels_q32)) - (1 if -1 in labels_q32 else 0)\n","n_noise_q32 = list(labels_q32).count(-1)\n","\n","print(f'Estimated number of clusters by DBSCAN: {n_clusters_q32}')\n","print(f'Estimated number of noise points by DBSCAN: {n_noise_q32}')\n","\n","# 4. Visualize the results\n","plt.figure(figsize=(10, 7))\n","unique_labels_q32 = set(labels_q32)\n","colors_q32 = [plt.cm.get_cmap('tab10', len(unique_labels_q32))(i) for i in range(len(unique_labels_q32))]\n","\n","for k, col in zip(unique_labels_q32, colors_q32):\n","    if k == -1: # Black for noise\n","        col = (0, 0, 0, 1) # Full black\n","\n","    class_member_mask_q32 = (labels_q32 == k)\n","    xy_q32 = X_tsne_q32[class_member_mask_q32]\n","    plt.plot(xy_q32[:, 0], xy_q32[:, 1], 'o', markerfacecolor=tuple(col),\n","             markeredgecolor='k', markersize=6, alpha=0.7)\n","\n","plt.title(f'DBSCAN Clustering on Digits Dataset (t-SNE Reduced)\\n(Clusters: {n_clusters_q32}, Noise: {n_noise_q32})')\n","plt.xlabel('t-SNE Component 1')\n","plt.ylabel('t-SNE Component 2')\n","plt.grid(True)\n","plt.show()\n","print(\"DBSCAN clustering visualization on Digits dataset (t-SNE reduced) displayed above.\\n\")\n","\n","\n","# ==============================================================================\n","# Q33. Generate synthetic data using make_blobs and apply Agglomerative\n","#      Clustering with complete linkage. Plot the result.\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"Q33. Agglomerative Clustering (complete linkage) on make_blobs.\")\n","print(\"=\"*80)\n","\n","# 1. Generate synthetic data using make_blobs\n","X_q33, y_q33 = make_blobs(n_samples=300, centers=4, cluster_std=0.7, random_state=42)\n","\n","# 2. Apply Agglomerative Clustering with complete linkage\n","# Complete linkage: distance between two clusters is the maximum distance between any two points.\n","agg_clust_q33 = AgglomerativeClustering(n_clusters=4, linkage='complete')\n","labels_q33 = agg_clust_q33.fit_predict(X_q33)\n","\n","# 3. Plot the result\n","plt.figure(figsize=(10, 7))\n","plt.scatter(X_q33[:, 0], X_q33[:, 1], c=labels_q33, cmap='viridis', s=50, alpha=0.8)\n","plt.title('Agglomerative Clustering (Complete Linkage) on Synthetic Blobs (4 Clusters)')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.grid(True)\n","plt.show()\n","print(\"Agglomerative Clustering visualization (complete linkage) displayed above.\\n\")\n","\n","\n","# ==============================================================================\n","# Q34. Load the Breast Cancer dataset and compare inertia values for K=2 to 6\n","#      using K-Means. Show results in a line plot.\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"Q34. K-Means Inertia (WCSS) comparison for K=2 to 6 on Breast Cancer.\")\n","print(\"=\"*80)\n","\n","# 1. Load the Breast Cancer dataset\n","breast_cancer_q34 = load_breast_cancer()\n","X_q34 = breast_cancer_q34.data\n","\n","# It's good practice to scale data before K-Means\n","scaler_q34 = StandardScaler()\n","X_scaled_q34 = scaler_q34.fit_transform(X_q34)\n","\n","# 2. Compare inertia values for K=2 to 6\n","k_values_q34 = range(2, 7) # K=2, 3, 4, 5, 6\n","inertia_values_q34 = []\n","\n","print(\"Inertia values for K-Means on Breast Cancer dataset:\")\n","for k in k_values_q34:\n","    kmeans_q34 = KMeans(n_clusters=k, random_state=42, n_init=10)\n","    kmeans_q34.fit(X_scaled_q34)\n","    inertia_values_q34.append(kmeans_q34.inertia_)\n","    print(f\"  K = {k}, Inertia: {kmeans_q34.inertia_:.4f}\")\n","\n","# 3. Show results in a line plot (Elbow Method visualization)\n","plt.figure(figsize=(10, 6))\n","plt.plot(k_values_q34, inertia_values_q34, marker='o', linestyle='-')\n","plt.xlabel(\"Number of Clusters (K)\")\n","plt.ylabel(\"Inertia (Within-Cluster Sum of Squares)\")\n","plt.title(\"K-Means Inertia vs. Number of Clusters (Breast Cancer Dataset)\")\n","plt.xticks(k_values_q34)\n","plt.grid(True)\n","plt.show()\n","print(\"Inertia comparison line plot displayed above.\\n\")\n","\n","\n","# ==============================================================================\n","# Q35. Generate synthetic concentric circles using make_circles and cluster\n","#      using Agglomerative Clustering with single linkage.\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"Q35. Agglomerative Clustering (single linkage) on concentric circles.\")\n","print(\"=\"*80)\n","\n","# 1. Generate synthetic concentric circles\n","X_q35, y_q35 = make_circles(n_samples=400, factor=0.5, noise=0.05, random_state=0)\n","\n","# 2. Cluster using Agglomerative Clustering with single linkage\n","# Single linkage is often effective for elongated or intertwined clusters like circles\n","agg_clust_q35 = AgglomerativeClustering(n_clusters=2, linkage='single') # Expecting 2 circles\n","labels_q35 = agg_clust_q35.fit_predict(X_q35)\n","\n","# 3. Plot the result\n","plt.figure(figsize=(10, 7))\n","plt.scatter(X_q35[:, 0], X_q35[:, 1], c=labels_q35, cmap='plasma', s=50, alpha=0.8)\n","plt.title('Agglomerative Clustering (Single Linkage) on Concentric Circles (2 Clusters)')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.grid(True)\n","plt.show()\n","print(\"Agglomerative Clustering visualization (single linkage) on concentric circles displayed above.\\n\")\n","\n","\n","# ==============================================================================\n","# Q36. Use the Wine dataset, apply DBSCAN after scaling the data, and count the\n","#      number of clusters (excluding noise).\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"Q36. DBSCAN on Wine dataset (scaled) and cluster count.\")\n","print(\"=\"*80)\n","\n","# 1. Use the Wine dataset\n","wine_q36 = load_wine()\n","X_q36 = wine_q36.data\n","\n","# 2. Apply scaling to the data\n","scaler_q36 = StandardScaler()\n","X_scaled_q36 = scaler_q36.fit_transform(X_q36)\n","\n","# 3. Apply DBSCAN after scaling the data\n","# DBSCAN parameters (eps, min_samples) are crucial and dataset-dependent.\n","# These are illustrative values and might need tuning.\n","dbscan_q36 = DBSCAN(eps=1.5, min_samples=5) # Example parameters\n","labels_q36 = dbscan_q36.fit_predict(X_scaled_q36)\n","\n","# 4. Count the number of clusters (excluding noise)\n","# Noise points are labeled as -1 by DBSCAN\n","n_clusters_q36 = len(set(labels_q36)) - (1 if -1 in labels_q36 else 0)\n","n_noise_q36 = list(labels_q36).count(-1)\n","\n","print(f\"Number of clusters found (excluding noise): {n_clusters_q36}\")\n","print(f\"Number of noise points identified: {n_noise_q36}\\n\")\n","\n","\n","# ==============================================================================\n","# Q37. Generate synthetic data with make_blobs and apply KMeans. Then plot the\n","#      cluster centers on top of the data points.\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"Q37. K-Means on make_blobs and plot cluster centers.\")\n","print(\"=\"*80)\n","\n","# 1. Generate synthetic data with make_blobs\n","X_q37, y_q37 = make_blobs(n_samples=300, centers=3, cluster_std=0.7, random_state=42)\n","\n","# 2. Apply KMeans\n","kmeans_q37 = KMeans(n_clusters=3, random_state=42, n_init=10)\n","kmeans_q37.fit(X_q37)\n","labels_q37 = kmeans_q37.labels_\n","centroids_q37 = kmeans_q37.cluster_centers_\n","\n","# 3. Plot the cluster centers on top of the data points\n","plt.figure(figsize=(10, 7))\n","plt.scatter(X_q37[:, 0], X_q37[:, 1], c=labels_q37, cmap='coolwarm', s=50, alpha=0.8, label='Data Points')\n","plt.scatter(centroids_q37[:, 0], centroids_q37[:, 1], c='black', s=250, alpha=1.0, marker='*', edgecolor='white', linewidth=1.5, label='Cluster Centroids')\n","plt.title('K-Means Clustering with Cluster Centroids')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","print(\"K-Means clustering visualization with cluster centroids displayed above.\\n\")\n","\n","\n","# ==============================================================================\n","# Q38. Load the Iris dataset, cluster with DBSCAN, and print how many samples\n","#      were identified as noise.\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"Q38. DBSCAN on Iris dataset and noise sample count.\")\n","print(\"=\"*80)\n","\n","# 1. Load the Iris dataset\n","iris_q38 = load_iris()\n","X_q38 = iris_q38.data\n","\n","# It's good practice to scale data for DBSCAN\n","scaler_q38 = StandardScaler()\n","X_scaled_q38 = scaler_q38.fit_transform(X_q38)\n","\n","# 2. Cluster with DBSCAN\n","# These parameters are often found to work reasonably well for Iris after scaling\n","dbscan_q38 = DBSCAN(eps=0.5, min_samples=5) # Tune eps/min_samples as needed\n","labels_q38 = dbscan_q38.fit_predict(X_scaled_q38)\n","\n","# 3. Print how many samples were identified as noise\n","n_noise_q38 = list(labels_q38).count(-1)\n","print(f\"Number of samples identified as noise by DBSCAN on Iris dataset: {n_noise_q38}\\n\")\n","\n","\n","# ==============================================================================\n","# Q39. Generate synthetic non-linearly separable data using make_moons, apply\n","#      K-Means, and visualize the clustering result.\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"Q39. K-Means on non-linearly separable make_moons data and visualization.\")\n","print(\"=\"*80)\n","\n","# 1. Generate synthetic non-linearly separable data using make_moons\n","X_q39, y_q39 = make_moons(n_samples=200, noise=0.05, random_state=0)\n","\n","# 2. Apply K-Means (K-Means will struggle here due to non-linear shape)\n","kmeans_q39 = KMeans(n_clusters=2, random_state=0, n_init=10) # Expecting 2 crescent shapes\n","kmeans_q39.fit(X_q39)\n","labels_q39 = kmeans_q39.labels_\n","centroids_q39 = kmeans_q39.cluster_centers_\n","\n","# 3. Visualize the clustering result\n","plt.figure(figsize=(10, 7))\n","plt.scatter(X_q39[:, 0], X_q39[:, 1], c=labels_q39, cmap='spring', s=50, alpha=0.8, label='K-Means Clusters')\n","plt.scatter(centroids_q39[:, 0], centroids_q39[:, 1], c='blue', s=200, marker='X', label='Centroids')\n","plt.title('K-Means Clustering on Non-Linearly Separable Make_Moons')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","print(\"K-Means clustering visualization on make_moons displayed above (note its struggle with non-linear shapes).\\n\")\n","\n","\n","# ==============================================================================\n","# Q40. Load the Digits dataset, apply PCA to reduce to 3 components, then use\n","#      KMeans and visualize with a 3D scatter plot.\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"Q40. K-Means on Digits (PCA-reduced to 3D) and 3D visualization.\")\n","print(\"=\"*80)\n","\n","# 1. Load the Digits dataset\n","digits_q40 = load_digits()\n","X_q40 = digits_q40.data\n","y_q40_true = digits_q40.target # True labels (0-9, 10 classes)\n","\n","# 2. Apply PCA to reduce to 3 components\n","pca_q40 = PCA(n_components=3, random_state=42)\n","X_pca_q40 = pca_q40.fit_transform(X_q40)\n","\n","# 3. Use KMeans (10 clusters for digits 0-9)\n","kmeans_q40 = KMeans(n_clusters=10, random_state=42, n_init=10)\n","kmeans_q40.fit(X_pca_q40)\n","labels_q40 = kmeans_q40.labels_\n","centroids_q40 = kmeans_q40.cluster_centers_\n","\n","# 4. Visualize with a 3D scatter plot\n","fig_q40 = plt.figure(figsize=(10, 8))\n","ax_q40 = fig_q40.add_subplot(111, projection='3d')\n","\n","scatter_q40 = ax_q40.scatter(X_pca_q40[:, 0], X_pca_q40[:, 1], X_pca_q40[:, 2],\n","                             c=labels_q40, cmap='tab20', s=30, alpha=0.8)\n","ax_q40.scatter(centroids_q40[:, 0], centroids_q40[:, 1], centroids_q40[:, 2],\n","               c='black', s=150, marker='X', edgecolor='white', linewidth=1.5, label='Centroids')\n","\n","ax_q40.set_title('K-Means Clustering on Digits Dataset (PCA-reduced to 3D)')\n","ax_q40.set_xlabel('Principal Component 1')\n","ax_q40.set_ylabel('Principal Component 2')\n","ax_q40.set_zlabel('Principal Component 3')\n","fig_q40.colorbar(scatter_q40, ax=ax_q40, label='Cluster Label')\n","plt.legend()\n","plt.show()\n","print(\"K-Means clustering visualization on Digits dataset (PCA-reduced to 3D) displayed above.\\n\")\n","\n","\n","# ==============================================================================\n","# Q41. Generate synthetic blobs with 5 centers and apply KMeans. Then use\n","#      silhouette_score to evaluate the clustering.\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"Q41. K-Means on 5-center blobs and Silhouette Score evaluation.\")\n","print(\"=\"*80)\n","\n","# 1. Generate synthetic blobs with 5 centers\n","X_q41, y_q41 = make_blobs(n_samples=400, centers=5, cluster_std=0.8, random_state=42)\n","\n","# 2. Apply KMeans\n","kmeans_q41 = KMeans(n_clusters=5, random_state=42, n_init=10)\n","labels_q41 = kmeans_q41.fit_predict(X_q41)\n","\n","# 3. Use silhouette_score to evaluate the clustering\n","score_q41 = silhouette_score(X_q41, labels_q41)\n","print(f\"Silhouette Score for K-Means (5 clusters) on synthetic blobs: {score_q41:.4f}\\n\")\n","\n","\n","# ==============================================================================\n","# Q42. Load the Breast Cancer dataset, reduce dimensionality using PCA, and\n","#      apply Agglomerative Clustering. Visualize in 2D.\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"Q42. Agglomerative Clustering on Breast Cancer (PCA-reduced to 2D) and visualization.\")\n","print(\"=\"*80)\n","\n","# 1. Load the Breast Cancer dataset\n","breast_cancer_q42 = load_breast_cancer()\n","X_q42 = breast_cancer_q42.data\n","y_q42_true = breast_cancer_q42.target\n","\n","# Scale the data first\n","scaler_q42 = StandardScaler()\n","X_scaled_q42 = scaler_q42.fit_transform(X_q42)\n","\n","# 2. Reduce dimensionality using PCA\n","pca_q42 = PCA(n_components=2, random_state=42)\n","X_pca_q42 = pca_q42.fit_transform(X_scaled_q42)\n","\n","# 3. Apply Agglomerative Clustering (2 clusters based on true labels)\n","agg_clust_q42 = AgglomerativeClustering(n_clusters=2, linkage='ward')\n","labels_q42 = agg_clust_q42.fit_predict(X_pca_q42)\n","\n","# 4. Visualize in 2D\n","plt.figure(figsize=(10, 7))\n","plt.scatter(X_pca_q42[:, 0], X_pca_q42[:, 1], c=labels_q42, cmap='cividis', s=50, alpha=0.8)\n","plt.title('Agglomerative Clustering on Breast Cancer (PCA-reduced to 2D)')\n","plt.xlabel('Principal Component 1')\n","plt.ylabel('Principal Component 2')\n","plt.grid(True)\n","plt.show()\n","print(\"Agglomerative Clustering visualization on Breast Cancer (PCA-reduced) displayed above.\\n\")\n","\n","\n","# ==============================================================================\n","# Q43. Generate noisy circular data using make_circles and visualize clustering\n","#      results from KMeans and DBSCAN side-by-side.\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"Q43. K-Means vs. DBSCAN on noisy circular data side-by-side.\")\n","print(\"=\"*80)\n","\n","# 1. Generate noisy circular data\n","X_q43, y_q43 = make_circles(n_samples=400, factor=0.5, noise=0.1, random_state=42)\n","\n","# 2. Apply K-Means\n","kmeans_q43 = KMeans(n_clusters=2, random_state=42, n_init=10)\n","labels_kmeans_q43 = kmeans_q43.fit_predict(X_q43)\n","\n","# 3. Apply DBSCAN\n","dbscan_q43 = DBSCAN(eps=0.2, min_samples=10) # Tune as needed\n","labels_dbscan_q43 = dbscan_q43.fit_predict(X_q43)\n","\n","# 4. Visualize clustering results side-by-side\n","fig_q43, axes_q43 = plt.subplots(1, 2, figsize=(16, 7))\n","\n","# K-Means plot\n","axes_q43[0].scatter(X_q43[:, 0], X_q43[:, 1], c=labels_kmeans_q43, cmap='viridis', s=50, alpha=0.8)\n","axes_q43[0].set_title('K-Means Clustering on Noisy Circles')\n","axes_q43[0].set_xlabel('Feature 1')\n","axes_q43[0].set_ylabel('Feature 2')\n","axes_q43[0].grid(True)\n","\n","# DBSCAN plot\n","# Handle noise points for DBSCAN visualization\n","unique_labels_dbscan_q43 = set(labels_dbscan_q43)\n","colors_dbscan_q43 = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels_dbscan_q43))]\n","\n","for k, col in zip(unique_labels_dbscan_q43, colors_dbscan_q43):\n","    if k == -1:\n","        col = [0, 0, 0, 1] # Black for noise\n","    class_member_mask_dbscan_q43 = (labels_dbscan_q43 == k)\n","    xy_dbscan_q43 = X_q43[class_member_mask_dbscan_q43]\n","    axes_q43[1].plot(xy_dbscan_q43[:, 0], xy_dbscan_q43[:, 1], 'o', markerfacecolor=tuple(col),\n","                     markeredgecolor='k', markersize=6, alpha=0.8)\n","\n","axes_q43[1].set_title('DBSCAN Clustering on Noisy Circles')\n","axes_q43[1].set_xlabel('Feature 1')\n","axes_q43[1].set_ylabel('Feature 2')\n","axes_q43[1].grid(True)\n","\n","plt.tight_layout()\n","plt.show()\n","print(\"K-Means vs. DBSCAN clustering on noisy circular data side-by-side displayed above.\\n\")\n","\n","\n","# ==============================================================================\n","# Q44. Load the Iris dataset and plot the Silhouette Coefficient for each sample\n","#      after KMeans clustering.\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"Q44. Silhouette Coefficient per sample for Iris K-Means.\")\n","print(\"=\"*80)\n","\n","# 1. Load the Iris dataset\n","iris_q44 = load_iris()\n","X_q44 = iris_q44.data\n","\n","# Scale the data first\n","scaler_q44 = StandardScaler()\n","X_scaled_q44 = scaler_q44.fit_transform(X_q44)\n","\n","# 2. Apply KMeans clustering (3 clusters)\n","kmeans_q44 = KMeans(n_clusters=3, random_state=42, n_init=10)\n","labels_q44 = kmeans_q44.fit_predict(X_scaled_q44)\n","\n","# 3. Calculate the Silhouette Coefficient for each sample\n","from sklearn.metrics import silhouette_samples\n","silhouette_per_sample_q44 = silhouette_samples(X_scaled_q44, labels_q44)\n","\n","# 4. Plot the Silhouette Coefficient for each sample\n","plt.figure(figsize=(10, 7))\n","y_lower = 10\n","for i in range(3): # Iterate through each cluster (0, 1, 2)\n","    # Aggregate the silhouette scores for samples belonging to cluster i, and sort them\n","    ith_cluster_silhouette_values = silhouette_per_sample_q44[labels_q44 == i]\n","    ith_cluster_silhouette_values.sort()\n","\n","    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n","    y_upper = y_lower + size_cluster_i\n","\n","    color = plt.cm.get_cmap(\"Spectral\")(float(i) / 3) # Using a color map\n","    plt.fill_betweenx(np.arange(y_lower, y_upper),\n","                      0, ith_cluster_silhouette_values,\n","                      facecolor=color, edgecolor=color, alpha=0.7)\n","\n","    # Label the silhouette plots with their cluster numbers at the middle\n","    plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n","\n","    # Compute the new y_lower for next plot\n","    y_lower = y_upper + 10 # 10 for the 0 samples\n","\n","plt.title(\"Silhouette plot for the various clusters (Iris Dataset)\")\n","plt.xlabel(\"The silhouette coefficient values\")\n","plt.ylabel(\"Cluster label\")\n","plt.axvline(x=silhouette_score(X_scaled_q44, labels_q44), color=\"red\", linestyle=\"--\", label='Average Silhouette Score')\n","plt.legend()\n","plt.yticks([]) # Clear the yaxis labels / ticks\n","plt.xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n","plt.show()\n","print(\"Silhouette Coefficient per sample plot displayed above.\\n\")\n","\n","\n","# ==============================================================================\n","# Q45. Generate synthetic data using make_blobs and apply Agglomerative\n","#      Clustering with 'average' linkage. Visualize clusters.\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"Q45. Agglomerative Clustering ('average' linkage) on make_blobs and visualization.\")\n","print(\"=\"*80)\n","\n","# 1. Generate synthetic data using make_blobs\n","X_q45, y_q45 = make_blobs(n_samples=300, centers=4, cluster_std=0.7, random_state=42)\n","\n","# 2. Apply Agglomerative Clustering with 'average' linkage\n","# 'average' linkage: distance between two clusters is the average distance between all pairs of observations.\n","agg_clust_q45 = AgglomerativeClustering(n_clusters=4, linkage='average')\n","labels_q45 = agg_clust_q45.fit_predict(X_q45)\n","\n","# 3. Visualize clusters\n","plt.figure(figsize=(10, 7))\n","plt.scatter(X_q45[:, 0], X_q45[:, 1], c=labels_q45, cmap='magma', s=50, alpha=0.8)\n","plt.title('Agglomerative Clustering (Average Linkage) on Synthetic Blobs (4 Clusters)')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.grid(True)\n","plt.show()\n","print(\"Agglomerative Clustering visualization (average linkage) displayed above.\\n\")\n","\n","\n","# ==============================================================================\n","# Q46. Load the Wine dataset, apply KMeans, and visualize the cluster\n","#      assignments in a seaborn pairplot (first 4 features).\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"Q46. K-Means on Wine dataset and pairplot visualization.\")\n","print(\"=\"*80)\n","\n","# 1. Load the Wine dataset\n","wine_q46 = load_wine()\n","X_q46 = wine_q46.data\n","feature_names_q46 = wine_q46.feature_names\n","\n","# Scale the data first for K-Means\n","scaler_q46 = StandardScaler()\n","X_scaled_q46 = scaler_q46.fit_transform(X_q46)\n","\n","# 2. Apply KMeans (3 clusters based on true labels)\n","kmeans_q46 = KMeans(n_clusters=3, random_state=42, n_init=10)\n","labels_q46 = kmeans_q46.fit_predict(X_scaled_q46)\n","\n","# 3. Visualize the cluster assignments in a seaborn pairplot (first 4 features)\n","# Convert scaled data back to DataFrame for pairplot with feature names\n","df_q46 = pd.DataFrame(X_scaled_q46, columns=feature_names_q46)\n","df_q46['Cluster'] = labels_q46\n","\n","# Select first 4 features plus the 'Cluster' column for pairplot\n","selected_features_q46 = list(feature_names_q46[:4]) + ['Cluster']\n","sns.pairplot(df_q46[selected_features_q46], hue='Cluster', palette='viridis', diag_kind='kde')\n","plt.suptitle('K-Means Cluster Assignments on Wine Dataset (First 4 Features)', y=1.02) # Adjust suptitle position\n","plt.show()\n","print(\"K-Means cluster assignments visualization in a seaborn pairplot displayed above.\\n\")\n","\n","\n","# ==============================================================================\n","# Q47. Generate noisy blobs using make_blobs and use DBSCAN to identify both\n","#      clusters and noise points. Print the count.\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"Q47. DBSCAN on noisy blobs: cluster and noise count.\")\n","print(\"=\"*80)\n","\n","# 1. Generate noisy blobs using make_blobs (higher cluster_std for more noise/overlap)\n","X_q47, y_q47 = make_blobs(n_samples=500, centers=3, cluster_std=1.2, random_state=42)\n","\n","# 2. Use DBSCAN to identify both clusters and noise points\n","# Parameters need tuning for noisy data\n","dbscan_q47 = DBSCAN(eps=0.8, min_samples=8) # Example parameters\n","labels_q47 = dbscan_q47.fit_predict(X_q47)\n","\n","# 3. Print the count of clusters and noise points\n","n_clusters_q47 = len(set(labels_q47)) - (1 if -1 in labels_q47 else 0)\n","n_noise_q47 = list(labels_q47).count(-1)\n","\n","print(f\"Number of clusters identified by DBSCAN: {n_clusters_q47}\")\n","print(f\"Number of noise points identified by DBSCAN: {n_noise_q47}\\n\")\n","\n","# Optional: Visualize for confirmation\n","plt.figure(figsize=(10, 7))\n","unique_labels_q47 = set(labels_q47)\n","colors_q47 = [plt.cm.turbo(each) for each in np.linspace(0, 1, len(unique_labels_q47))]\n","\n","for k, col in zip(unique_labels_q47, colors_q47):\n","    if k == -1: # Noise points are black\n","        col = [0, 0, 0, 1]\n","    class_member_mask_q47 = (labels_q47 == k)\n","    xy_q47 = X_q47[class_member_mask_q47]\n","    plt.plot(xy_q47[:, 0], xy_q47[:, 1], 'o', markerfacecolor=tuple(col),\n","             markeredgecolor='k', markersize=6, alpha=0.7)\n","plt.title(f'DBSCAN Clustering on Noisy Blobs (Clusters: {n_clusters_q47}, Noise: {n_noise_q47})')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.grid(True)\n","plt.show()\n","print(\"DBSCAN clustering visualization on noisy blobs (optional) displayed above.\\n\")\n","\n","\n","# ==============================================================================\n","# Q48. Load the Digits dataset, reduce dimensions using t-SNE, then apply\n","#      Agglomerative Clustering and plot the clusters.\n","# ==============================================================================\n","print(\"\\n\" + \"=\"*80)\n","print(\"Q48. Agglomerative Clustering on Digits (t-SNE reduced) and visualization.\")\n","print(\"=\"*80)\n","\n","# 1. Load the Digits dataset\n","digits_q48 = load_digits()\n","X_q48 = digits_q48.data\n","y_q48_true = digits_q48.target # True labels (10 classes)\n","\n","# 2. Reduce dimensions using t-SNE\n","print(\"  Applying t-SNE (this might take a moment)...\")\n","tsne_q48 = TSNE(n_components=2, random_state=42)\n","X_tsne_q48 = tsne_q48.fit_transform(X_q48)\n","print(\"  t-SNE dimensionality reduction complete.\")\n","\n","# 3. Apply Agglomerative Clustering (e.g., 10 clusters for digits)\n","# Linkage method choice can significantly impact results on t-SNE output.\n","agg_clust_q48 = AgglomerativeClustering(n_clusters=10, linkage='ward')\n","labels_q48 = agg_clust_q48.fit_predict(X_tsne_q48)\n","\n","# 4. Plot the clusters\n","plt.figure(figsize=(10, 7))\n","plt.scatter(X_tsne_q48[:, 0], X_tsne_q48[:, 1], c=labels_q48, cmap='tab20', s=30, alpha=0.8)\n","plt.title('Agglomerative Clustering on Digits Dataset (t-SNE Reduced to 2D)')\n","plt.xlabel('t-SNE Component 1')\n","plt.ylabel('t-SNE Component 2')\n","plt.grid(True)\n","plt.show()\n","print(\"Agglomerative Clustering visualization on Digits dataset (t-SNE reduced) displayed above.\\n\")\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"All practical clustering questions have been addressed with runnable code.\")\n","print(\"==============================================================================\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"g_cZdH1m4rAo"},"execution_count":null,"outputs":[]}]}