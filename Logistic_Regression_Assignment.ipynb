{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnedYMCCVjsAdu+XDG7M3F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rgg-code/ml_regression/blob/main/Logistic_Regression_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression Assignment"
      ],
      "metadata": {
        "id": "o5fZnmmyHvTN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theoretical Questions"
      ],
      "metadata": {
        "id": "7pekVeLiH-AC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Theoretical Questions\n",
        "\n",
        "1.  **What is Logistic Regression, and how does it differ from Linear Regression?**\n",
        "\n",
        "    Logistic Regression is a statistical model used for binary classification, predicting the probability of an event occurring[cite: 2]. Unlike Linear Regression, which predicts continuous values, Logistic Regression uses a sigmoid function to output probabilities between 0 and 1, suitable for classification tasks.\n",
        "\n",
        "2.  **What is the mathematical equation of Logistic Regression?**\n",
        "\n",
        "    The mathematical equation of Logistic Regression involves the sigmoid function[cite: 3]. The linear combination of input features and their coefficients is transformed by the sigmoid function to produce a probability:\n",
        "    $P(Y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + \\dots + \\beta_nx_n)}}$ [cite: 3]\n",
        "\n",
        "3.  **Why do we use the Sigmoid function in Logistic Regression?**\n",
        "\n",
        "    The Sigmoid function (also known as the logistic function) is used in Logistic Regression because it maps any real-valued number into a probability between 0 and 1[cite: 3]. This characteristic makes it ideal for binary classification problems where the output needs to be interpreted as a probability.\n",
        "\n",
        "4.  **What is the cost function of Logistic Regression?**\n",
        "\n",
        "    The cost function for Logistic Regression is typically the Log Loss (also known as Binary Cross-Entropy)[cite: 4]. It measures the performance of a classification model whose output is a probability value between 0 and 1[cite: 4]. The goal during training is to minimize this cost function[cite: 4].\n",
        "    $J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m [y^{(i)}\\log(h_\\theta(x^{(i)})) + (1-y^{(i)})\\log(1-h_\\theta(x^{(i)}))]$ [cite: 4]\n",
        "\n",
        "5.  **What is Regularization in Logistic Regression? Why is it needed?**\n",
        "\n",
        "    Regularization in Logistic Regression is a technique used to prevent overfitting by adding a penalty term to the cost function[cite: 5]. It's needed to control the complexity of the model and improve its generalization performance on unseen data[cite: 5].\n",
        "\n",
        "6.  **Explain the difference between Lasso, Ridge, and Elastic Net regression.**\n",
        "\n",
        "    * **Lasso Regression (L1 regularization)** adds a penalty equal to the absolute value of the magnitude of coefficients[cite: 5]. It can lead to sparse models where some coefficients become exactly zero, effectively performing feature selection[cite: 5].\n",
        "    * **Ridge Regression (L2 regularization)** adds a penalty equal to the square of the magnitude of coefficients[cite: 5]. It shrinks coefficients towards zero but doesn't set them exactly to zero[cite: 5].\n",
        "    * **Elastic Net Regression** is a hybrid that combines both L1 and L2 penalties[cite: 5]. It is useful when there are multiple correlated features[cite: 5].\n",
        "\n",
        "7.  **When should we use Elastic Net instead of Lasso or Ridge?**\n",
        "\n",
        "    Elastic Net should be used instead of Lasso or Ridge when there are highly correlated features in the dataset, or when you want to benefit from both the feature selection of Lasso and the coefficient shrinkage of Ridge[cite: 6].\n",
        "\n",
        "8.  **What is the impact of the regularization parameter (Î») in Logistic Regression?**\n",
        "\n",
        "    The regularization parameter ($\\lambda$, often denoted as C in `scikit-learn` where $C = 1/\\lambda$) controls the strength of the regularization[cite: 7]. A larger $\\lambda$ (smaller C) increases the penalty, leading to simpler models and potentially underfitting[cite: 7]. A smaller $\\lambda$ (larger C) reduces the penalty, allowing the model to fit the training data more closely, which can lead to overfitting[cite: 7].\n",
        "\n",
        "9.  **What are the key assumptions of Logistic Regression?**\n",
        "\n",
        "    Key assumptions of Logistic Regression include a binary outcome variable, independence of observations, no multicollinearity among independent variables, linearity of independent variables and log odds, and a large sample size[cite: 8].\n",
        "\n",
        "10. **What are some alternatives to Logistic Regression for classification tasks?**\n",
        "\n",
        "    Some alternatives to Logistic Regression for classification tasks include Support Vector Machines (SVMs), Decision Trees, Random Forests, Gradient Boosting Machines (e.g., XGBoost, LightGBM), K-Nearest Neighbors (KNN), and Naive Bayes[cite: 10].\n",
        "\n",
        "11. **What are Classification Evaluation Metrics?**\n",
        "\n",
        "    Classification Evaluation Metrics are measures used to quantify the performance of a classification model[cite: 11]. Common metrics include Accuracy, Precision, Recall, F1-Score, ROC-AUC, and Confusion Matrix[cite: 11].\n",
        "\n",
        "12. **How does class imbalance affect Logistic Regression?**\n",
        "\n",
        "    Class imbalance can negatively affect Logistic Regression by causing the model to be biased towards the majority class[cite: 12]. This can lead to poor performance on the minority class, as the model may incorrectly classify minority instances as majority[cite: 12].\n",
        "\n",
        "13. **What is Hyperparameter Tuning in Logistic Regression?**\n",
        "\n",
        "    Hyperparameter tuning in Logistic Regression involves finding the optimal values for parameters that are not learned from the data, such as the regularization strength (C) and the type of penalty (L1, L2, Elastic Net)[cite: 13]. This is typically done to improve model performance and generalization[cite: 13].\n",
        "\n",
        "14. **What are different solvers in Logistic Regression? Which one should be used?**\n",
        "\n",
        "    Common solvers in `scikit-learn`'s Logistic Regression include:\n",
        "    * `liblinear`: Good for small datasets, supports L1 and L2 regularization[cite: 14].\n",
        "    * `lbfgs`: Default solver, good for most cases, supports L2 regularization[cite: 14].\n",
        "    * `newton-cg`: Similar to `lbfgs`, good for large datasets, supports L2[cite: 14].\n",
        "    * `saga`: Supports L1, L2, and Elastic Net regularization, suitable for large datasets[cite: 14].\n",
        "    * `sag`: Faster for large datasets, supports L2[cite: 14].\n",
        "\n",
        "    The choice of solver depends on the dataset size, the type of regularization, and computational efficiency[cite: 14]. `lbfgs` is a good general-purpose choice, while `saga` is versatile for various regularization types[cite: 14].\n",
        "\n",
        "15. **How is Logistic Regression extended for multiclass classification?**\n",
        "\n",
        "    Logistic Regression is extended for multiclass classification using strategies like One-vs-Rest (OvR) or Softmax Regression (multinomial Logistic Regression)[cite: 15].\n",
        "\n",
        "16. **What are the advantages and disadvantages of Logistic Regression?**\n",
        "    \n",
        "    **Advantages:**\n",
        "    * Simple and easy to implement[cite: 16].\n",
        "    * Computationally efficient[cite: 16].\n",
        "    * Provides probabilities for outcomes[cite: 16].\n",
        "    * Interpretable coefficients[cite: 16].\n",
        "\n",
        "    **Disadvantages:**\n",
        "    * Assumes linearity between independent variables and the log odds[cite: 16].\n",
        "    * Can suffer from multicollinearity[cite: 16].\n",
        "    * May not perform well with complex relationships[cite: 16].\n",
        "    * Sensitive to outliers[cite: 16].\n",
        "\n",
        "17. **What are some use cases of Logistic Regression?**\n",
        "\n",
        "    Use cases for Logistic Regression include spam detection, disease prediction, customer churn prediction, credit scoring, and marketing campaign effectiveness[cite: 17].\n",
        "\n",
        "18. **What is the difference between Softmax Regression and Logistic Regression?**\n",
        "\n",
        "    Logistic Regression is primarily used for binary classification[cite: 18]. Softmax Regression (also known as Multinomial Logistic Regression) is an extension of Logistic Regression for multiclass classification, where it assigns probabilities to multiple classes[cite: 18].\n",
        "\n",
        "19. **How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?**\n",
        "\n",
        "    * **One-vs-Rest (OvR)** trains a separate binary logistic regression model for each class against all other classes[cite: 19]. It's generally preferred when the classes are not mutually exclusive (an instance can belong to multiple classes, though in typical classification, it's one class)[cite: 19].\n",
        "    * **Softmax Regression** directly models the probabilities of an instance belonging to each of the multiple classes[cite: 19]. It's suitable when the classes are mutually exclusive (an instance belongs to exactly one class)[cite: 19].\n",
        "\n",
        "20. **How do we interpret coefficients in Logistic Regression?**\n",
        "\n",
        "    In Logistic Regression, the coefficients represent the change in the log-odds of the dependent variable for a one-unit increase in the corresponding independent variable, holding other variables constant[cite: 15]. Exponentiating the coefficient gives the odds ratio[cite: 15]."
      ],
      "metadata": {
        "id": "FjOXAJFhIPBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Practical Questions"
      ],
      "metadata": {
        "id": "G7f08t5YgAti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy."
      ],
      "metadata": {
        "id": "dfLfa5E3h62y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Create a dummy dataset for demonstration ---\n",
        "# In a real-world scenario, you would load your data from a CSV or other source.\n",
        "# Example: df = pd.read_csv('your_dataset.csv')\n",
        "# For this example, we'll create a synthetic dataset.\n",
        "# Let's create a dataset with two features and a binary target variable.\n",
        "\n",
        "# Number of samples\n",
        "n_samples = 100\n",
        "\n",
        "# Feature 1: Random values\n",
        "np.random.seed(42) # for reproducibility\n",
        "feature1 = np.random.rand(n_samples) * 10\n",
        "\n",
        "# Feature 2: Random values\n",
        "feature2 = np.random.rand(n_samples) * 5\n",
        "\n",
        "# Target variable: A simple linear relationship with some noise, then binarized\n",
        "# For demonstration, let's say if feature1 + feature2 > 7, it's class 1, otherwise class 0\n",
        "target_raw = feature1 + feature2 + np.random.randn(n_samples) * 2\n",
        "target = (target_raw > 7).astype(int) # Convert to binary (0 or 1)\n",
        "\n",
        "# Create a Pandas DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Feature1': feature1,\n",
        "    'Feature2': feature2,\n",
        "    'Target': target\n",
        "})\n",
        "\n",
        "print(\"--- Dummy Dataset Head ---\")\n",
        "print(data.head())\n",
        "print(\"\\n--- Dummy Dataset Info ---\")\n",
        "data.info()\n",
        "print(\"\\n--- Target Variable Distribution ---\")\n",
        "print(data['Target'].value_counts())\n",
        "\n",
        "# --- 2. Separate features (X) and target (y) ---\n",
        "X = data[['Feature1', 'Feature2']] # Features\n",
        "y = data['Target']                 # Target variable\n",
        "\n",
        "# --- 3. Split the dataset into training and testing sets ---\n",
        "# We'll use 80% of the data for training and 20% for testing.\n",
        "# random_state ensures reproducibility of the split.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "print(f\"Training target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Testing target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
        "\n",
        "# --- 4. Initialize the Logistic Regression model ---\n",
        "# We are using the default parameters for LogisticRegression, which includes L2 regularization.\n",
        "# For simplicity, we set max_iter to avoid convergence warnings on small/simple datasets.\n",
        "model = LogisticRegression(random_state=42, max_iter=200)\n",
        "\n",
        "# --- 5. Train the Logistic Regression model using the training data ---\n",
        "print(\"\\nTraining the Logistic Regression model...\")\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# --- 6. Make predictions on the test set ---\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# --- 7. Evaluate the model performance using accuracy ---\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# --- 8. Print the model accuracy ---\n",
        "print(f\"\\nModel Accuracy on the test set: {accuracy:.4f}\")\n",
        "\n",
        "# Optional: Print other metrics or model details\n",
        "print(\"\\n--- Model Coefficients ---\")\n",
        "# Coefficients for each feature\n",
        "for i, feature_name in enumerate(X.columns):\n",
        "    print(f\"{feature_name}: {model.coef_[0][i]:.4f}\")\n",
        "print(f\"Intercept: {model.intercept_[0]:.4f}\")\n",
        "\n",
        "print(\"\\n--- Sample Predictions vs. Actuals ---\")\n",
        "# Display some sample predictions alongside actual values\n",
        "sample_results = pd.DataFrame({\n",
        "    'Actual': y_test.reset_index(drop=True),\n",
        "    'Predicted': y_pred\n",
        "}).head(10)\n",
        "print(sample_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sXy124xf43a",
        "outputId": "80e28905-f8a2-4372-cc95-7a0c1403d316"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dummy Dataset Head ---\n",
            "   Feature1  Feature2  Target\n",
            "0  3.745401  0.157146       0\n",
            "1  9.507143  3.182052       1\n",
            "2  7.319939  1.571780       1\n",
            "3  5.986585  2.542853       1\n",
            "4  1.560186  4.537832       1\n",
            "\n",
            "--- Dummy Dataset Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   Feature1  100 non-null    float64\n",
            " 1   Feature2  100 non-null    float64\n",
            " 2   Target    100 non-null    int64  \n",
            "dtypes: float64(2), int64(1)\n",
            "memory usage: 2.5 KB\n",
            "\n",
            "--- Target Variable Distribution ---\n",
            "Target\n",
            "1    51\n",
            "0    49\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training set size: 80 samples\n",
            "Testing set size: 20 samples\n",
            "Training target distribution:\n",
            "Target\n",
            "1    0.5125\n",
            "0    0.4875\n",
            "Name: proportion, dtype: float64\n",
            "Testing target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Training the Logistic Regression model...\n",
            "Model training complete.\n",
            "\n",
            "Model Accuracy on the test set: 0.7500\n",
            "\n",
            "--- Model Coefficients ---\n",
            "Feature1: 1.0965\n",
            "Feature2: 1.3416\n",
            "Intercept: -8.3437\n",
            "\n",
            "--- Sample Predictions vs. Actuals ---\n",
            "   Actual  Predicted\n",
            "0       1          1\n",
            "1       1          0\n",
            "2       1          0\n",
            "3       1          1\n",
            "4       0          1\n",
            "5       0          0\n",
            "6       0          0\n",
            "7       1          1\n",
            "8       0          0\n",
            "9       0          0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "and print the model accuracy."
      ],
      "metadata": {
        "id": "QRUiwY16knp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# --- 1. Create a dummy dataset for demonstration ---\n",
        "# This dataset is designed to be versatile enough for practical questions.\n",
        "# It includes numerical, categorical features, and some missing values.\n",
        "np.random.seed(42) # for reproducibility\n",
        "N_SAMPLES = 500\n",
        "\n",
        "# Numerical features\n",
        "feature_num1 = np.random.rand(N_SAMPLES) * 100\n",
        "feature_num2 = np.random.randn(N_SAMPLES) * 15 + 50\n",
        "feature_num3 = np.random.randint(0, 10, N_SAMPLES)\n",
        "\n",
        "# Categorical features\n",
        "gender = np.random.choice(['Male', 'Female'], N_SAMPLES, p=[0.5, 0.5])\n",
        "city = np.random.choice(['New York', 'London', 'Paris', 'Tokyo'], N_SAMPLES, p=[0.4, 0.3, 0.2, 0.1])\n",
        "\n",
        "# Target variable (binary classification)\n",
        "linear_combination = (0.5 * feature_num1 + 0.8 * feature_num2 - 0.3 * feature_num3 +\n",
        "                      (1 if gender[0] == 'Male' else 0) * 10 +\n",
        "                      (5 if city[0] == 'New York' else 0) +\n",
        "                      np.random.randn(N_SAMPLES) * 20)\n",
        "binary_target = (linear_combination > np.percentile(linear_combination, 50)).astype(int)\n",
        "\n",
        "# Introduce missing values for demonstration\n",
        "missing_indices_num1 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.05), replace=False)\n",
        "missing_indices_num2 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.03), replace=False)\n",
        "feature_num1[missing_indices_num1] = np.nan\n",
        "feature_num2[missing_indices_num2] = np.nan\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Feature_Num1': feature_num1,\n",
        "    'Feature_Num2': feature_num2,\n",
        "    'Feature_Num3': feature_num3,\n",
        "    'Gender': gender,\n",
        "    'City': city,\n",
        "    'Target': binary_target\n",
        "})\n",
        "\n",
        "print(\"--- Dummy Dataset Head ---\")\n",
        "print(data.head())\n",
        "print(\"\\n--- Dummy Dataset Info ---\")\n",
        "data.info()\n",
        "print(\"\\n--- Target Variable Distribution ---\")\n",
        "print(data['Target'].value_counts())\n",
        "\n",
        "# --- 2. Separate features (X) and target (y) ---\n",
        "X = data[['Feature_Num1', 'Feature_Num2', 'Feature_Num3', 'Gender', 'City']]\n",
        "y = data['Target']\n",
        "\n",
        "# --- 3. Identify numerical and categorical columns for preprocessing ---\n",
        "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# --- 4. Create a preprocessing pipeline ---\n",
        "# This pipeline will handle missing values and encoding for categorical features,\n",
        "# and scaling for numerical features.\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')), # Impute missing numerical values with mean\n",
        "    ('scaler', StandardScaler())                  # Scale numerical features\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), # Impute missing categorical values with mode\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encode categorical features\n",
        "])\n",
        "\n",
        "# Create a preprocessor using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# --- 5. Split the dataset into training and testing sets ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "print(f\"Training target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Testing target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
        "\n",
        "# --- Practical Question 2: Apply L1 regularization (Lasso) on a dataset ---\n",
        "print(\"\\n--- Question 2: Logistic Regression with L1 Regularization (Lasso) ---\")\n",
        "\n",
        "# Create a pipeline that first preprocesses the data and then applies Logistic Regression.\n",
        "# For L1 regularization (penalty='l1'), 'liblinear' or 'saga' solvers must be used.\n",
        "# 'C' is the inverse of regularization strength; a smaller C means stronger regularization.\n",
        "model_l1 = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', LogisticRegression(penalty='l1', solver='liblinear', random_state=42, max_iter=500))])\n",
        "\n",
        "# Train the model\n",
        "print(\"Training Logistic Regression model with L1 regularization...\")\n",
        "model_l1.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_l1 = model_l1.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy_l1 = accuracy_score(y_test, y_pred_l1)\n",
        "print(f\"Model Accuracy with L1 Regularization: {accuracy_l1:.4f}\")\n",
        "\n",
        "# Optional: Print coefficients to observe the effect of L1 (some might be zero)\n",
        "# To get meaningful coefficients, you would need to inverse transform the features\n",
        "# or map them back to original feature names after one-hot encoding and scaling.\n",
        "# For simplicity, we'll print the raw coefficients from the classifier.\n",
        "print(\"\\n--- Model Coefficients (after preprocessing) ---\")\n",
        "print(\"Note: Coefficients are for the preprocessed (scaled and one-hot encoded) features.\")\n",
        "print(model_l1.named_steps['classifier'].coef_)\n",
        "print(f\"Intercept: {model_l1.named_steps['classifier'].intercept_[0]:.4f}\")\n",
        "\n",
        "# To get feature names after preprocessing for better interpretability:\n",
        "# This part is more complex as it requires accessing the fitted preprocessor\n",
        "# and its transformers to get the feature names.\n",
        "try:\n",
        "    fitted_preprocessor = model_l1.named_steps['preprocessor']\n",
        "    onehot_encoder = fitted_preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
        "    encoded_feature_names = list(onehot_encoder.get_feature_names_out(categorical_cols))\n",
        "    all_feature_names = numerical_cols + encoded_feature_names\n",
        "\n",
        "    coefficients_df = pd.DataFrame({\n",
        "        'Feature': all_feature_names,\n",
        "        'Coefficient': model_l1.named_steps['classifier'].coef_[0]\n",
        "    }).sort_values(by='Coefficient', ascending=False)\n",
        "    print(\"\\n--- Coefficients with Feature Names ---\")\n",
        "    print(coefficients_df)\n",
        "except Exception as e:\n",
        "    print(f\"Could not display coefficients with feature names: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swfnPUMdkTd5",
        "outputId": "75bb12fd-87d1-4059-8bf3-fbcbbc18437d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dummy Dataset Head ---\n",
            "   Feature_Num1  Feature_Num2  Feature_Num3  Gender      City  Target\n",
            "0     37.454012     55.126340             0  Female  New York       0\n",
            "1     95.071431     78.142563             5  Female     Paris       1\n",
            "2     73.199394     64.256358             7    Male  New York       0\n",
            "3     59.865848     41.346445             2    Male  New York       0\n",
            "4     15.601864     36.523780             7  Female     Tokyo       0\n",
            "\n",
            "--- Dummy Dataset Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 6 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Feature_Num1  475 non-null    float64\n",
            " 1   Feature_Num2  485 non-null    float64\n",
            " 2   Feature_Num3  500 non-null    int64  \n",
            " 3   Gender        500 non-null    object \n",
            " 4   City          500 non-null    object \n",
            " 5   Target        500 non-null    int64  \n",
            "dtypes: float64(2), int64(2), object(2)\n",
            "memory usage: 23.6+ KB\n",
            "\n",
            "--- Target Variable Distribution ---\n",
            "Target\n",
            "0    250\n",
            "1    250\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training set size: 400 samples\n",
            "Testing set size: 100 samples\n",
            "Training target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "Testing target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "--- Question 2: Logistic Regression with L1 Regularization (Lasso) ---\n",
            "Training Logistic Regression model with L1 regularization...\n",
            "Model training complete.\n",
            "Model Accuracy with L1 Regularization: 0.7500\n",
            "\n",
            "--- Model Coefficients (after preprocessing) ---\n",
            "Note: Coefficients are for the preprocessed (scaled and one-hot encoded) features.\n",
            "[[ 1.1743622   1.03417876 -0.06684952  0.21567154  0.         -0.5258825\n",
            "   0.          0.01015241  0.42588408]]\n",
            "Intercept: 0.0000\n",
            "\n",
            "--- Coefficients with Feature Names ---\n",
            "         Feature  Coefficient\n",
            "0   Feature_Num1     1.174362\n",
            "1   Feature_Num2     1.034179\n",
            "8     City_Tokyo     0.425884\n",
            "3  Gender_Female     0.215672\n",
            "7     City_Paris     0.010152\n",
            "6  City_New York     0.000000\n",
            "4    Gender_Male     0.000000\n",
            "2   Feature_Num3    -0.066850\n",
            "5    City_London    -0.525883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "LogisticRegression(penalty='l2'). Print model accuracy and coefficients."
      ],
      "metadata": {
        "id": "SdfO8eJ4qeDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# --- 1. Create a dummy dataset for demonstration ---\n",
        "# This dataset is designed to be versatile enough for practical questions.\n",
        "# It includes numerical, categorical features, and some missing values.\n",
        "np.random.seed(42) # for reproducibility\n",
        "N_SAMPLES = 500\n",
        "\n",
        "# Numerical features\n",
        "feature_num1 = np.random.rand(N_SAMPLES) * 100\n",
        "feature_num2 = np.random.randn(N_SAMPLES) * 15 + 50\n",
        "feature_num3 = np.random.randint(0, 10, N_SAMPLES)\n",
        "\n",
        "# Categorical features\n",
        "gender = np.random.choice(['Male', 'Female'], N_SAMPLES, p=[0.5, 0.5])\n",
        "city = np.random.choice(['New York', 'London', 'Paris', 'Tokyo'], N_SAMPLES, p=[0.4, 0.3, 0.2, 0.1])\n",
        "\n",
        "# Target variable (binary classification)\n",
        "linear_combination = (0.5 * feature_num1 + 0.8 * feature_num2 - 0.3 * feature_num3 +\n",
        "                      (1 if gender[0] == 'Male' else 0) * 10 +\n",
        "                      (5 if city[0] == 'New York' else 0) +\n",
        "                      np.random.randn(N_SAMPLES) * 20)\n",
        "binary_target = (linear_combination > np.percentile(linear_combination, 50)).astype(int)\n",
        "\n",
        "# Introduce missing values for demonstration\n",
        "missing_indices_num1 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.05), replace=False)\n",
        "missing_indices_num2 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.03), replace=False)\n",
        "feature_num1[missing_indices_num1] = np.nan\n",
        "feature_num2[missing_indices_num2] = np.nan\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Feature_Num1': feature_num1,\n",
        "    'Feature_Num2': feature_num2,\n",
        "    'Feature_Num3': feature_num3,\n",
        "    'Gender': gender,\n",
        "    'City': city,\n",
        "    'Target': binary_target\n",
        "})\n",
        "\n",
        "print(\"--- Dummy Dataset Head ---\")\n",
        "print(data.head())\n",
        "print(\"\\n--- Dummy Dataset Info ---\")\n",
        "data.info()\n",
        "print(\"\\n--- Target Variable Distribution ---\")\n",
        "print(data['Target'].value_counts())\n",
        "\n",
        "# --- 2. Separate features (X) and target (y) ---\n",
        "X = data[['Feature_Num1', 'Feature_Num2', 'Feature_Num3', 'Gender', 'City']]\n",
        "y = data['Target']\n",
        "\n",
        "# --- 3. Identify numerical and categorical columns for preprocessing ---\n",
        "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# --- 4. Create a preprocessing pipeline ---\n",
        "# This pipeline will handle missing values and encoding for categorical features,\n",
        "# and scaling for numerical features.\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')), # Impute missing numerical values with mean\n",
        "    ('scaler', StandardScaler())                  # Scale numerical features\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), # Impute missing categorical values with mode\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encode categorical features\n",
        "])\n",
        "\n",
        "# Create a preprocessor using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# --- 5. Split the dataset into training and testing sets ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "print(f\"Training target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Testing target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
        "\n",
        "# --- Practical Question 3: Train Logistic Regression with L2 regularization (Ridge) ---\n",
        "print(\"\\n--- Question 3: Logistic Regression with L2 Regularization (Ridge) ---\")\n",
        "\n",
        "# Create a pipeline that first preprocesses the data and then applies Logistic Regression.\n",
        "# For L2 regularization (penalty='l2'), 'lbfgs' is a common and robust solver.\n",
        "# 'l2' is often the default penalty, but it's good practice to specify it.\n",
        "model_l2 = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', LogisticRegression(penalty='l2', solver='lbfgs', random_state=42, max_iter=500))])\n",
        "\n",
        "# Train the model\n",
        "print(\"Training Logistic Regression model with L2 regularization...\")\n",
        "model_l2.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_l2 = model_l2.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy_l2 = accuracy_score(y_test, y_pred_l2)\n",
        "print(f\"Model Accuracy with L2 Regularization: {accuracy_l2:.4f}\")\n",
        "\n",
        "# --- Print model coefficients ---\n",
        "# Access the LogisticRegression estimator from the pipeline\n",
        "logistic_reg_model = model_l2.named_steps['classifier']\n",
        "\n",
        "# Coefficients are for the preprocessed (scaled and one-hot encoded) features.\n",
        "print(\"\\n--- Model Coefficients (after preprocessing) ---\")\n",
        "print(\"Note: Coefficients are for the preprocessed (scaled and one-hot encoded) features.\")\n",
        "print(f\"Coefficients array: {logistic_reg_model.coef_[0]}\")\n",
        "print(f\"Intercept: {logistic_reg_model.intercept_[0]:.4f}\")\n",
        "\n",
        "# To get feature names after preprocessing for better interpretability:\n",
        "try:\n",
        "    fitted_preprocessor = model_l2.named_steps['preprocessor']\n",
        "    # Get feature names from the one-hot encoder for categorical features\n",
        "    onehot_encoder = fitted_preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
        "    encoded_feature_names = list(onehot_encoder.get_feature_names_out(categorical_cols))\n",
        "\n",
        "    # Combine numerical and encoded categorical feature names\n",
        "    all_feature_names = numerical_cols + encoded_feature_names\n",
        "\n",
        "    coefficients_df = pd.DataFrame({\n",
        "        'Feature': all_feature_names,\n",
        "        'Coefficient': logistic_reg_model.coef_[0]\n",
        "    }).sort_values(by='Coefficient', ascending=False)\n",
        "    print(\"\\n--- Coefficients with Feature Names ---\")\n",
        "    print(coefficients_df)\n",
        "except Exception as e:\n",
        "    print(f\"Could not display coefficients with feature names: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUFvcYsuqjbu",
        "outputId": "e46db889-91cf-4ee0-b641-8d5879096752"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dummy Dataset Head ---\n",
            "   Feature_Num1  Feature_Num2  Feature_Num3  Gender      City  Target\n",
            "0     37.454012     55.126340             0  Female  New York       0\n",
            "1     95.071431     78.142563             5  Female     Paris       1\n",
            "2     73.199394     64.256358             7    Male  New York       0\n",
            "3     59.865848     41.346445             2    Male  New York       0\n",
            "4     15.601864     36.523780             7  Female     Tokyo       0\n",
            "\n",
            "--- Dummy Dataset Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 6 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Feature_Num1  475 non-null    float64\n",
            " 1   Feature_Num2  485 non-null    float64\n",
            " 2   Feature_Num3  500 non-null    int64  \n",
            " 3   Gender        500 non-null    object \n",
            " 4   City          500 non-null    object \n",
            " 5   Target        500 non-null    int64  \n",
            "dtypes: float64(2), int64(2), object(2)\n",
            "memory usage: 23.6+ KB\n",
            "\n",
            "--- Target Variable Distribution ---\n",
            "Target\n",
            "0    250\n",
            "1    250\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training set size: 400 samples\n",
            "Testing set size: 100 samples\n",
            "Training target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "Testing target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "--- Question 3: Logistic Regression with L2 Regularization (Ridge) ---\n",
            "Training Logistic Regression model with L2 regularization...\n",
            "Model training complete.\n",
            "Model Accuracy with L2 Regularization: 0.7500\n",
            "\n",
            "--- Model Coefficients (after preprocessing) ---\n",
            "Note: Coefficients are for the preprocessed (scaled and one-hot encoded) features.\n",
            "Coefficients array: [ 1.18035544  1.03646275 -0.07887621  0.15202954 -0.09360303 -0.5400961\n",
            "  0.00136676  0.08333146  0.51382438]\n",
            "Intercept: 0.0714\n",
            "\n",
            "--- Coefficients with Feature Names ---\n",
            "         Feature  Coefficient\n",
            "0   Feature_Num1     1.180355\n",
            "1   Feature_Num2     1.036463\n",
            "8     City_Tokyo     0.513824\n",
            "3  Gender_Female     0.152030\n",
            "7     City_Paris     0.083331\n",
            "6  City_New York     0.001367\n",
            "2   Feature_Num3    -0.078876\n",
            "4    Gender_Male    -0.093603\n",
            "5    City_London    -0.540096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')."
      ],
      "metadata": {
        "id": "m33JrclbrOQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# --- 1. Create a dummy dataset for demonstration ---\n",
        "# This dataset is designed to be versatile enough for practical questions.\n",
        "# It includes numerical, categorical features, and some missing values.\n",
        "np.random.seed(42) # for reproducibility\n",
        "N_SAMPLES = 500\n",
        "\n",
        "# Numerical features\n",
        "feature_num1 = np.random.rand(N_SAMPLES) * 100\n",
        "feature_num2 = np.random.randn(N_SAMPLES) * 15 + 50\n",
        "feature_num3 = np.random.randint(0, 10, N_SAMPLES)\n",
        "\n",
        "# Categorical features\n",
        "gender = np.random.choice(['Male', 'Female'], N_SAMPLES, p=[0.5, 0.5])\n",
        "city = np.random.choice(['New York', 'London', 'Paris', 'Tokyo'], N_SAMPLES, p=[0.4, 0.3, 0.2, 0.1])\n",
        "\n",
        "# Target variable (binary classification)\n",
        "linear_combination = (0.5 * feature_num1 + 0.8 * feature_num2 - 0.3 * feature_num3 +\n",
        "                      (1 if gender[0] == 'Male' else 0) * 10 +\n",
        "                      (5 if city[0] == 'New York' else 0) +\n",
        "                      np.random.randn(N_SAMPLES) * 20)\n",
        "binary_target = (linear_combination > np.percentile(linear_combination, 50)).astype(int)\n",
        "\n",
        "# Introduce missing values for demonstration\n",
        "missing_indices_num1 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.05), replace=False)\n",
        "missing_indices_num2 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.03), replace=False)\n",
        "feature_num1[missing_indices_num1] = np.nan\n",
        "feature_num2[missing_indices_num2] = np.nan\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Feature_Num1': feature_num1,\n",
        "    'Feature_Num2': feature_num2,\n",
        "    'Feature_Num3': feature_num3,\n",
        "    'Gender': gender,\n",
        "    'City': city,\n",
        "    'Target': binary_target\n",
        "})\n",
        "\n",
        "print(\"--- Dummy Dataset Head ---\")\n",
        "print(data.head())\n",
        "print(\"\\n--- Dummy Dataset Info ---\")\n",
        "data.info()\n",
        "print(\"\\n--- Target Variable Distribution ---\")\n",
        "print(data['Target'].value_counts())\n",
        "\n",
        "# --- 2. Separate features (X) and target (y) ---\n",
        "X = data[['Feature_Num1', 'Feature_Num2', 'Feature_Num3', 'Gender', 'City']]\n",
        "y = data['Target']\n",
        "\n",
        "# --- 3. Identify numerical and categorical columns for preprocessing ---\n",
        "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# --- 4. Create a preprocessing pipeline ---\n",
        "# This pipeline will handle missing values and encoding for categorical features,\n",
        "# and scaling for numerical features.\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')), # Impute missing numerical values with mean\n",
        "    ('scaler', StandardScaler())                  # Scale numerical features\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), # Impute missing categorical values with mode\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encode categorical features\n",
        "])\n",
        "\n",
        "# Create a preprocessor using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# --- 5. Split the dataset into training and testing sets ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "print(f\"Training target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Testing target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
        "\n",
        "# --- Practical Question 4: Train Logistic Regression with Elastic Net Regularization ---\n",
        "print(\"\\n--- Question 4: Logistic Regression with Elastic Net Regularization ---\")\n",
        "\n",
        "# Create a pipeline that first preprocesses the data and then applies Logistic Regression.\n",
        "# For Elastic Net regularization (penalty='elasticnet'), the 'saga' solver must be used.\n",
        "# 'l1_ratio' controls the mix of L1 and L2 penalties:\n",
        "#   - l1_ratio = 0: equivalent to L2 (Ridge)\n",
        "#   - l1_ratio = 1: equivalent to L1 (Lasso)\n",
        "#   - 0 < l1_ratio < 1: a combination of L1 and L2\n",
        "# We'll use l1_ratio=0.5 for an equal mix.\n",
        "model_elastic_net = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                     ('classifier', LogisticRegression(\n",
        "                                         penalty='elasticnet',\n",
        "                                         solver='saga', # 'saga' is the only solver that supports elasticnet\n",
        "                                         l1_ratio=0.5,  # 0.5 means equal mix of L1 and L2\n",
        "                                         random_state=42,\n",
        "                                         max_iter=1000 # Increased max_iter for convergence with saga\n",
        "                                     ))])\n",
        "\n",
        "# Train the model\n",
        "print(\"Training Logistic Regression model with Elastic Net regularization...\")\n",
        "model_elastic_net.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_elastic_net = model_elastic_net.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy_elastic_net = accuracy_score(y_test, y_pred_elastic_net)\n",
        "print(f\"Model Accuracy with Elastic Net Regularization: {accuracy_elastic_net:.4f}\")\n",
        "\n",
        "# Optional: Print model coefficients\n",
        "print(\"\\n--- Model Coefficients (after preprocessing) ---\")\n",
        "print(\"Note: Coefficients are for the preprocessed (scaled and one-hot encoded) features.\")\n",
        "print(f\"Coefficients array: {model_elastic_net.named_steps['classifier'].coef_[0]}\")\n",
        "print(f\"Intercept: {model_elastic_net.named_steps['classifier'].intercept_[0]:.4f}\")\n",
        "\n",
        "# To get feature names after preprocessing for better interpretability:\n",
        "try:\n",
        "    fitted_preprocessor = model_elastic_net.named_steps['preprocessor']\n",
        "    onehot_encoder = fitted_preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
        "    encoded_feature_names = list(onehot_encoder.get_feature_names_out(categorical_cols))\n",
        "    all_feature_names = numerical_cols + encoded_feature_names\n",
        "\n",
        "    coefficients_df = pd.DataFrame({\n",
        "        'Feature': all_feature_names,\n",
        "        'Coefficient': model_elastic_net.named_steps['classifier'].coef_[0]\n",
        "    }).sort_values(by='Coefficient', ascending=False)\n",
        "    print(\"\\n--- Coefficients with Feature Names ---\")\n",
        "    print(coefficients_df)\n",
        "except Exception as e:\n",
        "    print(f\"Could not display coefficients with feature names: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYpCbUjErPio",
        "outputId": "95e95912-c1bc-4648-e854-5787007932eb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dummy Dataset Head ---\n",
            "   Feature_Num1  Feature_Num2  Feature_Num3  Gender      City  Target\n",
            "0     37.454012     55.126340             0  Female  New York       0\n",
            "1     95.071431     78.142563             5  Female     Paris       1\n",
            "2     73.199394     64.256358             7    Male  New York       0\n",
            "3     59.865848     41.346445             2    Male  New York       0\n",
            "4     15.601864     36.523780             7  Female     Tokyo       0\n",
            "\n",
            "--- Dummy Dataset Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 6 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Feature_Num1  475 non-null    float64\n",
            " 1   Feature_Num2  485 non-null    float64\n",
            " 2   Feature_Num3  500 non-null    int64  \n",
            " 3   Gender        500 non-null    object \n",
            " 4   City          500 non-null    object \n",
            " 5   Target        500 non-null    int64  \n",
            "dtypes: float64(2), int64(2), object(2)\n",
            "memory usage: 23.6+ KB\n",
            "\n",
            "--- Target Variable Distribution ---\n",
            "Target\n",
            "0    250\n",
            "1    250\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training set size: 400 samples\n",
            "Testing set size: 100 samples\n",
            "Training target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "Testing target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "--- Question 4: Logistic Regression with Elastic Net Regularization ---\n",
            "Training Logistic Regression model with Elastic Net regularization...\n",
            "Model training complete.\n",
            "Model Accuracy with Elastic Net Regularization: 0.7500\n",
            "\n",
            "--- Model Coefficients (after preprocessing) ---\n",
            "Note: Coefficients are for the preprocessed (scaled and one-hot encoded) features.\n",
            "Coefficients array: [ 1.17668392  1.03520025 -0.07233357  0.11298968 -0.10713189 -0.54762402\n",
            "  0.          0.03347861  0.45817462]\n",
            "Intercept: 0.1154\n",
            "\n",
            "--- Coefficients with Feature Names ---\n",
            "         Feature  Coefficient\n",
            "0   Feature_Num1     1.176684\n",
            "1   Feature_Num2     1.035200\n",
            "8     City_Tokyo     0.458175\n",
            "3  Gender_Female     0.112990\n",
            "7     City_Paris     0.033479\n",
            "6  City_New York     0.000000\n",
            "2   Feature_Num3    -0.072334\n",
            "4    Gender_Male    -0.107132\n",
            "5    City_London    -0.547624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "multi_class='ovr'."
      ],
      "metadata": {
        "id": "JQdK_yVSrm3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# --- 1. Create a dummy dataset for demonstration ---\n",
        "# This dataset will have a multiclass target variable.\n",
        "np.random.seed(42) # for reproducibility\n",
        "N_SAMPLES = 500\n",
        "\n",
        "# Numerical features\n",
        "feature_num1 = np.random.rand(N_SAMPLES) * 100\n",
        "feature_num2 = np.random.randn(N_SAMPLES) * 15 + 50\n",
        "feature_num3 = np.random.randint(0, 10, N_SAMPLES)\n",
        "\n",
        "# Categorical features\n",
        "gender = np.random.choice(['Male', 'Female'], N_SAMPLES, p=[0.5, 0.5])\n",
        "city = np.random.choice(['New York', 'London', 'Paris', 'Tokyo'], N_SAMPLES, p=[0.4, 0.3, 0.2, 0.1])\n",
        "\n",
        "# Multiclass Target variable: Let's create 3 classes based on different thresholds\n",
        "# of a linear combination of features + noise.\n",
        "linear_combination = (0.5 * feature_num1 + 0.8 * feature_num2 - 0.3 * feature_num3 +\n",
        "                      (1 if gender[0] == 'Male' else 0) * 10 +\n",
        "                      (5 if city[0] == 'New York' else 0) +\n",
        "                      np.random.randn(N_SAMPLES) * 20)\n",
        "\n",
        "# Define thresholds to create 3 distinct classes\n",
        "multi_target = np.zeros(N_SAMPLES, dtype=int)\n",
        "# Class 0: lowest values\n",
        "multi_target[linear_combination <= np.percentile(linear_combination, 33)] = 0\n",
        "# Class 1: middle values\n",
        "multi_target[(linear_combination > np.percentile(linear_combination, 33)) &\n",
        "             (linear_combination <= np.percentile(linear_combination, 66))] = 1\n",
        "# Class 2: highest values\n",
        "multi_target[linear_combination > np.percentile(linear_combination, 66)] = 2\n",
        "\n",
        "\n",
        "# Introduce some missing values for demonstration of preprocessing\n",
        "missing_indices_num1 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.05), replace=False)\n",
        "missing_indices_num2 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.03), replace=False)\n",
        "feature_num1[missing_indices_num1] = np.nan\n",
        "feature_num2[missing_indices_num2] = np.nan\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Feature_Num1': feature_num1,\n",
        "    'Feature_Num2': feature_num2,\n",
        "    'Feature_Num3': feature_num3,\n",
        "    'Gender': gender,\n",
        "    'City': city,\n",
        "    'Multi_Target': multi_target # Our multiclass target\n",
        "})\n",
        "\n",
        "print(\"--- Dummy Dataset Head ---\")\n",
        "print(data.head())\n",
        "print(\"\\n--- Dummy Dataset Info ---\")\n",
        "data.info()\n",
        "print(\"\\n--- Multiclass Target Variable Distribution ---\")\n",
        "print(data['Multi_Target'].value_counts())\n",
        "\n",
        "# --- 2. Separate features (X) and target (y) ---\n",
        "X = data[['Feature_Num1', 'Feature_Num2', 'Feature_Num3', 'Gender', 'City']]\n",
        "y = data['Multi_Target'] # Use the multiclass target\n",
        "\n",
        "# --- 3. Identify numerical and categorical columns for preprocessing ---\n",
        "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# --- 4. Create a preprocessing pipeline ---\n",
        "# This pipeline will handle missing values, scaling, and one-hot encoding.\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')), # Impute missing numerical values with mean\n",
        "    ('scaler', StandardScaler())                  # Scale numerical features\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), # Impute missing categorical values with mode\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encode categorical features\n",
        "])\n",
        "\n",
        "# Create a preprocessor using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# --- 5. Split the dataset into training and testing sets ---\n",
        "# For multiclass classification, it's crucial to use stratify=y to maintain\n",
        "# the class distribution in both training and testing sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "print(f\"Training target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Testing target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
        "\n",
        "# --- Practical Question 5: Train Logistic Regression for multiclass classification using multi_class='ovr' ---\n",
        "print(\"\\n--- Question 5: Multiclass Logistic Regression (One-vs-Rest) ---\")\n",
        "\n",
        "# Create a pipeline that first preprocesses the data and then applies Logistic Regression.\n",
        "# multi_class='ovr' (One-vs-Rest):\n",
        "#   Trains a binary classifier for each class against all other classes.\n",
        "#   The final prediction is the class for which the corresponding classifier outputs the highest probability.\n",
        "# solver='lbfgs' is a good choice for 'ovr' as it handles multiclass problems well.\n",
        "model_ovr = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                             ('classifier', LogisticRegression(multi_class='ovr', solver='lbfgs', random_state=42, max_iter=500))])\n",
        "\n",
        "# Train the model\n",
        "print(\"Training Multiclass Logistic Regression model with 'ovr' strategy...\")\n",
        "model_ovr.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_ovr = model_ovr.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "print(f\"Multiclass Model Accuracy (One-vs-Rest): {accuracy_ovr:.4f}\")\n",
        "\n",
        "# Optional: Print classification report for more detailed metrics per class\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"\\n--- Classification Report (One-vs-Rest) ---\")\n",
        "print(classification_report(y_test, y_pred_ovr))\n",
        "\n",
        "# Optional: Inspect coefficients for each class (each row corresponds to a class)\n",
        "print(\"\\n--- Model Coefficients per Class (after preprocessing) ---\")\n",
        "# The .coef_ attribute will have shape (n_classes, n_features) for multiclass.\n",
        "print(\"Shape of coefficients:\", model_ovr.named_steps['classifier'].coef_.shape)\n",
        "print(\"Coefficients for each class:\\n\", model_ovr.named_steps['classifier'].coef_)\n",
        "print(\"Intercepts for each class:\\n\", model_ovr.named_steps['classifier'].intercept_)\n",
        "\n",
        "# To get feature names after preprocessing for better interpretability:\n",
        "try:\n",
        "    fitted_preprocessor = model_ovr.named_steps['preprocessor']\n",
        "    onehot_encoder = fitted_preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
        "    encoded_feature_names = list(onehot_encoder.get_feature_names_out(categorical_cols))\n",
        "    all_feature_names = numerical_cols + encoded_feature_names\n",
        "\n",
        "    # Display coefficients for each class\n",
        "    for i, class_label in enumerate(model_ovr.named_steps['classifier'].classes_):\n",
        "        coefficients_df = pd.DataFrame({\n",
        "            'Feature': all_feature_names,\n",
        "            'Coefficient': model_ovr.named_steps['classifier'].coef_[i]\n",
        "        }).sort_values(by='Coefficient', ascending=False)\n",
        "        print(f\"\\n--- Coefficients for Class {class_label} ---\")\n",
        "        print(coefficients_df.head(5)) # Print top 5 for brevity\n",
        "        print(coefficients_df.tail(5)) # Print bottom 5 for brevity\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Could not display coefficients with feature names for each class: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VoILINxrzwY",
        "outputId": "a352616c-24d9-428d-a39c-35b9bd6f1f9f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dummy Dataset Head ---\n",
            "   Feature_Num1  Feature_Num2  Feature_Num3  Gender      City  Multi_Target\n",
            "0     37.454012     55.126340             0  Female  New York             1\n",
            "1     95.071431     78.142563             5  Female     Paris             2\n",
            "2     73.199394     64.256358             7    Male  New York             1\n",
            "3     59.865848     41.346445             2    Male  New York             1\n",
            "4     15.601864     36.523780             7  Female     Tokyo             1\n",
            "\n",
            "--- Dummy Dataset Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 6 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Feature_Num1  475 non-null    float64\n",
            " 1   Feature_Num2  485 non-null    float64\n",
            " 2   Feature_Num3  500 non-null    int64  \n",
            " 3   Gender        500 non-null    object \n",
            " 4   City          500 non-null    object \n",
            " 5   Multi_Target  500 non-null    int64  \n",
            "dtypes: float64(2), int64(2), object(2)\n",
            "memory usage: 23.6+ KB\n",
            "\n",
            "--- Multiclass Target Variable Distribution ---\n",
            "Multi_Target\n",
            "2    170\n",
            "1    165\n",
            "0    165\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training set size: 400 samples\n",
            "Testing set size: 100 samples\n",
            "Training target distribution:\n",
            "Multi_Target\n",
            "2    0.34\n",
            "0    0.33\n",
            "1    0.33\n",
            "Name: proportion, dtype: float64\n",
            "Testing target distribution:\n",
            "Multi_Target\n",
            "2    0.34\n",
            "1    0.33\n",
            "0    0.33\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "--- Question 5: Multiclass Logistic Regression (One-vs-Rest) ---\n",
            "Training Multiclass Logistic Regression model with 'ovr' strategy...\n",
            "Model training complete.\n",
            "Multiclass Model Accuracy (One-vs-Rest): 0.5800\n",
            "\n",
            "--- Classification Report (One-vs-Rest) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.64      0.63        33\n",
            "           1       0.44      0.33      0.38        33\n",
            "           2       0.63      0.76      0.69        34\n",
            "\n",
            "    accuracy                           0.58       100\n",
            "   macro avg       0.56      0.58      0.57       100\n",
            "weighted avg       0.56      0.58      0.57       100\n",
            "\n",
            "\n",
            "--- Model Coefficients per Class (after preprocessing) ---\n",
            "Shape of coefficients: (3, 9)\n",
            "Coefficients for each class:\n",
            " [[-1.16643062 -0.84393233 -0.08458521  0.00218975 -0.00478142  0.30949112\n",
            "  -0.10528139 -0.37113852  0.16433711]\n",
            " [ 0.08051786 -0.19715197  0.25705769 -0.08491544  0.08542403 -0.08569416\n",
            "   0.13578534  0.34239751 -0.39198009]\n",
            " [ 1.14600277  1.09112319 -0.32959301  0.09496461 -0.08931044 -0.17460283\n",
            "  -0.02940795 -0.02597483  0.23563978]]\n",
            "Intercepts for each class:\n",
            " [-1.00035168 -0.80371072 -0.93854784]\n",
            "\n",
            "--- Coefficients for Class 0 ---\n",
            "         Feature  Coefficient\n",
            "5    City_London     0.309491\n",
            "8     City_Tokyo     0.164337\n",
            "3  Gender_Female     0.002190\n",
            "4    Gender_Male    -0.004781\n",
            "2   Feature_Num3    -0.084585\n",
            "         Feature  Coefficient\n",
            "2   Feature_Num3    -0.084585\n",
            "6  City_New York    -0.105281\n",
            "7     City_Paris    -0.371139\n",
            "1   Feature_Num2    -0.843932\n",
            "0   Feature_Num1    -1.166431\n",
            "\n",
            "--- Coefficients for Class 1 ---\n",
            "         Feature  Coefficient\n",
            "7     City_Paris     0.342398\n",
            "2   Feature_Num3     0.257058\n",
            "6  City_New York     0.135785\n",
            "4    Gender_Male     0.085424\n",
            "0   Feature_Num1     0.080518\n",
            "         Feature  Coefficient\n",
            "0   Feature_Num1     0.080518\n",
            "3  Gender_Female    -0.084915\n",
            "5    City_London    -0.085694\n",
            "1   Feature_Num2    -0.197152\n",
            "8     City_Tokyo    -0.391980\n",
            "\n",
            "--- Coefficients for Class 2 ---\n",
            "         Feature  Coefficient\n",
            "0   Feature_Num1     1.146003\n",
            "1   Feature_Num2     1.091123\n",
            "8     City_Tokyo     0.235640\n",
            "3  Gender_Female     0.094965\n",
            "7     City_Paris    -0.025975\n",
            "         Feature  Coefficient\n",
            "7     City_Paris    -0.025975\n",
            "6  City_New York    -0.029408\n",
            "4    Gender_Male    -0.089310\n",
            "5    City_London    -0.174603\n",
            "2   Feature_Num3    -0.329593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "Regression. Print the best parameters and accuracy."
      ],
      "metadata": {
        "id": "WQWy3M-yuoKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# --- 1. Create a dummy dataset for demonstration ---\n",
        "# This dataset is designed to be versatile enough for practical questions.\n",
        "# It includes numerical, categorical features, and some missing values.\n",
        "np.random.seed(42) # for reproducibility\n",
        "N_SAMPLES = 500\n",
        "\n",
        "# Numerical features\n",
        "feature_num1 = np.random.rand(N_SAMPLES) * 100\n",
        "feature_num2 = np.random.randn(N_SAMPLES) * 15 + 50\n",
        "feature_num3 = np.random.randint(0, 10, N_SAMPLES)\n",
        "\n",
        "# Categorical features\n",
        "gender = np.random.choice(['Male', 'Female'], N_SAMPLES, p=[0.5, 0.5])\n",
        "city = np.random.choice(['New York', 'London', 'Paris', 'Tokyo'], N_SAMPLES, p=[0.4, 0.3, 0.2, 0.1])\n",
        "\n",
        "# Target variable (binary classification)\n",
        "linear_combination = (0.5 * feature_num1 + 0.8 * feature_num2 - 0.3 * feature_num3 +\n",
        "                      (1 if gender[0] == 'Male' else 0) * 10 +\n",
        "                      (5 if city[0] == 'New York' else 0) +\n",
        "                      np.random.randn(N_SAMPLES) * 20)\n",
        "binary_target = (linear_combination > np.percentile(linear_combination, 50)).astype(int)\n",
        "\n",
        "# Introduce missing values for demonstration\n",
        "missing_indices_num1 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.05), replace=False)\n",
        "missing_indices_num2 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.03), replace=False)\n",
        "feature_num1[missing_indices_num1] = np.nan\n",
        "feature_num2[missing_indices_num2] = np.nan\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Feature_Num1': feature_num1,\n",
        "    'Feature_Num2': feature_num2,\n",
        "    'Feature_Num3': feature_num3,\n",
        "    'Gender': gender,\n",
        "    'City': city,\n",
        "    'Target': binary_target\n",
        "})\n",
        "\n",
        "print(\"--- Dummy Dataset Head ---\")\n",
        "print(data.head())\n",
        "print(\"\\n--- Dummy Dataset Info ---\")\n",
        "data.info()\n",
        "print(\"\\n--- Target Variable Distribution ---\")\n",
        "print(data['Target'].value_counts())\n",
        "\n",
        "# --- 2. Separate features (X) and target (y) ---\n",
        "X = data[['Feature_Num1', 'Feature_Num2', 'Feature_Num3', 'Gender', 'City']]\n",
        "y = data['Target']\n",
        "\n",
        "# --- 3. Identify numerical and categorical columns for preprocessing ---\n",
        "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# --- 4. Create a preprocessing pipeline ---\n",
        "# This pipeline will handle missing values, scaling, and one-hot encoding.\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')), # Impute missing numerical values with mean\n",
        "    ('scaler', StandardScaler())                  # Scale numerical features\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), # Impute missing categorical values with mode\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encode categorical features\n",
        "])\n",
        "\n",
        "# Create a preprocessor using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# --- 5. Split the dataset into training and testing sets ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "print(f\"Training target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Testing target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
        "\n",
        "# --- Practical Question 6: Apply GridSearchCV to tune hyperparameters (C and penalty) ---\n",
        "print(\"\\n--- Question 6: Hyperparameter Tuning with GridSearchCV ---\")\n",
        "\n",
        "# Define the Logistic Regression model within a pipeline, as GridSearchCV works well with pipelines.\n",
        "# The 'classifier__' prefix is used to specify parameters for the 'classifier' step in the pipeline.\n",
        "pipeline_grid = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                ('classifier', LogisticRegression(random_state=42, max_iter=1000))])\n",
        "\n",
        "# Define the parameter grid to search.\n",
        "# 'C': Inverse of regularization strength; smaller values specify stronger regularization.\n",
        "# 'penalty': 'l1' for Lasso, 'l2' for Ridge.\n",
        "# 'solver': 'liblinear' is chosen because it supports both 'l1' and 'l2' penalties.\n",
        "param_grid = {\n",
        "    'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
        "    'classifier__penalty': ['l1', 'l2'],\n",
        "    'classifier__solver': ['liblinear'] # Ensure solver supports the chosen penalties\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV.\n",
        "# - estimator: The pipeline we want to tune.\n",
        "# - param_grid: The dictionary of hyperparameters to search.\n",
        "# - cv: Cross-validation strategy. StratifiedKFold is good for classification to preserve class ratios.\n",
        "# - scoring: Metric to optimize (e.g., 'accuracy').\n",
        "# - n_jobs: Number of CPU cores to use (-1 means all available cores).\n",
        "# - verbose: Controls the verbosity of the output.\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=pipeline_grid,\n",
        "    param_grid=param_grid,\n",
        "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), # 5-fold stratified cross-validation\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=2 # Set to 2 for more detailed output during search\n",
        ")\n",
        "\n",
        "# Fit GridSearchCV to the training data. This will perform the search.\n",
        "print(\"\\nStarting GridSearchCV to find best hyperparameters...\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(\"GridSearchCV complete.\")\n",
        "\n",
        "# Print the best parameters found by GridSearchCV\n",
        "print(f\"\\nBest parameters found by GridSearchCV: {grid_search.best_params_}\")\n",
        "\n",
        "# Print the best cross-validation accuracy achieved with these parameters\n",
        "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate the best model (best_estimator_) on the unseen test set\n",
        "y_pred_best_grid = grid_search.best_estimator_.predict(X_test)\n",
        "test_accuracy_best_grid = accuracy_score(y_test, y_pred_best_grid)\n",
        "print(f\"Test accuracy with the best GridSearchCV model: {test_accuracy_best_grid:.4f}\")\n",
        "\n",
        "# Optional: Print the full results of the grid search\n",
        "# print(\"\\n--- Full GridSearchCV Results ---\")\n",
        "# print(pd.DataFrame(grid_search.cv_results_).sort_values(by='rank_test_score').head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCRR34CLuvFK",
        "outputId": "e571dc5f-96b7-4d6f-f033-2a0419fb9293"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dummy Dataset Head ---\n",
            "   Feature_Num1  Feature_Num2  Feature_Num3  Gender      City  Target\n",
            "0     37.454012     55.126340             0  Female  New York       0\n",
            "1     95.071431     78.142563             5  Female     Paris       1\n",
            "2     73.199394     64.256358             7    Male  New York       0\n",
            "3     59.865848     41.346445             2    Male  New York       0\n",
            "4     15.601864     36.523780             7  Female     Tokyo       0\n",
            "\n",
            "--- Dummy Dataset Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 6 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Feature_Num1  475 non-null    float64\n",
            " 1   Feature_Num2  485 non-null    float64\n",
            " 2   Feature_Num3  500 non-null    int64  \n",
            " 3   Gender        500 non-null    object \n",
            " 4   City          500 non-null    object \n",
            " 5   Target        500 non-null    int64  \n",
            "dtypes: float64(2), int64(2), object(2)\n",
            "memory usage: 23.6+ KB\n",
            "\n",
            "--- Target Variable Distribution ---\n",
            "Target\n",
            "0    250\n",
            "1    250\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training set size: 400 samples\n",
            "Testing set size: 100 samples\n",
            "Training target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "Testing target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "--- Question 6: Hyperparameter Tuning with GridSearchCV ---\n",
            "\n",
            "Starting GridSearchCV to find best hyperparameters...\n",
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "GridSearchCV complete.\n",
            "\n",
            "Best parameters found by GridSearchCV: {'classifier__C': 0.1, 'classifier__penalty': 'l1', 'classifier__solver': 'liblinear'}\n",
            "Best cross-validation accuracy: 0.7300\n",
            "Test accuracy with the best GridSearchCV model: 0.7900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.  Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "average accuracy."
      ],
      "metadata": {
        "id": "Mn9ZZGqau9bP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# --- 1. Create a dummy dataset for demonstration ---\n",
        "# This dataset will be used for cross-validation.\n",
        "np.random.seed(42) # for reproducibility\n",
        "N_SAMPLES = 500\n",
        "\n",
        "# Numerical features\n",
        "feature_num1 = np.random.rand(N_SAMPLES) * 100\n",
        "feature_num2 = np.random.randn(N_SAMPLES) * 15 + 50\n",
        "feature_num3 = np.random.randint(0, 10, N_SAMPLES)\n",
        "\n",
        "# Categorical features\n",
        "gender = np.random.choice(['Male', 'Female'], N_SAMPLES, p=[0.5, 0.5])\n",
        "city = np.random.choice(['New York', 'London', 'Paris', 'Tokyo'], N_SAMPLES, p=[0.4, 0.3, 0.2, 0.1])\n",
        "\n",
        "# Target variable (binary classification)\n",
        "linear_combination = (0.5 * feature_num1 + 0.8 * feature_num2 - 0.3 * feature_num3 +\n",
        "                      (1 if gender[0] == 'Male' else 0) * 10 +\n",
        "                      (5 if city[0] == 'New York' else 0) +\n",
        "                      np.random.randn(N_SAMPLES) * 20)\n",
        "binary_target = (linear_combination > np.percentile(linear_combination, 50)).astype(int)\n",
        "\n",
        "# Introduce some missing values for demonstration of preprocessing\n",
        "missing_indices_num1 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.05), replace=False)\n",
        "missing_indices_num2 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.03), replace=False)\n",
        "feature_num1[missing_indices_num1] = np.nan\n",
        "feature_num2[missing_indices_num2] = np.nan\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Feature_Num1': feature_num1,\n",
        "    'Feature_Num2': feature_num2,\n",
        "    'Feature_Num3': feature_num3,\n",
        "    'Gender': gender,\n",
        "    'City': city,\n",
        "    'Target': binary_target\n",
        "})\n",
        "\n",
        "print(\"--- Dummy Dataset Head ---\")\n",
        "print(data.head())\n",
        "print(\"\\n--- Dummy Dataset Info ---\")\n",
        "data.info()\n",
        "print(\"\\n--- Target Variable Distribution ---\")\n",
        "print(data['Target'].value_counts())\n",
        "\n",
        "# --- 2. Separate features (X) and target (y) ---\n",
        "X = data[['Feature_Num1', 'Feature_Num2', 'Feature_Num3', 'Gender', 'City']]\n",
        "y = data['Target']\n",
        "\n",
        "# --- 3. Identify numerical and categorical columns for preprocessing ---\n",
        "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# --- 4. Create a preprocessing pipeline ---\n",
        "# This pipeline will handle missing values, scaling, and one-hot encoding.\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')), # Impute missing numerical values with mean\n",
        "    ('scaler', StandardScaler())                  # Scale numerical features\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), # Impute missing categorical values with mode\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encode categorical features\n",
        "])\n",
        "\n",
        "# Create a preprocessor using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# --- Practical Question 7: Evaluate Logistic Regression using Stratified K-Fold Cross-Validation ---\n",
        "print(\"\\n--- Question 7: Stratified K-Fold Cross-Validation ---\")\n",
        "\n",
        "# Define the Logistic Regression model within a pipeline.\n",
        "# This ensures that preprocessing (imputation, scaling, encoding) is applied correctly\n",
        "# within each fold of the cross-validation.\n",
        "pipeline_cv = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                              ('classifier', LogisticRegression(random_state=42, max_iter=500))])\n",
        "\n",
        "# Initialize Stratified K-Fold Cross-Validation.\n",
        "# - n_splits: The number of folds (e.g., 5-fold cross-validation).\n",
        "# - shuffle: Whether to shuffle the data before splitting into batches. Recommended for randomness.\n",
        "# - random_state: For reproducibility of the shuffle.\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# List to store accuracy scores for each fold\n",
        "accuracies_per_fold = []\n",
        "\n",
        "# Perform cross-validation\n",
        "print(f\"\\nPerforming {skf.n_splits}-fold Stratified Cross-Validation...\")\n",
        "# skf.split(X, y) generates indices for training and validation sets for each fold.\n",
        "for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
        "    print(f\"\\n--- Fold {fold + 1}/{skf.n_splits} ---\")\n",
        "\n",
        "    # Split data into training and validation sets for the current fold\n",
        "    X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
        "    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "    print(f\"  Training samples: {len(X_train_fold)}\")\n",
        "    print(f\"  Validation samples: {len(X_val_fold)}\")\n",
        "    print(f\"  Training target distribution:\\n{y_train_fold.value_counts(normalize=True)}\")\n",
        "    print(f\"  Validation target distribution:\\n{y_val_fold.value_counts(normalize=True)}\")\n",
        "\n",
        "    # Train the pipeline (which includes preprocessing and Logistic Regression) on the training fold\n",
        "    print(\"  Training model for current fold...\")\n",
        "    pipeline_cv.fit(X_train_fold, y_train_fold)\n",
        "    print(\"  Model training for fold complete.\")\n",
        "\n",
        "    # Make predictions on the validation fold\n",
        "    y_pred_fold = pipeline_cv.predict(X_val_fold)\n",
        "\n",
        "    # Calculate accuracy for the current fold\n",
        "    fold_accuracy = accuracy_score(y_val_fold, y_pred_fold)\n",
        "    accuracies_per_fold.append(fold_accuracy)\n",
        "    print(f\"  Accuracy for Fold {fold + 1}: {fold_accuracy:.4f}\")\n",
        "\n",
        "# Calculate and print the average accuracy across all folds\n",
        "average_accuracy_cv = np.mean(accuracies_per_fold)\n",
        "std_dev_accuracy_cv = np.std(accuracies_per_fold) # Also good to report standard deviation\n",
        "\n",
        "print(\"\\n--- Cross-Validation Results ---\")\n",
        "print(f\"Accuracies per fold: {[f'{acc:.4f}' for acc in accuracies_per_fold]}\")\n",
        "print(f\"Average Accuracy across all folds: {average_accuracy_cv:.4f}\")\n",
        "print(f\"Standard Deviation of Accuracy: {std_dev_accuracy_cv:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdMkr6yLu-r2",
        "outputId": "e7978c26-3a14-4013-a44b-afe7a45f68bd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dummy Dataset Head ---\n",
            "   Feature_Num1  Feature_Num2  Feature_Num3  Gender      City  Target\n",
            "0     37.454012     55.126340             0  Female  New York       0\n",
            "1     95.071431     78.142563             5  Female     Paris       1\n",
            "2     73.199394     64.256358             7    Male  New York       0\n",
            "3     59.865848     41.346445             2    Male  New York       0\n",
            "4     15.601864     36.523780             7  Female     Tokyo       0\n",
            "\n",
            "--- Dummy Dataset Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 6 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Feature_Num1  475 non-null    float64\n",
            " 1   Feature_Num2  485 non-null    float64\n",
            " 2   Feature_Num3  500 non-null    int64  \n",
            " 3   Gender        500 non-null    object \n",
            " 4   City          500 non-null    object \n",
            " 5   Target        500 non-null    int64  \n",
            "dtypes: float64(2), int64(2), object(2)\n",
            "memory usage: 23.6+ KB\n",
            "\n",
            "--- Target Variable Distribution ---\n",
            "Target\n",
            "0    250\n",
            "1    250\n",
            "Name: count, dtype: int64\n",
            "\n",
            "--- Question 7: Stratified K-Fold Cross-Validation ---\n",
            "\n",
            "Performing 5-fold Stratified Cross-Validation...\n",
            "\n",
            "--- Fold 1/5 ---\n",
            "  Training samples: 400\n",
            "  Validation samples: 100\n",
            "  Training target distribution:\n",
            "Target\n",
            "0    0.5\n",
            "1    0.5\n",
            "Name: proportion, dtype: float64\n",
            "  Validation target distribution:\n",
            "Target\n",
            "0    0.5\n",
            "1    0.5\n",
            "Name: proportion, dtype: float64\n",
            "  Training model for current fold...\n",
            "  Model training for fold complete.\n",
            "  Accuracy for Fold 1: 0.6800\n",
            "\n",
            "--- Fold 2/5 ---\n",
            "  Training samples: 400\n",
            "  Validation samples: 100\n",
            "  Training target distribution:\n",
            "Target\n",
            "0    0.5\n",
            "1    0.5\n",
            "Name: proportion, dtype: float64\n",
            "  Validation target distribution:\n",
            "Target\n",
            "0    0.5\n",
            "1    0.5\n",
            "Name: proportion, dtype: float64\n",
            "  Training model for current fold...\n",
            "  Model training for fold complete.\n",
            "  Accuracy for Fold 2: 0.7600\n",
            "\n",
            "--- Fold 3/5 ---\n",
            "  Training samples: 400\n",
            "  Validation samples: 100\n",
            "  Training target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "  Validation target distribution:\n",
            "Target\n",
            "0    0.5\n",
            "1    0.5\n",
            "Name: proportion, dtype: float64\n",
            "  Training model for current fold...\n",
            "  Model training for fold complete.\n",
            "  Accuracy for Fold 3: 0.7400\n",
            "\n",
            "--- Fold 4/5 ---\n",
            "  Training samples: 400\n",
            "  Validation samples: 100\n",
            "  Training target distribution:\n",
            "Target\n",
            "0    0.5\n",
            "1    0.5\n",
            "Name: proportion, dtype: float64\n",
            "  Validation target distribution:\n",
            "Target\n",
            "0    0.5\n",
            "1    0.5\n",
            "Name: proportion, dtype: float64\n",
            "  Training model for current fold...\n",
            "  Model training for fold complete.\n",
            "  Accuracy for Fold 4: 0.7100\n",
            "\n",
            "--- Fold 5/5 ---\n",
            "  Training samples: 400\n",
            "  Validation samples: 100\n",
            "  Training target distribution:\n",
            "Target\n",
            "0    0.5\n",
            "1    0.5\n",
            "Name: proportion, dtype: float64\n",
            "  Validation target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "  Training model for current fold...\n",
            "  Model training for fold complete.\n",
            "  Accuracy for Fold 5: 0.7500\n",
            "\n",
            "--- Cross-Validation Results ---\n",
            "Accuracies per fold: ['0.6800', '0.7600', '0.7400', '0.7100', '0.7500']\n",
            "Average Accuracy across all folds: 0.7280\n",
            "Standard Deviation of Accuracy: 0.0293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "accuracy."
      ],
      "metadata": {
        "id": "Og6MMOxTvRht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import os # For file operations\n",
        "\n",
        "# --- 1. Create a Dummy CSV File for Demonstration ---\n",
        "# In a real scenario, you would already have your CSV file.\n",
        "# This step is just to make the example runnable without an external file.\n",
        "csv_filename = 'sample_classification_data.csv'\n",
        "N_SAMPLES = 500\n",
        "np.random.seed(42) # for reproducibility\n",
        "\n",
        "# Generate synthetic data\n",
        "feature_num1 = np.random.rand(N_SAMPLES) * 100\n",
        "feature_num2 = np.random.randn(N_SAMPLES) * 15 + 50\n",
        "feature_num3 = np.random.randint(0, 10, N_SAMPLES)\n",
        "gender = np.random.choice(['Male', 'Female'], N_SAMPLES, p=[0.5, 0.5])\n",
        "city = np.random.choice(['New York', 'London', 'Paris', 'Tokyo'], N_SAMPLES, p=[0.4, 0.3, 0.2, 0.1])\n",
        "\n",
        "# Introduce some missing values\n",
        "missing_indices_num1 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.05), replace=False)\n",
        "missing_indices_num2 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.03), replace=False)\n",
        "feature_num1[missing_indices_num1] = np.nan\n",
        "feature_num2[missing_indices_num2] = np.nan\n",
        "\n",
        "# Create a binary target based on a linear combination\n",
        "linear_combination = (0.5 * feature_num1 + 0.8 * feature_num2 - 0.3 * feature_num3 +\n",
        "                      (1 if gender[0] == 'Male' else 0) * 10 +\n",
        "                      (5 if city[0] == 'New York' else 0) +\n",
        "                      np.random.randn(N_SAMPLES) * 20)\n",
        "target = (linear_combination > np.percentile(linear_combination[~np.isnan(linear_combination)], 50)).astype(int) # Handle NaN in percentile\n",
        "\n",
        "# Create DataFrame and save to CSV\n",
        "dummy_df = pd.DataFrame({\n",
        "    'Numerical_Feature_A': feature_num1,\n",
        "    'Numerical_Feature_B': feature_num2,\n",
        "    'Numerical_Feature_C': feature_num3,\n",
        "    'Categorical_Gender': gender,\n",
        "    'Categorical_City': city,\n",
        "    'Target_Variable': target\n",
        "})\n",
        "dummy_df.to_csv(csv_filename, index=False)\n",
        "print(f\"Dummy CSV file '{csv_filename}' created successfully.\")\n",
        "\n",
        "# --- Practical Question 8: Load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy ---\n",
        "print(f\"\\n--- Question 8: Load Dataset from '{csv_filename}', Apply Logistic Regression, and Evaluate Accuracy ---\")\n",
        "\n",
        "# --- 2. Load the dataset from the CSV file ---\n",
        "try:\n",
        "    df = pd.read_csv(csv_filename)\n",
        "    print(f\"\\nDataset loaded successfully from '{csv_filename}'.\")\n",
        "    print(\"\\n--- Loaded Dataset Head ---\")\n",
        "    print(df.head())\n",
        "    print(\"\\n--- Loaded Dataset Info ---\")\n",
        "    df.info()\n",
        "    print(\"\\n--- Target Variable Distribution ---\")\n",
        "    print(df['Target_Variable'].value_counts())\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{csv_filename}' was not found. Please ensure it's in the correct directory.\")\n",
        "    exit() # Exit if the file isn't found\n",
        "\n",
        "# --- 3. Separate features (X) and target (y) ---\n",
        "# Assuming 'Target_Variable' is the column to predict\n",
        "X = df.drop('Target_Variable', axis=1)\n",
        "y = df['Target_Variable']\n",
        "\n",
        "# --- 4. Identify numerical and categorical columns for preprocessing ---\n",
        "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# --- 5. Create a preprocessing pipeline ---\n",
        "# This pipeline will handle missing values, scaling, and one-hot encoding.\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')), # Impute missing numerical values with mean\n",
        "    ('scaler', StandardScaler())                  # Scale numerical features\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), # Impute missing categorical values with mode\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encode categorical features\n",
        "])\n",
        "\n",
        "# Create a preprocessor using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# --- 6. Split the dataset into training and testing sets ---\n",
        "# Use stratify=y to ensure class distribution is preserved in splits, important for classification.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "print(f\"Training target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Testing target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
        "\n",
        "# --- 7. Initialize and train the Logistic Regression model within a Pipeline ---\n",
        "# The pipeline ensures that preprocessing steps are applied consistently\n",
        "# to both training and testing data.\n",
        "model_q8 = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', LogisticRegression(random_state=42, max_iter=500))])\n",
        "\n",
        "print(\"\\nTraining Logistic Regression model...\")\n",
        "model_q8.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# --- 8. Make predictions on the test set ---\n",
        "y_pred_q8 = model_q8.predict(X_test)\n",
        "\n",
        "# --- 9. Evaluate the model performance using accuracy ---\n",
        "accuracy_q8 = accuracy_score(y_test, y_pred_q8)\n",
        "\n",
        "# --- 10. Print the model accuracy ---\n",
        "print(f\"\\nModel Accuracy on the test set: {accuracy_q8:.4f}\")\n",
        "\n",
        "# Optional: Clean up the dummy CSV file\n",
        "try:\n",
        "    os.remove(csv_filename)\n",
        "    print(f\"\\nDummy CSV file '{csv_filename}' removed.\")\n",
        "except OSError as e:\n",
        "    print(f\"Error removing dummy CSV file: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zby_D2U0vSnT",
        "outputId": "f962aefe-6f48-4aa5-81b9-191c67245b3f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy CSV file 'sample_classification_data.csv' created successfully.\n",
            "\n",
            "--- Question 8: Load Dataset from 'sample_classification_data.csv', Apply Logistic Regression, and Evaluate Accuracy ---\n",
            "\n",
            "Dataset loaded successfully from 'sample_classification_data.csv'.\n",
            "\n",
            "--- Loaded Dataset Head ---\n",
            "   Numerical_Feature_A  Numerical_Feature_B  Numerical_Feature_C  \\\n",
            "0            37.454012            55.126340                    0   \n",
            "1                  NaN            78.142563                    5   \n",
            "2            73.199394            64.256358                    7   \n",
            "3            59.865848            41.346445                    2   \n",
            "4            15.601864            36.523780                    7   \n",
            "\n",
            "  Categorical_Gender Categorical_City  Target_Variable  \n",
            "0             Female         New York                1  \n",
            "1             Female            Paris                0  \n",
            "2               Male         New York                1  \n",
            "3               Male         New York                0  \n",
            "4             Female            Tokyo                0  \n",
            "\n",
            "--- Loaded Dataset Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 6 columns):\n",
            " #   Column               Non-Null Count  Dtype  \n",
            "---  ------               --------------  -----  \n",
            " 0   Numerical_Feature_A  475 non-null    float64\n",
            " 1   Numerical_Feature_B  485 non-null    float64\n",
            " 2   Numerical_Feature_C  500 non-null    int64  \n",
            " 3   Categorical_Gender   500 non-null    object \n",
            " 4   Categorical_City     500 non-null    object \n",
            " 5   Target_Variable      500 non-null    int64  \n",
            "dtypes: float64(2), int64(2), object(2)\n",
            "memory usage: 23.6+ KB\n",
            "\n",
            "--- Target Variable Distribution ---\n",
            "Target_Variable\n",
            "0    270\n",
            "1    230\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training set size: 400 samples\n",
            "Testing set size: 100 samples\n",
            "Training target distribution:\n",
            "Target_Variable\n",
            "0    0.54\n",
            "1    0.46\n",
            "Name: proportion, dtype: float64\n",
            "Testing target distribution:\n",
            "Target_Variable\n",
            "0    0.54\n",
            "1    0.46\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Training Logistic Regression model...\n",
            "Model training complete.\n",
            "\n",
            "Model Accuracy on the test set: 0.7200\n",
            "\n",
            "Dummy CSV file 'sample_classification_data.csv' removed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "Logistic Regression. Print the best parameters and accuracy."
      ],
      "metadata": {
        "id": "OTbRjazgvfKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from scipy.stats import uniform, loguniform # For defining parameter distributions\n",
        "\n",
        "# --- 1. Create a dummy dataset for demonstration ---\n",
        "# This dataset is designed to be versatile enough for practical questions.\n",
        "# It includes numerical, categorical features, and some missing values.\n",
        "np.random.seed(42) # for reproducibility\n",
        "N_SAMPLES = 500\n",
        "\n",
        "# Numerical features\n",
        "feature_num1 = np.random.rand(N_SAMPLES) * 100\n",
        "feature_num2 = np.random.randn(N_SAMPLES) * 15 + 50\n",
        "feature_num3 = np.random.randint(0, 10, N_SAMPLES)\n",
        "\n",
        "# Categorical features\n",
        "gender = np.random.choice(['Male', 'Female'], N_SAMPLES, p=[0.5, 0.5])\n",
        "city = np.random.choice(['New York', 'London', 'Paris', 'Tokyo'], N_SAMPLES, p=[0.4, 0.3, 0.2, 0.1])\n",
        "\n",
        "# Target variable (binary classification)\n",
        "linear_combination = (0.5 * feature_num1 + 0.8 * feature_num2 - 0.3 * feature_num3 +\n",
        "                      (1 if gender[0] == 'Male' else 0) * 10 +\n",
        "                      (5 if city[0] == 'New York' else 0) +\n",
        "                      np.random.randn(N_SAMPLES) * 20)\n",
        "binary_target = (linear_combination > np.percentile(linear_combination, 50)).astype(int)\n",
        "\n",
        "# Introduce missing values for demonstration\n",
        "missing_indices_num1 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.05), replace=False)\n",
        "missing_indices_num2 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.03), replace=False)\n",
        "feature_num1[missing_indices_num1] = np.nan\n",
        "feature_num2[missing_indices_num2] = np.nan\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Feature_Num1': feature_num1,\n",
        "    'Feature_Num2': feature_num2,\n",
        "    'Feature_Num3': feature_num3,\n",
        "    'Gender': gender,\n",
        "    'City': city,\n",
        "    'Target': binary_target\n",
        "})\n",
        "\n",
        "print(\"--- Dummy Dataset Head ---\")\n",
        "print(data.head())\n",
        "print(\"\\n--- Dummy Dataset Info ---\")\n",
        "data.info()\n",
        "print(\"\\n--- Target Variable Distribution ---\")\n",
        "print(data['Target'].value_counts())\n",
        "\n",
        "# --- 2. Separate features (X) and target (y) ---\n",
        "X = data[['Feature_Num1', 'Feature_Num2', 'Feature_Num3', 'Gender', 'City']]\n",
        "y = data['Target']\n",
        "\n",
        "# --- 3. Identify numerical and categorical columns for preprocessing ---\n",
        "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# --- 4. Create a preprocessing pipeline ---\n",
        "# This pipeline will handle missing values, scaling, and one-hot encoding.\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')), # Impute missing numerical values with mean\n",
        "    ('scaler', StandardScaler())                  # Scale numerical features\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), # Impute missing categorical values with mode\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encode categorical features\n",
        "])\n",
        "\n",
        "# Create a preprocessor using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# --- 5. Split the dataset into training and testing sets ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "print(f\"Training target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Testing target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
        "\n",
        "# --- Practical Question 9: Apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) ---\n",
        "print(\"\\n--- Question 9: Hyperparameter Tuning with RandomizedSearchCV ---\")\n",
        "\n",
        "# Create a pipeline for RandomizedSearchCV.\n",
        "# The 'classifier__' prefix is used for parameters of the LogisticRegression step.\n",
        "pipeline_random = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                  ('classifier', LogisticRegression(random_state=42, max_iter=1000))])\n",
        "\n",
        "# Define the parameter distributions to sample from.\n",
        "# 'C': Use loguniform for C as it typically spans several orders of magnitude.\n",
        "# 'penalty': A list of categorical choices.\n",
        "# 'solver': A list of categorical choices. Ensure solvers support the chosen penalties.\n",
        "# 'l1_ratio': Only relevant when penalty is 'elasticnet'. Use uniform distribution.\n",
        "param_distributions = {\n",
        "    'classifier__C': loguniform(0.001, 100), # C values from 0.001 to 100 (log-scaled)\n",
        "    'classifier__penalty': ['l1', 'l2', 'elasticnet'],\n",
        "    'classifier__solver': ['liblinear', 'saga'], # 'liblinear' supports 'l1' and 'l2'; 'saga' supports all\n",
        "    'classifier__l1_ratio': uniform(0, 1) # l1_ratio from 0 to 1 (only used with elasticnet)\n",
        "}\n",
        "\n",
        "# Initialize RandomizedSearchCV.\n",
        "# - estimator: The pipeline we want to tune.\n",
        "# - param_distributions: The dictionary of parameter distributions.\n",
        "# - n_iter: Number of parameter settings that are sampled. More iterations mean a more exhaustive search.\n",
        "# - cv: Cross-validation strategy. StratifiedKFold is recommended for classification.\n",
        "# - scoring: Metric to optimize (e.g., 'accuracy').\n",
        "# - n_jobs: Number of CPU cores to use (-1 means all available cores).\n",
        "# - random_state: For reproducibility of the random sampling.\n",
        "# - verbose: Controls the verbosity of the output.\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=pipeline_random,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=50, # Number of different parameter combinations to try\n",
        "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), # 5-fold stratified cross-validation\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=2 # Set to 2 for more detailed output during search\n",
        ")\n",
        "\n",
        "# Fit RandomizedSearchCV to the training data. This will perform the search.\n",
        "print(\"\\nStarting RandomizedSearchCV to find best hyperparameters...\")\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"RandomizedSearchCV complete.\")\n",
        "\n",
        "# Print the best parameters found by RandomizedSearchCV\n",
        "print(f\"\\nBest parameters found by RandomizedSearchCV: {random_search.best_params_}\")\n",
        "\n",
        "# Print the best cross-validation accuracy achieved with these parameters\n",
        "print(f\"Best cross-validation accuracy: {random_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate the best model (best_estimator_) on the unseen test set\n",
        "y_pred_best_random = random_search.best_estimator_.predict(X_test)\n",
        "test_accuracy_best_random = accuracy_score(y_test, y_pred_best_random)\n",
        "print(f\"Test accuracy with the best RandomizedSearchCV model: {test_accuracy_best_random:.4f}\")\n",
        "\n",
        "# Optional: Print the full results of the randomized search\n",
        "# print(\"\\n--- Full RandomizedSearchCV Results ---\")\n",
        "# print(pd.DataFrame(random_search.cv_results_).sort_values(by='rank_test_score').head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZrnHOVPvgeP",
        "outputId": "b7040066-eda3-4e18-a589-fb47cac2b090"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dummy Dataset Head ---\n",
            "   Feature_Num1  Feature_Num2  Feature_Num3  Gender      City  Target\n",
            "0     37.454012     55.126340             0  Female  New York       0\n",
            "1     95.071431     78.142563             5  Female     Paris       1\n",
            "2     73.199394     64.256358             7    Male  New York       0\n",
            "3     59.865848     41.346445             2    Male  New York       0\n",
            "4     15.601864     36.523780             7  Female     Tokyo       0\n",
            "\n",
            "--- Dummy Dataset Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 6 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Feature_Num1  475 non-null    float64\n",
            " 1   Feature_Num2  485 non-null    float64\n",
            " 2   Feature_Num3  500 non-null    int64  \n",
            " 3   Gender        500 non-null    object \n",
            " 4   City          500 non-null    object \n",
            " 5   Target        500 non-null    int64  \n",
            "dtypes: float64(2), int64(2), object(2)\n",
            "memory usage: 23.6+ KB\n",
            "\n",
            "--- Target Variable Distribution ---\n",
            "Target\n",
            "0    250\n",
            "1    250\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training set size: 400 samples\n",
            "Testing set size: 100 samples\n",
            "Training target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "Testing target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "--- Question 9: Hyperparameter Tuning with RandomizedSearchCV ---\n",
            "\n",
            "Starting RandomizedSearchCV to find best hyperparameters...\n",
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
            "RandomizedSearchCV complete.\n",
            "\n",
            "Best parameters found by RandomizedSearchCV: {'classifier__C': np.float64(0.0745934328572655), 'classifier__l1_ratio': np.float64(0.9507143064099162), 'classifier__penalty': 'elasticnet', 'classifier__solver': 'saga'}\n",
            "Best cross-validation accuracy: 0.7300\n",
            "Test accuracy with the best RandomizedSearchCV model: 0.7900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "65 fits failed out of a total of 250.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "65 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\", line 662, in fit\n",
            "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.73      nan 0.5    0.7025 0.5    0.705  0.705  0.5       nan 0.5\n",
            " 0.71   0.7225 0.7075 0.7025 0.7075 0.7125 0.7125 0.71      nan 0.705\n",
            " 0.7125    nan    nan 0.71   0.7125 0.5       nan    nan 0.7125 0.7025\n",
            "    nan 0.7025    nan    nan 0.5    0.71   0.705  0.7125    nan 0.7025\n",
            "    nan 0.715  0.705  0.695  0.7025 0.5    0.7       nan 0.7025 0.7075]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy."
      ],
      "metadata": {
        "id": "TrCChhivvtCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.multiclass import OneVsOneClassifier # Import OneVsOneClassifier\n",
        "\n",
        "# --- 1. Create a dummy dataset for demonstration ---\n",
        "# This dataset will have a multiclass target variable for OvO classification.\n",
        "np.random.seed(42) # for reproducibility\n",
        "N_SAMPLES = 500\n",
        "\n",
        "# Numerical features\n",
        "feature_num1 = np.random.rand(N_SAMPLES) * 100\n",
        "feature_num2 = np.random.randn(N_SAMPLES) * 15 + 50\n",
        "feature_num3 = np.random.randint(0, 10, N_SAMPLES)\n",
        "\n",
        "# Categorical features\n",
        "gender = np.random.choice(['Male', 'Female'], N_SAMPLES, p=[0.5, 0.5])\n",
        "city = np.random.choice(['New York', 'London', 'Paris', 'Tokyo'], N_SAMPLES, p=[0.4, 0.3, 0.2, 0.1])\n",
        "\n",
        "# Multiclass Target variable: Let's create 3 distinct classes (0, 1, 2)\n",
        "# based on different thresholds of a linear combination of features + noise.\n",
        "linear_combination = (0.5 * feature_num1 + 0.8 * feature_num2 - 0.3 * feature_num3 +\n",
        "                      (1 if gender[0] == 'Male' else 0) * 10 +\n",
        "                      (5 if city[0] == 'New York' else 0) +\n",
        "                      np.random.randn(N_SAMPLES) * 20)\n",
        "\n",
        "multi_target = np.zeros(N_SAMPLES, dtype=int)\n",
        "# Class 0: lowest values\n",
        "multi_target[linear_combination <= np.percentile(linear_combination, 33)] = 0\n",
        "# Class 1: middle values\n",
        "multi_target[(linear_combination > np.percentile(linear_combination, 33)) &\n",
        "             (linear_combination <= np.percentile(linear_combination, 66))] = 1\n",
        "# Class 2: highest values\n",
        "multi_target[linear_combination > np.percentile(linear_combination, 66)] = 2\n",
        "\n",
        "\n",
        "# Introduce some missing values for demonstration of preprocessing\n",
        "missing_indices_num1 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.05), replace=False)\n",
        "missing_indices_num2 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.03), replace=False)\n",
        "feature_num1[missing_indices_num1] = np.nan\n",
        "feature_num2[missing_indices_num2] = np.nan\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Feature_Num1': feature_num1,\n",
        "    'Feature_Num2': feature_num2,\n",
        "    'Feature_Num3': feature_num3,\n",
        "    'Gender': gender,\n",
        "    'City': city,\n",
        "    'Multi_Target': multi_target # Our multiclass target\n",
        "})\n",
        "\n",
        "print(\"--- Dummy Dataset Head ---\")\n",
        "print(data.head())\n",
        "print(\"\\n--- Dummy Dataset Info ---\")\n",
        "data.info()\n",
        "print(\"\\n--- Multiclass Target Variable Distribution ---\")\n",
        "print(data['Multi_Target'].value_counts())\n",
        "\n",
        "# --- 2. Separate features (X) and target (y) ---\n",
        "X = data[['Feature_Num1', 'Feature_Num2', 'Feature_Num3', 'Gender', 'City']]\n",
        "y = data['Multi_Target'] # Use the multiclass target for OvO\n",
        "\n",
        "# --- 3. Identify numerical and categorical columns for preprocessing ---\n",
        "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# --- 4. Create a preprocessing pipeline ---\n",
        "# This pipeline will handle missing values, scaling, and one-hot encoding.\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')), # Impute missing numerical values with mean\n",
        "    ('scaler', StandardScaler())                  # Scale numerical features\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), # Impute missing categorical values with mode\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encode categorical features\n",
        "])\n",
        "\n",
        "# Create a preprocessor using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# --- 5. Split the dataset into training and testing sets ---\n",
        "# For multiclass classification, it's crucial to use stratify=y to maintain\n",
        "# the class distribution in both training and testing sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "print(f\"Training target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Testing target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
        "\n",
        "# --- Practical Question 10: Implement One-vs-One (OvO) Multiclass Logistic Regression ---\n",
        "print(\"\\n--- Question 10: One-vs-One (OvO) Multiclass Logistic Regression ---\")\n",
        "\n",
        "# Create a pipeline where the preprocessor is followed by OneVsOneClassifier.\n",
        "# OneVsOneClassifier trains a binary classifier for each unique pair of classes.\n",
        "# For K classes, it trains K * (K - 1) / 2 binary classifiers.\n",
        "# The base_estimator is the LogisticRegression model.\n",
        "model_ovo = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', OneVsOneClassifier(\n",
        "        estimator=LogisticRegression(solver='lbfgs', random_state=42, max_iter=500)\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Train the One-vs-One model\n",
        "print(\"Training Multiclass Logistic Regression model with One-vs-One strategy...\")\n",
        "model_ovo.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_ovo = model_ovo.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "print(f\"Multiclass Model Accuracy (One-vs-One): {accuracy_ovo:.4f}\")\n",
        "\n",
        "# Optional: Print classification report for more detailed metrics per class\n",
        "print(\"\\n--- Classification Report (One-vs-One) ---\")\n",
        "print(classification_report(y_test, y_pred_ovo))\n",
        "\n",
        "# Note on coefficients for OvO:\n",
        "# OneVsOneClassifier trains multiple binary classifiers.\n",
        "# To inspect coefficients, you would access `model_ovo.named_steps['classifier'].estimators_`.\n",
        "# Each element in `estimators_` is a trained LogisticRegression model for a specific pair of classes.\n",
        "# Interpreting these can be more complex than OvR or Softmax, as each model has its own coefficients.\n",
        "print(\"\\n--- Note on Coefficients for One-vs-One ---\")\n",
        "print(\"One-vs-One trains multiple binary classifiers. Each classifier has its own set of coefficients.\")\n",
        "print(f\"Number of binary classifiers trained: {len(model_ovo.named_steps['classifier'].estimators_)}\")\n",
        "# Example of accessing the first binary classifier's coefficients:\n",
        "# print(model_ovo.named_steps['classifier'].estimators_[0].coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiFxCnkgvuGP",
        "outputId": "ffd1ff5b-58ae-444f-bc93-2e363e310611"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dummy Dataset Head ---\n",
            "   Feature_Num1  Feature_Num2  Feature_Num3  Gender      City  Multi_Target\n",
            "0     37.454012     55.126340             0  Female  New York             1\n",
            "1     95.071431     78.142563             5  Female     Paris             2\n",
            "2     73.199394     64.256358             7    Male  New York             1\n",
            "3     59.865848     41.346445             2    Male  New York             1\n",
            "4     15.601864     36.523780             7  Female     Tokyo             1\n",
            "\n",
            "--- Dummy Dataset Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 6 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Feature_Num1  475 non-null    float64\n",
            " 1   Feature_Num2  485 non-null    float64\n",
            " 2   Feature_Num3  500 non-null    int64  \n",
            " 3   Gender        500 non-null    object \n",
            " 4   City          500 non-null    object \n",
            " 5   Multi_Target  500 non-null    int64  \n",
            "dtypes: float64(2), int64(2), object(2)\n",
            "memory usage: 23.6+ KB\n",
            "\n",
            "--- Multiclass Target Variable Distribution ---\n",
            "Multi_Target\n",
            "2    170\n",
            "1    165\n",
            "0    165\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training set size: 400 samples\n",
            "Testing set size: 100 samples\n",
            "Training target distribution:\n",
            "Multi_Target\n",
            "2    0.34\n",
            "0    0.33\n",
            "1    0.33\n",
            "Name: proportion, dtype: float64\n",
            "Testing target distribution:\n",
            "Multi_Target\n",
            "2    0.34\n",
            "1    0.33\n",
            "0    0.33\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "--- Question 10: One-vs-One (OvO) Multiclass Logistic Regression ---\n",
            "Training Multiclass Logistic Regression model with One-vs-One strategy...\n",
            "Model training complete.\n",
            "Multiclass Model Accuracy (One-vs-One): 0.6000\n",
            "\n",
            "--- Classification Report (One-vs-One) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.58      0.60        33\n",
            "           1       0.48      0.48      0.48        33\n",
            "           2       0.68      0.74      0.70        34\n",
            "\n",
            "    accuracy                           0.60       100\n",
            "   macro avg       0.60      0.60      0.60       100\n",
            "weighted avg       0.60      0.60      0.60       100\n",
            "\n",
            "\n",
            "--- Note on Coefficients for One-vs-One ---\n",
            "One-vs-One trains multiple binary classifiers. Each classifier has its own set of coefficients.\n",
            "Number of binary classifiers trained: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "classification."
      ],
      "metadata": {
        "id": "lPmbQNonv5P8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns # For enhanced visualization\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# --- 1. Create a dummy dataset for binary classification ---\n",
        "np.random.seed(42) # for reproducibility\n",
        "N_SAMPLES = 500\n",
        "\n",
        "# Numerical features\n",
        "feature_num1 = np.random.rand(N_SAMPLES) * 100\n",
        "feature_num2 = np.random.randn(N_SAMPLES) * 15 + 50\n",
        "feature_num3 = np.random.randint(0, 10, N_SAMPLES)\n",
        "\n",
        "# Categorical features\n",
        "gender = np.random.choice(['Male', 'Female'], N_SAMPLES, p=[0.5, 0.5])\n",
        "city = np.random.choice(['New York', 'London', 'Paris', 'Tokyo'], N_SAMPLES, p=[0.4, 0.3, 0.2, 0.1])\n",
        "\n",
        "# Binary Target variable:\n",
        "linear_combination = (0.5 * feature_num1 + 0.8 * feature_num2 - 0.3 * feature_num3 +\n",
        "                      (1 if gender[0] == 'Male' else 0) * 10 +\n",
        "                      (5 if city[0] == 'New York' else 0) +\n",
        "                      np.random.randn(N_SAMPLES) * 20)\n",
        "binary_target = (linear_combination > np.percentile(linear_combination, 50)).astype(int)\n",
        "\n",
        "# Introduce some missing values for demonstration of preprocessing\n",
        "missing_indices_num1 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.05), replace=False)\n",
        "missing_indices_num2 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.03), replace=False)\n",
        "feature_num1[missing_indices_num1] = np.nan\n",
        "feature_num2[missing_indices_num2] = np.nan\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Feature_Num1': feature_num1,\n",
        "    'Feature_Num2': feature_num2,\n",
        "    'Feature_Num3': feature_num3,\n",
        "    'Gender': gender,\n",
        "    'City': city,\n",
        "    'Target': binary_target # Our binary target\n",
        "})\n",
        "\n",
        "print(\"--- Dummy Dataset Head ---\")\n",
        "print(data.head())\n",
        "print(\"\\n--- Dummy Dataset Info ---\")\n",
        "data.info()\n",
        "print(\"\\n--- Target Variable Distribution ---\")\n",
        "print(data['Target'].value_counts())\n",
        "\n",
        "# --- 2. Separate features (X) and target (y) ---\n",
        "X = data[['Feature_Num1', 'Feature_Num2', 'Feature_Num3', 'Gender', 'City']]\n",
        "y = data['Target'] # Use the binary target\n",
        "\n",
        "# --- 3. Identify numerical and categorical columns for preprocessing ---\n",
        "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# --- 4. Create a preprocessing pipeline ---\n",
        "# This pipeline will handle missing values, scaling, and one-hot encoding.\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')), # Impute missing numerical values with mean\n",
        "    ('scaler', StandardScaler())                  # Scale numerical features\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), # Impute missing categorical values with mode\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encode categorical features\n",
        "])\n",
        "\n",
        "# Create a preprocessor using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# --- 5. Split the dataset into training and testing sets ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "print(f\"Training target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Testing target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
        "\n",
        "# --- Practical Question 11: Train Logistic Regression and visualize the confusion matrix ---\n",
        "print(\"\\n--- Question 11: Visualize Confusion Matrix for Binary Classification ---\")\n",
        "\n",
        "# Create a pipeline for the Logistic Regression model\n",
        "model_q11 = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                            ('classifier', LogisticRegression(random_state=42, max_iter=500))])\n",
        "\n",
        "# Train the model\n",
        "print(\"Training Logistic Regression model...\")\n",
        "model_q11.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_q11 = model_q11.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy (optional, but good to see overall performance)\n",
        "accuracy_q11 = accuracy_score(y_test, y_pred_q11)\n",
        "print(f\"Model Accuracy: {accuracy_q11:.4f}\")\n",
        "\n",
        "# --- Generate the Confusion Matrix ---\n",
        "# A confusion matrix shows the number of correct and incorrect predictions,\n",
        "# broken down by each class.\n",
        "# Rows represent actual classes, columns represent predicted classes.\n",
        "# For binary classification (classes 0 and 1):\n",
        "# [[True Negatives (TN), False Positives (FP)]\n",
        "#  [False Negatives (FN), True Positives (TP)]]\n",
        "cm = confusion_matrix(y_test, y_pred_q11)\n",
        "print(\"\\n--- Raw Confusion Matrix ---\")\n",
        "print(cm)\n",
        "\n",
        "# --- Visualize the Confusion Matrix using Seaborn ---\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'],\n",
        "            linewidths=.5, linecolor='black') # Add lines for better separation\n",
        "plt.title('Confusion Matrix for Binary Classification')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# --- Visualize the Confusion Matrix using Scikit-learn's ConfusionMatrixDisplay (more modern) ---\n",
        "# This method is often preferred as it handles labels and plotting directly.\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "disp.plot(cmap=plt.cm.Blues, ax=ax, values_format='d') # 'd' for integer format\n",
        "ax.set_title('Confusion Matrix (using ConfusionMatrixDisplay)')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nConfusion matrix visualization complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N2D0LEM_v6Rp",
        "outputId": "72681337-1151-491e-bec1-baed30012f17"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dummy Dataset Head ---\n",
            "   Feature_Num1  Feature_Num2  Feature_Num3  Gender      City  Target\n",
            "0     37.454012     55.126340             0  Female  New York       0\n",
            "1     95.071431     78.142563             5  Female     Paris       1\n",
            "2     73.199394     64.256358             7    Male  New York       0\n",
            "3     59.865848     41.346445             2    Male  New York       0\n",
            "4     15.601864     36.523780             7  Female     Tokyo       0\n",
            "\n",
            "--- Dummy Dataset Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 6 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Feature_Num1  475 non-null    float64\n",
            " 1   Feature_Num2  485 non-null    float64\n",
            " 2   Feature_Num3  500 non-null    int64  \n",
            " 3   Gender        500 non-null    object \n",
            " 4   City          500 non-null    object \n",
            " 5   Target        500 non-null    int64  \n",
            "dtypes: float64(2), int64(2), object(2)\n",
            "memory usage: 23.6+ KB\n",
            "\n",
            "--- Target Variable Distribution ---\n",
            "Target\n",
            "0    250\n",
            "1    250\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training set size: 400 samples\n",
            "Testing set size: 100 samples\n",
            "Training target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "Testing target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "--- Question 11: Visualize Confusion Matrix for Binary Classification ---\n",
            "Training Logistic Regression model...\n",
            "Model training complete.\n",
            "Model Accuracy: 0.7500\n",
            "\n",
            "--- Raw Confusion Matrix ---\n",
            "[[36 14]\n",
            " [11 39]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqsAAAIjCAYAAAAk+FJEAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAR1VJREFUeJzt3XmcTvX///HnNWPmMoyZMczERIMwyK6SlEGyZVeij2aGkOWTXaVPJIVCES3KniYpSdKCrMmSNS3Ivo7dYJjNzPv3h5/r6zIzzDCck3ncbze3m+t93uec17k2T+/rnPdxGGOMAAAAABvysLoAAAAAICOEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVcAGduzYofr168vf318Oh0Nz587N1u3v3btXDodD06ZNy9bt/pvVrl1btWvXzrbtxcXFqVOnTipUqJAcDod69+6dbdu+WUOGDJHD4bC6jNsiu1/XrErvub548aJefPFFFS1aVB4eHmrRooUkyeFwaMiQIbe9xqioKBUrVuy27xe4UYRV4P/btWuXnn/+eZUoUUK5c+eWn5+fatasqffee0/x8fG3dN+RkZH6448/NGzYMM2YMUP333//Ld3f7RQVFSWHwyE/P790n8cdO3bI4XDI4XBo9OjRWd7+4cOHNWTIEG3evDkbqr1xw4cP17Rp09StWzfNmDFDzz777C3dX7FixVzPm8PhUO7cuVWqVCkNGDBAp06duqX7tsLRo0fVv39/lSlTRnny5FHevHlVrVo1vfnmm4qNjbW6vGuaMmWKRo0apSeffFLTp09Xnz59bvk+7fK5ALKDwxhjrC4CsNr333+vp556Sk6nUxERESpfvrySkpK0cuVKff3114qKitInn3xyS/YdHx+vPHny6H//+5/efPPNW7IPY4wSExPl5eUlT0/PW7KPjERFRSk6OlrGGH3++edq06aN2/IhQ4bo7bffVkJCgkaNGqX+/ftnafvr16/XAw88oKlTpyoqKirT6yUlJUmSvL29s7S/jDz00EPKlSuXVq5cmS3bu55ixYopf/786tevnyQpISFBGzZs0KRJk1SlShX99ttvrr4XL17UxYsXlTt37ttSW3Zbt26dGjdurLi4OLVv317VqlWTdOm1/+KLL/Twww9r4cKFkuQaVV22bJkltab3XLdt21YrV67UwYMH3fomJCQoV65cypUrV7bXca3PRXJyslJTU+V0OrN9v8CtkP2fEOBfZs+ePWrbtq1CQ0O1ZMkSFS5c2LWsR48e2rlzp77//vtbtv/jx49LkgICAm7ZPi6PvFnF6XSqZs2amjlzZpqw+vnnn+uJJ57Q119/fVtquXDhgvLkyZNtIfWyY8eOqVy5ctm2vYsXLyo1NfWadd59991q376963GnTp3k6+ur0aNHa8eOHSpVqpQk3bJAdD3nz59X3rx5b2obsbGxatmypTw9PbVp0yaVKVPGbfmwYcM0ceLEm9pHdkrvuT527Fi6n2+rPpNeXl6W7Be4YQbI4bp27WokmV9//TVT/ZOTk83QoUNNiRIljLe3twkNDTUDBw40CQkJbv1CQ0PNE088YX755RfzwAMPGKfTaYoXL26mT5/u6vPaa68ZSW5/QkNDjTHGREZGuv5+pcvrXGnhwoWmZs2axt/f3+TNm9eULl3aDBw40LV8z549RpKZOnWq23qLFy82jzzyiMmTJ4/x9/c3zZo1M3///Xe6+9uxY4eJjIw0/v7+xs/Pz0RFRZnz589f9/mKjIw0efPmNdOmTTNOp9OcPn3atey3334zkszXX39tJJlRo0a5lp08edL069fPlC9f3uTNm9fky5fPNGzY0GzevNnVZ+nSpWmevyuPMzw83Nx3331m/fr15tFHHzU+Pj6mV69ermXh4eGubUVERBin05nm+OvXr28CAgLMoUOH0j2+jGrYs2ePMcaYo0ePmo4dO5rg4GDjdDpNxYoVzbRp09y2cfn1GTVqlBkzZowpUaKE8fDwMJs2bcrweb38/rra6NGjjSSze/duV1t67xlJpkePHuabb74x9913n/H29jblypUzP/74o1u/vXv3mm7dupnSpUub3Llzm8DAQPPkk0+6ju+yqVOnGklm2bJlplu3biYoKMgEBASYJUuWGElmzpw5aWqNjo42ksyqVasyPM633nrLSDLR0dEZ9rnS1a9rYmKiGTRokKlatarx8/MzefLkMY888ohZsmRJmnVnzpxpqlatanx9fU2+fPlM+fLlzdixY13Lk5KSzJAhQ0zJkiWN0+k0gYGBpmbNmmbhwoWuPlc+15df16v/LF261Bhz6TV47bXX3Go4ePCg6dixoylcuLDx9vY2xYoVM127djWJiYnGmOz5XKT33RIXF2f69u1rihQpYry9vU3p0qXNqFGjTGpqqlu/zL5vgOzEyCpyvO+++04lSpTQww8/nKn+nTp10vTp0/Xkk0+qX79+Wrt2rUaMGKGtW7fqm2++ceu7c+dOPfnkk3ruuecUGRmpKVOmKCoqStWqVdN9992nVq1aKSAgQH369FG7du3UuHFj+fr6Zqn+v/76S02aNFHFihU1dOhQOZ1O7dy5U7/++us11/v555/VqFEjlShRQkOGDFF8fLzGjx+vmjVrauPGjWkuwGjTpo2KFy+uESNGaOPGjZo0aZKCg4P19ttvZ6rOVq1aqWvXrpozZ446duwo6dKoapkyZVS1atU0/Xfv3q25c+fqqaeeUvHixXX06FF9/PHHCg8P199//62QkBCVLVtWQ4cO1eDBg9WlSxc9+uijkuT2Wp48eVKNGjVS27Zt1b59e911113p1vfee+9pyZIlioyM1OrVq+Xp6amPP/5YCxcu1IwZMxQSEpLuemXLltWMGTPUp08fFSlSxPWzfFBQkOLj41W7dm3t3LlT//3vf1W8eHF99dVXioqKUmxsrHr16uW2ralTpyohIUFdunSR0+lUYGDgNZ/T5ORknThxQtKln5Q3bdqkd999V7Vq1VLx4sWvua4krVy5UnPmzFH37t2VL18+jRs3Tq1bt9b+/ftVoEABSZd+gl+1apXatm2rIkWKaO/evfroo49Uu3Zt/f3338qTJ4/bNrt3766goCANHjxY58+fV+3atVW0aFFFR0erZcuWbn2jo6N17733qkaNGhnWOG/ePPn4+OjJJ5+87vGk5+zZs5o0aZLatWunzp0769y5c5o8ebIaNGig3377TZUrV5YkLVq0SO3atdNjjz3mek9v3bpVv/76q+t1GjJkiEaMGKFOnTrpwQcf1NmzZ7V+/Xpt3LhRjz/+eJp9BwUFacaMGRo2bJji4uI0YsQISZfeM+k5fPiwHnzwQcXGxqpLly4qU6aMDh06pNmzZ+vChQvy9vbOts/FlYwxatasmZYuXarnnntOlStX1oIFCzRgwAAdOnRIY8aMceufmfcNkK2sTsuAlc6cOWMkmebNm2eq/+bNm40k06lTJ7f2/v37G0luozWhoaFGklmxYoWr7dixY8bpdJp+/fq52q4cVbtSZkdWx4wZYySZ48ePZ1h3eiOrlStXNsHBwebkyZOutt9//914eHiYiIiINPvr2LGj2zZbtmxpChQokOE+rzyOvHnzGmOMefLJJ81jjz1mjDEmJSXFFCpUyLz++uvpPgcJCQkmJSUlzXE4nU4zdOhQV9u6devSHTU25tIomyQzYcKEdJddOQJnjDELFiwwksybb75pdu/ebXx9fU2LFi2ue4zGpD/SOXbsWCPJfPbZZ662pKQkU6NGDePr62vOnj3rOi5Jxs/Pzxw7dizT+1M6o2c1a9Y0J06ccOub0ciqt7e32blzp6vt999/N5LM+PHjXW0XLlxIs+/Vq1cbSebTTz91tV0eWX3kkUfMxYsX3foPHDjQOJ1OExsb62o7duyYyZUrV5qRxavlz5/fVKpU6Zp9rnT163rx4kXXqORlp0+fNnfddZfbe7pXr17Gz88vTe1XqlSpUrqj2VdK77m+PMJ/NV01shoREWE8PDzMunXr0vS9PMKZHZ+Lq79b5s6d63rfX+nJJ580DofD7T2S2fcNkJ2YDQA52tmzZyVJ+fLly1T/H374QZLUt29ft/bLo2lXn9tarlw516iGdGmkJSwsTLt3777hmq92+Vy4b7/9VqmpqZlaJyYmRps3b1ZUVJTb6F3FihX1+OOPu47zSl27dnV7/Oijj+rkyZOu5zAznnnmGS1btkxHjhzRkiVLdOTIET3zzDPp9nU6nfLwuPQVlZKSopMnT8rX11dhYWHauHFjpvfpdDrVoUOHTPWtX7++nn/+eQ0dOlStWrVS7ty59fHHH2d6X1f74YcfVKhQIbVr187V5uXlpZ49eyouLk7Lly9369+6dWsFBQVlevvVq1fXokWLtGjRIs2fP1/Dhg3TX3/9pWbNmmVqBot69erp3nvvdT2uWLGi/Pz83N6fPj4+rr8nJyfr5MmTKlmypAICAtJ9HTp37pzmIr6IiAglJiZq9uzZrrZZs2bp4sWLbufcpufs2bOZ/nymx9PT03Xeb2pqqk6dOqWLFy/q/vvvd6s/ICBA58+f16JFizLcVkBAgP766y/t2LHjhuvJSGpqqubOnaumTZumOxvI5emwsutzcaUffvhBnp6e6tmzp1t7v379ZIzRjz/+6NaemfcNkJ0Iq8jR/Pz8JEnnzp3LVP99+/bJw8NDJUuWdGsvVKiQAgICtG/fPrf2e+65J8028ufPr9OnT99gxWk9/fTTqlmzpjp16qS77rpLbdu21ZdffnnN4Hq5zrCwsDTLypYtqxMnTuj8+fNu7VcfS/78+SUpS8fSuHFj5cuXT7NmzVJ0dLQeeOCBNM/lZampqRozZoxKlSolp9OpggULKigoSFu2bNGZM2cyvc+77747SxdTjR49WoGBgdq8ebPGjRun4ODgTK97tX379qlUqVKucHHZ5Z+Br36/ZOan+ysVLFhQ9erVU7169fTEE0/olVde0aRJk7Rq1SpNmjTpuutn5v0ZHx+vwYMHq2jRom6vQ2xsbLqvQ3rHUKZMGT3wwAOKjo52tUVHR+uhhx7K8PW/zM/PL9Ofz4xMnz5dFStWVO7cuVWgQAEFBQXp+++/d6u/e/fuKl26tBo1aqQiRYqoY8eO+umnn9y2M3ToUMXGxqp06dKqUKGCBgwYoC1bttxUbZcdP35cZ8+eVfny5a/ZL7s+F1fat2+fQkJC0vynIKP36e34XgOuRFhFjubn56eQkBD9+eefWVovsxOsZzRNlMnEjHEZ7SMlJcXtsY+Pj1asWKGff/5Zzz77rLZs2aKnn35ajz/+eJq+N+NmjuUyp9OpVq1aafr06frmm28yHFWVLs1b2rdvX9WqVUufffaZFixYoEWLFum+++7L9Aiy5D4ymBmbNm3SsWPHJEl//PFHlta9WVmtNT2PPfaYJGnFihXX7ZuZ1/SFF17QsGHD1KZNG3355ZdauHChFi1apAIFCqT7OmR0DBEREVq+fLkOHjyoXbt2ac2aNdcdVZUuBd1//vnHNdVYVn322WeKiorSvffeq8mTJ+unn37SokWLVLduXbf6g4ODtXnzZs2bN891/majRo0UGRnp6lOrVi3t2rVLU6ZMUfny5TVp0iRVrVo1U/8xyC7Z9bm4GdnxXQBkBWEVOV6TJk20a9curV69+rp9Q0NDlZqamuZnwKNHjyo2NlahoaHZVlf+/PnTnez86lEOSfLw8NBjjz2md999V3///beGDRumJUuWaOnSpelu+3Kd27dvT7Ns27ZtKliw4E1POZSRZ555Rps2bdK5c+fUtm3bDPvNnj1bderU0eTJk9W2bVvVr19f9erVS/OcZOedmc6fP68OHTqoXLly6tKli0aOHKl169bd8PZCQ0O1Y8eONCFi27ZtruXZ7eLFi5Iu3VErO8yePVuRkZF655139OSTT+rxxx/XI488kuWJ+Nu2bStPT0/NnDlT0dHR8vLy0tNPP33d9Zo2bar4+Pgbntps9uzZKlGihObMmaNnn31WDRo0UL169ZSQkJCmr7e3t5o2baoPP/zQdZOQTz/9VDt37nT1CQwMVIcOHTRz5kwdOHBAFStWzJa7UAUFBcnPz++6/3G+FZ+L0NBQHT58OM0I9q18nwJZQVhFjvfiiy8qb9686tSpk44ePZpm+a5du/Tee+9JuvQztiSNHTvWrc+7774rSXriiSeyra57771XZ86ccfuZMSYmJs2MA+ndrejyFc6JiYnpbrtw4cKqXLmypk+f7vaP3J9//qmFCxe6jvNWqFOnjt544w29//77KlSoUIb9PD0904zUfPXVVzp06JBb2+VQnR13MXrppZe0f/9+TZ8+Xe+++66KFSumyMjIDJ/H62ncuLGOHDmiWbNmudouXryo8ePHy9fXV+Hh4Tdd89W+++47SVKlSpWyZXvpvQ7jx4/P8qh9wYIF1ahRI3322WeKjo5Ww4YNVbBgweuu17VrVxUuXFj9+vXTP//8k2b5sWPHrnkzjcujgFcew9q1a9P85/TkyZNujz08PFSxYkVJ//c5urqPr6+vSpYsecPvj6v316JFC3333Xdav359muWX678Vn4vGjRsrJSVF77//vlv7mDFj5HA41KhRo6wcCpDtmLoKOd69996rzz//XE8//bTKli3rdgerVatWuaYaki4FgMjISH3yySeKjY1VeHi4fvvtN02fPl0tWrRQnTp1sq2utm3b6qWXXlLLli3Vs2dPXbhwQR999JFKly7tdiHF0KFDtWLFCj3xxBMKDQ3VsWPH9OGHH6pIkSJ65JFHMtz+qFGj1KhRI9WoUUPPPfeca+oqf3//W3q/cg8PD7366qvX7dekSRMNHTpUHTp00MMPP6w//vhD0dHRKlGihFu/e++9VwEBAZowYYLy5cunvHnzqnr16lk+/3PJkiX68MMP9dprr7mm0po6dapq166tQYMGaeTIkVnaniR16dJFH3/8saKiorRhwwYVK1ZMs2fP1q+//qqxY8fe1IVDknTo0CF99tlnki7dkev333/Xxx9/rIIFC+qFF164qW1f1qRJE82YMUP+/v4qV66cVq9erZ9//vmGpiiKiIhwTUH1xhtvZGqd/Pnz65tvvlHjxo1VuXJltztYbdy4UTNnzrzm1FdNmjTRnDlz1LJlSz3xxBPas2ePJkyYoHLlyrmNPnfq1EmnTp1S3bp1VaRIEe3bt0/jx49X5cqVXedulitXTrVr11a1atUUGBio9evXa/bs2frvf/+b5eciPcOHD9fChQsVHh6uLl26qGzZsoqJidFXX32llStXKiAg4JZ8Lpo2bao6derof//7n/bu3atKlSpp4cKF+vbbb9W7d2+3i6kAS1g1DQFgN//884/p3LmzKVasmPH29jb58uUzNWvWNOPHj3eb8D85Odm8/vrrpnjx4sbLy8sULVr0mjcFuNrVU+tkNHWVMZcm+y9fvrzx9vY2YWFh5rPPPkszNc7ixYtN8+bNTUhIiPH29jYhISGmXbt25p9//kmzj6unsfn5559NzZo1jY+Pj/Hz8zNNmzbN8KYAV0+NdXmqoqsnh7/alVNXZSSjqav69etnChcubHx8fEzNmjXN6tWr051y6ttvvzXlypUzuXLlSvemAOm5cjtnz541oaGhpmrVqiY5OdmtX58+fYyHh4dZvXr1NY8ho9f76NGjpkOHDqZgwYLG29vbVKhQIc3rcK33wLX2pyumrPLw8DDBwcGmXbt2btMKGXPtmwKkt93IyEjX49OnT7vq9/X1NQ0aNDDbtm1L0+/y+yG9aZcuS0xMNPnz5zf+/v4mPj4+08dqjDGHDx82ffr0cd2cIE+ePKZatWpm2LBh5syZM65+V78/UlNTzfDhw01oaKhxOp2mSpUqZv78+Wmmb5o9e7apX7++CQ4ONt7e3uaee+4xzz//vImJiXH1efPNN82DDz5oAgICjI+PjylTpowZNmyYSUpKcvW5mamrjDFm3759JiIiwgQFBRmn02lKlChhevTo4Zp+Kzs+F+lNi3fu3DnTp08fExISYry8vEypUqWueVOAq139fgCyk8MYzogGANx6Fy9eVEhIiJo2barJkydbXQ6AfwnOWQUA3BZz587V8ePHFRERYXUpAP5FGFkFANxSa9eu1ZYtW/TGG2+oYMGCNzx5PYCciZFVAMAt9dFHH6lbt24KDg7Wp59+anU5AP5lGFkFAACAbTGyCgAAANsirAIAAMC2CKsAAACwrTvyDlbZea9wAAAAZL/MXjZ1R4ZVScrd5AOrSwCAbJMwv4f2n0ywugwAuO04DQAAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFu5rNz5iRMnNGXKFK1evVpHjhyRJBUqVEgPP/ywoqKiFBQUZGV5AAAAsJhlI6vr1q1T6dKlNW7cOPn7+6tWrVqqVauW/P39NW7cOJUpU0br16+3qjwAAADYgGUjqy+88IKeeuopTZgwQQ6Hw22ZMUZdu3bVCy+8oNWrV1tUIQAAAKxmWVj9/fffNW3atDRBVZIcDof69OmjKlWqWFAZAAAA7MKy0wAKFSqk3377LcPlv/32m+66667bWBEAAADsxrKR1f79+6tLly7asGGDHnvsMVcwPXr0qBYvXqyJEydq9OjRVpUHAAAAG7AsrPbo0UMFCxbUmDFj9OGHHyolJUWS5OnpqWrVqmnatGlq06aNVeUBAADABhzGGGN1EcnJyTpx4oQkqWDBgvLy8rqp7TkcDuVu8kF2lAYAtpAwv4f2n0ywugwAyDZFA52Z6mfpPKuXeXl5qXDhwlaXAQAAAJvhDlYAAACwLcIqAAAAbIuwCgAAANsirAIAAMC2LLnAat68eZnu26xZs1tYCQAAAOzMkrDaokWLTPVzOByu+VcBAACQ81gSVlNTU63YLQAAAP5lOGcVAAAAtmWLmwKcP39ey5cv1/79+5WUlOS2rGfPnhZVBQAAAKtZHlY3bdqkxo0b68KFCzp//rwCAwN14sQJ5cmTR8HBwYRVAACAHMzy0wD69Omjpk2b6vTp0/Lx8dGaNWu0b98+VatWTaNHj7a6PAAAAFjI8rC6efNm9evXTx4eHvL09FRiYqKKFi2qkSNH6pVXXrG6PAAAAFjI8rDq5eUlD49LZQQHB2v//v2SJH9/fx04cMDK0gBJUudG9+m3cU/r6KxOOjqrk5aNaqX61e5x61M97C79+GYznfiqs47O6qRFI1oot7enRRUDwLVt2bRer/b/r55u+pjq1aioX5cvybDv2LffUL0aFfX1FzNuY4XA/7H8nNUqVapo3bp1KlWqlMLDwzV48GCdOHFCM2bMUPny5a0uD9ChE3EaNH21dh4+I4dDav9YGX31v0Z6qPeX2rr/tKqH3aVvX2+i0bM3qu8nv+hiilHF4gWUmmqsLh0A0pWQEK8SpcLUsElLDRnYJ8N+K5ct1ta/tqhAweDbWB3gzvKwOnz4cJ07d06SNGzYMEVERKhbt24qVaqUpkyZYnF1gPTDun1uj4fMWKvOje7Tg2GFtHX/aY3sVFMffveHRs/e5Oqz41Dsba4SADLvwRqP6sEaj16zz4ljR/X+uyP01tgJ+l+//96myoC0LA+r999/v+vvwcHB+umnnyysBrg2Dw+HWte8V3lze2nttiMK8vfRg2UK6YvlO7R0ZCsVL+Snfw6d1pAZa7Xq7yNWlwsANyQ1NVVvDX1Fbf4TpWIlSlpdDnI4y8PqzUpMTFRiYqLVZeAOd19ooJaNaq3c3p6Ki0/W08N+1LYDp/Vg2F2SpP+1e0ADp6zSlj0n9J+6Yfrhzeaq1uML7Yo5Y3HlAJB1X8yYIk/PXGrZ5j9WlwJYH1aLFy8uh8OR4fLdu3dfc/0RI0bo9ddfz+6yADf/HIpV9V6z5J/HqZY179XEPo+p/sC58vj/793JP/2lGYu3SZJ+331CtSsWUeTjZTX40zVWlg0AWfbPtr/1zZfR+mjarGv++wzcLpaH1d69e7s9Tk5O1qZNm/TTTz9pwIAB111/4MCB6tu3r1ubv79/dpYIKPliqnbHnJUkbdp1XNVKBalHs4oaPXujJGnrgdNu/bcfPK2iQb63vU4AuFl/bN6g2NOn9EzLBq621JQUfTz+Hc2ZFa3obzhdD7eX5WG1V69e6bZ/8MEHWr9+/XXXdzqdcjqd2V0WcE0eDoecXp7ad/ScDp+MU+m7A9yWlwzx18IN+60pDgBuQr1GTVX1gYfc2l7u3U31GjVRwyeaW1QVcjLLw2pGGjVqpIEDB2rq1KlWl4IcbmjEQ1qwYZ8OHI9TPh8vPR1eWrUq3K2mr30nSRozZ7NefeYB/bHnhH7fc0Lt65ZRWJH8euatBRZXDgDpi79wQYcO/t9/qGMOH9LOf7Ypn5+/7ipUWP7+AW79c+XKpcDAAioaWvw2VwrYOKzOnj1bgYGBVpcBKMjfR5P7PKZCgXl15nyi/tx7Uk1f+05LNh+UJL0/b4tye3tqZKdHlD+fU3/sOakmg+dpz5GzFlcOAOnbvu0v9e/xnOvxhHGjJEn1GzfTi4PetKosIF0OY4ylM5dXqVLF7QRuY4yOHDmi48eP68MPP1SXLl2yvE2Hw6HcTT7IzjIBwFIJ83to/8kEq8sAgGxTNDBzp3FaPrLavHlzt7Dq4eGhoKAg1a5dW2XKlLGwMgAAAFjN8rA6ZMgQq0sAAACATXlYXYCnp6eOHTuWpv3kyZPy9PS0oCIAAADYheVhNaNTZhMTE+Xt7X2bqwEAAICdWHYawLhx4yRduhhq0qRJ8vX9vwnUU1JStGLFCs5ZBQAAyOEsC6tjxoyRdGlkdcKECW4/+Xt7e6tYsWKaMGGCVeUBAADABiwLq3v27JEk1alTR3PmzFH+/PmtKgUAAAA2ZflsAEuXLrW6BAAAANiU5RdYtW7dWm+//Xaa9pEjR+qpp56yoCIAAADYheVhdcWKFWrcuHGa9kaNGmnFihUWVAQAAAC7sDysxsXFpTtFlZeXl86e5d7qAAAAOZnlYbVChQqaNWtWmvYvvvhC5cqVs6AiAAAA2IXlF1gNGjRIrVq10q5du1S3bl1J0uLFizVz5kx99dVXFlcHAAAAK1keVps2baq5c+dq+PDhmj17tnx8fFSxYkX9/PPPCg8Pt7o8AAAAWMjysCpJTzzxhJ544ok07X/++afKly9vQUUAAACwA8vPWb3auXPn9Mknn+jBBx9UpUqVrC4HAAAAFrJNWF2xYoUiIiJUuHBhjR49WnXr1tWaNWusLgsAAAAWsvQ0gCNHjmjatGmaPHmyzp49qzZt2igxMVFz585lJgAAAABYN7LatGlThYWFacuWLRo7dqwOHz6s8ePHW1UOAAAAbMiykdUff/xRPXv2VLdu3VSqVCmrygAAAICNWTayunLlSp07d07VqlVT9erV9f777+vEiRNWlQMAAAAbsiysPvTQQ5o4caJiYmL0/PPP64svvlBISIhSU1O1aNEinTt3zqrSAAAAYBOWzwaQN29edezYUStXrtQff/yhfv366a233lJwcLCaNWtmdXkAAACwkOVh9UphYWEaOXKkDh48qJkzZ1pdDgAAACxmq7B6maenp1q0aKF58+ZZXQoAAAAsZMuwCgAAAEiEVQAAANgYYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2RVgFAACAbRFWAQAAYFuEVQAAANgWYRUAAAC2lSsznbZs2ZLpDVasWPGGiwEAAACulKmwWrlyZTkcDhlj0l1+eZnD4VBKSkq2FggAAICcK1Nhdc+ePbe6DgAAACCNTIXV0NDQW10HAAAAkMYNXWA1Y8YM1axZUyEhIdq3b58kaezYsfr222+ztTgAAADkbFkOqx999JH69u2rxo0bKzY21nWOakBAgMaOHZvd9QEAACAHy3JYHT9+vCZOnKj//e9/8vT0dLXff//9+uOPP7K1OAAAAORsWQ6re/bsUZUqVdK0O51OnT9/PluKAgAAAKQbCKvFixfX5s2b07T/9NNPKlu2bHbUBAAAAEjK5GwAV+rbt6969OihhIQEGWP022+/aebMmRoxYoQmTZp0K2oEAABADpXlsNqpUyf5+Pjo1Vdf1YULF/TMM88oJCRE7733ntq2bXsragQAAEAO5TAZ3ZYqEy5cuKC4uDgFBwdnZ003zeFwKHeTD6wuAwCyTcL8Htp/MsHqMgAg2xQNdGaqX5ZHVi87duyYtm/fLulSOAwKCrrRTQEAAADpyvIFVufOndOzzz6rkJAQhYeHKzw8XCEhIWrfvr3OnDlzK2oEAABADpXlsNqpUyetXbtW33//vWJjYxUbG6v58+dr/fr1ev75529FjQAAAMihsnwawPz587VgwQI98sgjrrYGDRpo4sSJatiwYbYWBwAAgJwtyyOrBQoUkL+/f5p2f39/5c+fP1uKAgAAAKQbCKuvvvqq+vbtqyNHjrjajhw5ogEDBmjQoEHZWhwAAABytkydBlClShU5HA7X4x07duiee+7RPffcI0nav3+/nE6njh8/znmrAAAAyDaZCqstWrS4xWUAAAAAaWUqrL722mu3ug4AAAAgjSyfswoAAADcLlmeuiolJUVjxozRl19+qf379yspKclt+alTp7KtOAAAAORsWR5Zff311/Xuu+/q6aef1pkzZ9S3b1+1atVKHh4eGjJkyC0oEQAAADlVlsNqdHS0Jk6cqH79+ilXrlxq166dJk2apMGDB2vNmjW3okYAAADkUFkOq0eOHFGFChUkSb6+vjpz5owkqUmTJvr++++ztzoAAADkaFkOq0WKFFFMTIwk6d5779XChQslSevWrZPT6cze6gAAAJCjZTmstmzZUosXL5YkvfDCCxo0aJBKlSqliIgIdezYMdsLBAAAQM7lMMaYm9nAmjVrtGrVKpUqVUpNmzbNrrpuisPhUO4mH1hdBgBkm4T5PbT/ZILVZQBAtikamLlf5G96ntWHHnpIffv2VfXq1TV8+PCb3RwAAADgkm03BYiJidGgQYOya3MAAAAAd7ACAACAfRFWAQAAYFuEVQAAANhWrsx27Nu37zWXHz9+/KaLyU4J83tYXQIAZKt7CuS2ugQAyDaZnZAq02F106ZN1+1Tq1atzG7ulotPvqkZuQDAVny8HMpdmf+EA8h5Mh1Wly5deivrAAAAANLgnFUAAADYFmEVAAAAtkVYBQAAgG0RVgEAAGBbhFUAAADY1g2F1V9++UXt27dXjRo1dOjQIUnSjBkztHLlymwtDgAAADlblsPq119/rQYNGsjHx0ebNm1SYmKiJOnMmTMaPnx4thcIAACAnCvLYfXNN9/UhAkTNHHiRHl5ebnaa9asqY0bN2ZrcQAAAMjZshxWt2/fnu6dqvz9/RUbG5sdNQEAAACSbiCsFipUSDt37kzTvnLlSpUoUSJbigIAAACkGwirnTt3Vq9evbR27Vo5HA4dPnxY0dHR6t+/v7p163YragQAAEAOlSurK7z88stKTU3VY489pgsXLqhWrVpyOp3q37+/XnjhhVtRIwAAAHIohzHG3MiKSUlJ2rlzp+Li4lSuXDn5+vpmd203zOFwKD75hg4LAGzJx8uh3JV7WF0GAGSb+E3vZ6pflkdWL/P29la5cuVudHUAAADgurIcVuvUqSOHw5Hh8iVLltxUQQAAAMBlWQ6rlStXdnucnJyszZs3688//1RkZGR21QUAAABkPayOGTMm3fYhQ4YoLi7upgsCAAAALsvy1FUZad++vaZMmZJdmwMAAACyL6yuXr1auXPnzq7NAQAAAFk/DaBVq1Zuj40xiomJ0fr16zVo0KBsKwwAAADIclj19/d3e+zh4aGwsDANHTpU9evXz7bCAAAAgCyF1ZSUFHXo0EEVKlRQ/vz5b1VNAAAAgKQsnrPq6emp+vXrKzY29haVAwAAAPyfLF9gVb58ee3evftW1AIAAAC4yXJYffPNN9W/f3/Nnz9fMTExOnv2rNsfAAAAILs4jDEmMx2HDh2qfv36KV++fP+38hW3XTXGyOFwKCUlJfurzCKHw6H45EwdFgD8K/h4OZS7cg+rywCAbBO/6f1M9ct0WPX09FRMTIy2bt16zX7h4eGZ2vGtRFgFcKchrAK402Q2rGZ6NoDLmdYOYRQAAAA5Q5bOWb3yZ38AAADgVsvSPKulS5e+bmA9derUTRUEAAAAXJalsPr666+nuYMVAAAAcKtkKay2bdtWwcHBt6oWAAAAwE2mz1nlfFUAAADcbpkOq5mc4QoAAADINpk+DSA1NfVW1gEAAACkkeXbrQIAAAC3C2EVAAAAtkVYBQAAgG0RVgEAAGBbhFUAAADYFmEVAAAAtkVYBQAAgG0RVgEAAGBbhFUAAADYFmEVAAAAtkVYBQAAgG0RVgEAAGBbhFUAAADYFmEVAAAAtkVYBQAAgG0RVgEAAGBbhFUAAADYFmEVAAAAtkVYBQAAgG0RVgEAAGBbhFUAAADYFmEVAAAAtkVYBQAAgG0RVgEAAGBbhFUAAADYFmEVAAAAtkVYBQAAgG0RVgEAAGBbhFUAAADYFmEVAAAAtkVYBQAAgG0RVgEAAGBbhFUAAADYFmEVAAAAtkVYBQAAgG0RVgEAAGBbhFUAAADYFmEVAAAAtkVYBQAAgG0RVgEAAGBbhFUAAADYFmEVAAAAtkVYBQAAgG0RVgEAAGBbhFUAAADYFmEVAAAAtkVYBQAAgG0RVgEAAGBbhFUAAADYFmEVAAAAtkVYBQAAgG3ZNqweOHBAHTt2tLoMAAAAWMi2YfXUqVOaPn261WUAAADAQrms2vG8efOuuXz37t23qRIAAADYlWVhtUWLFnI4HDLGZNjH4XDcxooAAABgN5adBlC4cGHNmTNHqamp6f7ZuHGjVaUBAADAJiwLq9WqVdOGDRsyXH69UVcAAADc+Sw7DWDAgAE6f/58hstLliyppUuX3saKAAAAYDcOcwcOXzocDsUn33GHBSAH8/FyKHflHlaXAQDZJn7T+5nqZ9upqwAAAADCKgAAAGyLsAoAAADbIqwCAADAtgirAAAAsC1Lpq663q1Wr9SsWbNbWAkAAADszJKw2qJFi0z1czgcSklJubXFAAAAwLYsCaupqalW7BYAAAD/MpyzCgAAANuy7HarVzp//ryWL1+u/fv3KykpyW1Zz549LaoKAAAAVrM8rG7atEmNGzfWhQsXdP78eQUGBurEiRPKkyePgoODCasAAAA5mOWnAfTp00dNmzbV6dOn5ePjozVr1mjfvn2qVq2aRo8ebXV5AAAAsJDlYXXz5s3q16+fPDw85OnpqcTERBUtWlQjR47UK6+8YnV5AAAAsJDlYdXLy0seHpfKCA4O1v79+yVJ/v7+OnDggJWlAZKkDevX6YXuXVWv9iOqdF+Yliz+2W35z4sW6vnOHVXr4eqqdF+Ytm3dalGlAJA5nZ96RL/NGqijv4zS0V9Gadn0fqpfs5xrefEiBTXrnc7av2SEjv4ySp+93VHBgfksrBg5meVhtUqVKlq3bp0kKTw8XIMHD1Z0dLR69+6t8uXLW1wdIMXHX1BYWJgGvvpahsurVKmq3n373+bKAODGHDoaq0Hjv9XD/xmpmv8ZpWW//aOvxnRR2RKFlCe3t+Z/2EPGGDXqMl51O4yRt5envn7veTkcDqtLRw5k+QVWw4cP17lz5yRJw4YNU0REhLp166ZSpUppypQpFlcHSI88Gq5HHg3PcHnTZi0kSYcOHbxNFQHAzflhxZ9uj4d88J06P/WIHqxYXCHBAQoNKaCH2r2tc+cTJEmdBs9QzPKRqv1gaS1du92KkpGDWR5W77//ftffg4OD9dNPP1lYDQAAOYuHh0OtH6+qvD7eWrtlj0oUKShjjBKTLrr6JCReVGqq0cOV7yWs4razPKzerMTERCUmJlpdBgAA/yr3lQzRsun9lNs7l+LiE/V0v4natvuITpyO0/n4JA3r1VyD358nhxx6s1dz5crlqUIF/awuGzmQ5WG1ePHi1zwHZvfu3ddcf8SIEXr99dezuywAAO5o/+w9quptR8jf10ct61XRxKHPqn6n97Rt9xH958XJGvfK0+reLlypqUZf/rRBG//er1RjrC4bOZDlYbV3795uj5OTk7Vp0yb99NNPGjBgwHXXHzhwoPr27evW5u/vn50lAgBwx0m+mKLdB05IkjZtPaBq992jHu1q64VhX2jxmm26r9nrKhCQVxcvpupMXLz2LBquvQs2WFw1ciLLw2qvXr3Sbf/ggw+0fv36667vdDrldDqzuywAAHIUD4dDTm/3WHAy9rwkKfyB0goO9NX85X9YURpyOMvDakYaNWqkgQMHaurUqVaXghzuwvnzrvl/JenQwYPatnWr/P39VTgkRGdiYxUTE6Pjx49Jkvbu3SNJKliwoAoGBVlSMwBcy9AXmmnBr3/pQMxp5cubW083ul+17i+lpt0/lCQ92+whbd9zRMdPx6l6xeIaPeBJjY9eqh37jllcOXIi24bV2bNnKzAw0OoyAP3115/q1CHC9Xj0yBGSpGbNW+qN4W9p2dIlGvzqQNfyl/r3kSR17f5fdevxwu0tFgAyISjQV5PfiFChgn46E5egP3ccUtPuH2rJ2m2SpNLFgjX0hWYK9M+jfYdPaeTkBRr32RKLq0ZO5TDG2rOlq1Sp4naBlTFGR44c0fHjx/Xhhx+qS5cuWd6mw+FQfDIngQO4c/h4OZS7cg+rywCAbBO/6f1M9bN8ZLV58+ZuYdXDw0NBQUGqXbu2ypQpY2FlAAAAsJrlI6u3AiOrAO40jKwCuNNkdmTV4xbXcV2enp46diztCdsnT56Up6enBRUBAADALiwPqxkN7CYmJsrb2/s2VwMAAAA7seyc1XHjxkm69JP9pEmT5Ovr61qWkpKiFStWcM4qAABADmdZWB0zZoykSyOrEyZMcPvJ39vbW8WKFdOECROsKg8AAAA2YFlY3bPn0sTpderU0Zw5c5Q/f36rSgEAAIBNWT511dKlS60uAQAAADZl+QVWrVu31ttvv52mfeTIkXrqqacsqAgAAAB2YXlYXbFihRo3bpymvVGjRlqxYoUFFQEAAMAuLA+rcXFx6U5R5eXlpbNnz1pQEQAAAOzC8rBaoUIFzZo1K037F198oXLlyllQEQAAAOzC8gusBg0apFatWmnXrl2qW7euJGnx4sWaOXOmvvrqK4urAwAAgJUsD6tNmzbV3LlzNXz4cM2ePVs+Pj6qWLGifv75Z4WHh1tdHgAAACzkMBnd79QG/vzzT5UvXz7L6zkcDsUn2/awACDLfLwcyl25h9VlAEC2id/0fqb6WX7O6tXOnTunTz75RA8++KAqVapkdTkAAACwkG3C6ooVKxQREaHChQtr9OjRqlu3rtasWWN1WQAAALCQpeesHjlyRNOmTdPkyZN19uxZtWnTRomJiZo7dy4zAQAAAMC6kdWmTZsqLCxMW7Zs0dixY3X48GGNHz/eqnIAAABgQ5aNrP7444/q2bOnunXrplKlSllVBgAAAGzMspHVlStX6ty5c6pWrZqqV6+u999/XydOnLCqHAAAANiQZWH1oYce0sSJExUTE6Pnn39eX3zxhUJCQpSamqpFixbp3LlzVpUGAAAAm7DVPKvbt2/X5MmTNWPGDMXGxurxxx/XvHnzsrwd5lkFcKdhnlUAd5p/5TyrYWFhGjlypA4ePKiZM2daXQ4AAAAsZquR1ezCyCqAOw0jqwDuNP/KkVUAAADgSoRVAAAA2BZhFQAAALZFWAUAAIBtEVYBAABgW4RVAAAA2BZhFQAAALZFWAUAAIBtEVYBAABgW4RVAAAA2BZhFQAAALZFWAUAAIBtEVYBAABgW4RVAAAA2BZhFQAAALZFWAUAAIBtEVYBAABgW4RVAAAA2BZhFQAAALZFWAUAAIBtEVYBAABgW4RVAAAA2BZhFQAAALZFWAUAAIBtEVYBAABgW4RVAAAA2BZhFQAAALZFWAUAAIBtEVYBAABgW4RVAAAA2BZhFQAAALZFWAUAAIBtEVYBAABgW4RVAAAA2BZhFQAAALZFWAUAAIBtEVYBAABgW4RVAAAA2BZhFQAAALZFWAUAAIBtEVYBAABgW4RVAAAA2BZhFQAAALZFWAUAAIBtEVYBAABgW4RVAAAA2BZhFQAAALZFWAUAAIBtEVYBAABgW4RVAAAA2BZhFQAAALZFWAUAAIBtEVYBAABgW4RVAAAA2BZhFQAAALZFWAUAAIBtEVYBAABgW4RVAAAA2BZhFQAAALZFWAUAAIBtEVYBAABgW4RVAAAA2BZhFQAAALZFWAUAAIBtEVYBAABgW4RVAAAA2BZhFQAAALZFWAUAAIBtEVYBAABgW4RVAAAA2BZhFQAAALZFWAUAAIBtEVYBAABgW4RVAAAA2BZhFQAAALZFWAUAAIBtEVYBAABgW4RVAAAA2BZhFQAAALZFWAUAAIBtEVYBAABgW4RVAAAA2BZhFQAAALZFWAUAAIBtEVYBAABgWw5jjLG6CODfKDExUSNGjNDAgQPldDqtLgcAbhrfa7Ajwipwg86ePSt/f3+dOXNGfn5+VpcDADeN7zXYEacBAAAAwLYIqwAAALAtwioAAABsi7AK3CCn06nXXnuNixAA3DH4XoMdcYEVAAAAbIuRVQAAANgWYRUAAAC2RVgFAACAbRFWgatERUWpRYsWrse1a9dW7969b3sdy5Ytk8PhUGxs7G3fN4A7C99r+DcjrOJfISoqSg6HQw6HQ97e3ipZsqSGDh2qixcv3vJ9z5kzR2+88Uam+t7uL+KEhAT16NFDBQoUkK+vr1q3bq2jR4/eln0DuDl8r6Xvk08+Ue3ateXn50ewhSTCKv5FGjZsqJiYGO3YsUP9+vXTkCFDNGrUqHT7JiUlZdt+AwMDlS9fvmzbXnbq06ePvvvuO3311Vdavny5Dh8+rFatWlldFoBM4nstrQsXLqhhw4Z65ZVXrC4FNkFYxb+G0+lUoUKFFBoaqm7duqlevXqaN2+epP/7iWvYsGEKCQlRWFiYJOnAgQNq06aNAgICFBgYqObNm2vv3r2ubaakpKhv374KCAhQgQIF9OKLL+rq2dyu/rksMTFRL730kooWLSqn06mSJUtq8uTJ2rt3r+rUqSNJyp8/vxwOh6KioiRJqampGjFihIoXLy4fHx9VqlRJs2fPdtvPDz/8oNKlS8vHx0d16tRxqzM9Z86c0eTJk/Xuu++qbt26qlatmqZOnapVq1ZpzZo1N/AMA7jd+F5Lq3fv3nr55Zf10EMPZfHZxJ2KsIp/LR8fH7eRhsWLF2v79u1atGiR5s+fr+TkZDVo0ED58uXTL7/8ol9//VW+vr5q2LCha7133nlH06ZN05QpU7Ry5UqdOnVK33zzzTX3GxERoZkzZ2rcuHHaunWrPv74Y/n6+qpo0aL6+uuvJUnbt29XTEyM3nvvPUnSiBEj9Omnn2rChAn666+/1KdPH7Vv317Lly+XdOkfn1atWqlp06bavHmzOnXqpJdffvmadWzYsEHJycmqV6+eq61MmTK65557tHr16qw/oQAsl9O/14B0GeBfIDIy0jRv3twYY0xqaqpZtGiRcTqdpn///q7ld911l0lMTHStM2PGDBMWFmZSU1NdbYmJicbHx8csWLDAGGNM4cKFzciRI13Lk5OTTZEiRVz7MsaY8PBw06tXL2OMMdu3bzeSzKJFi9Ktc+nSpUaSOX36tKstISHB5MmTx6xatcqt73PPPWfatWtnjDFm4MCBply5cm7LX3rppTTbulJ0dLTx9vZO0/7AAw+YF198Md11ANgH32vXlt5+kTPlsjAnA1kyf/58+fr6Kjk5WampqXrmmWc0ZMgQ1/IKFSrI29vb9fj333/Xzp0705yXlZCQoF27dunMmTOKiYlR9erVXcty5cql+++/P81PZpdt3rxZnp6eCg8Pz3TdO3fu1IULF/T444+7tSclJalKlSqSpK1bt7rVIUk1atTI9D4A/DvxvQZcH2EV/xp16tTRRx99JG9vb4WEhChXLve3b968ed0ex8XFqVq1aoqOjk6zraCgoBuqwcfHJ8vrxMXFSZK+//573X333W7Lbub+24UKFVJSUpJiY2MVEBDgaj969KgKFSp0w9sFcPvwvQZcH2EV/xp58+ZVyZIlM92/atWqmjVrloKDg+Xn55dun8KFC2vt2rWqVauWJOnixYvasGGDqlatmm7/ChUqKDU1VcuXL3c7V/SyyyMgKSkprrZy5crJ6XRq//79GY5clC1b1nVRxWXXu0iqWrVq8vLy0uLFi9W6dWtJl84p279/P6MXwL8E32vA9XGBFe5Y//nPf1SwYEE1b95cv/zyi/bs2aNly5apZ8+eOnjwoCSpV69eeuuttzR37lxt27ZN3bt3v+acfsWKFVNkZKQ6duyouXPnurb55ZdfSpJCQ0PlcDg0f/58HT9+XHFxccqXL5/69++vPn36aPr06dq1a5c2btyo8ePHa/r06ZKkrl27aseOHRowYIC2b9+uzz//XNOmTbvm8fn7++u5555T3759tXTpUm3YsEEdOnRQjRo1uIoWuEPd6d9rknTkyBFt3rxZO3fulCT98ccf2rx5s06dOnVzTx7+vaw+aRbIjCsvRMjK8piYGBMREWEKFixonE6nKVGihOncubM5c+aMMebShQe9evUyfn5+JiAgwPTt29dERERkeCGCMcbEx8ebPn36mMKFCxtvb29TsmRJM2XKFNfyoUOHmkKFChmHw2EiIyONMZcunhg7dqwJCwszXl5eJigoyDRo0MAsX77ctd53331nSpYsaZxOp3n00UfNlClTrntxQXx8vOnevbvJnz+/yZMnj2nZsqWJiYm55nMJwB74Xkvfa6+9ZiSl+TN16tRrPZ24gzmMyeCMawAAAMBinAYAAAAA2yKsAgAAwLYIqwAAALAtwioAAABsi7AKAAAA2yKsAgAAwLYIqwAAALAtwioAAABsi7AKADcpKipKLVq0cD2uXbu2evfufdvrWLZsmRwOxzVvrXmzrj7WG3E76gRw5yCsArgjRUVFyeFwyOFwyNvbWyVLltTQoUN18eLFW77vOXPm6I033shU39sd3IoVK6axY8feln0BQHbIZXUBAHCrNGzYUFOnTlViYqJ++OEH9ejRQ15eXho4cGCavklJSfL29s6W/QYGBmbLdgAAjKwCuIM5nU4VKlRIoaGh6tatm+rVq6d58+ZJ+r+fs4cNG6aQkBCFhYVJkg4cOKA2bdooICBAgYGBat68ufbu3evaZkpKivr27auAgAAVKFBAL774oowxbvu9+jSAxMREvfTSSypatKicTqdKliypyZMna+/evapTp44kKX/+/HI4HIqKipIkpaamasSIESpevLh8fHxUqVIlzZ49220/P/zwg0qXLi0fHx/VqVPHrc4bkZKSoueee861z7CwML333nvp9n399dcVFBQkPz8/de3aVUlJSa5lmakdADKLkVUAOYaPj49Onjzperx48WL5+flp0aJFkqTk5GQ1aNBANWrU0C+//KJcuXLpzTffVMOGDbVlyxZ5e3vrnXfe0bRp0zRlyhSVLVtW77zzjr755hvVrVs3w/1GRERo9erVGjdunCpVqqQ9e/boxIkTKlq0qL7++mu1bt1a27dvl5+fn3x8fCRJI0aM0GeffaYJEyaoVKlSWrFihdq3b6+goCCFh4frwIEDatWqlXr06KEuXbpo/fr16tev3009P6mpqSpSpIi++uorFShQQKtWrVKXLl1UuHBhtWnTxu15y507t5YtW6a9e/eqQ4cOKlCggIYNG5ap2gEgSwwA3IEiIyNN8+bNjTHGpKammkWLFhmn02n69+/vWn7XXXeZxMRE1zozZswwYWFhJjU11dWWmJhofHx8zIIFC4wxxhQuXNiMHDnStTw5OdkUKVLEtS9jjAkPDze9evUyxhizfft2I8ksWrQo3TqXLl1qJJnTp0+72hISEkyePHnMqlWr3Po+99xzpl27dsYYYwYOHGjKlSvntvyll15Ks62rhYaGmjFjxmS4/Go9evQwrVu3dj2OjIw0gYGB5vz58662jz76yPj6+pqUlJRM1Z7eMQNARhhZBXDHmj9/vnx9fZWcnKzU1FQ988wzGjJkiGt5hQoV3M5T/f3337Vz507ly5fPbTsJCQnatWuXzpw5o5iYGFWvXt21LFeuXLr//vvTnApw2ebNm+Xp6ZmlEcWdO3fqwoULevzxx93ak5KSVKVKFUnS1q1b3eqQpBo1amR6Hxn54IMPNGXKFO3fv1/x8fFKSkpS5cqV3fpUqlRJefLkcdtvXFycDhw4oLi4uOvWDgBZQVgFcMeqU6eOPvroI3l7eyskJES5crl/5eXNm9ftcVxcnKpVq6bo6Og02woKCrqhGi7/rJ8VcXFxkqTvv/9ed999t9syp9N5Q3VkxhdffKH+/fvrnXfeUY0aNZQvXz6NGjVKa9euzfQ2rKodwJ2LsArgjpU3b16VLFky0/2rVq2qWbNmKTg4WH5+fun2KVy4sNauXatatWpJki5evKgNGzaoatWq6favUKGCUlNTtXz5ctWrVy/N8ssjuykpKa62cuXKyel0av/+/RmOyJYtW9Z1sdhla9asuf5BXsOvv/6qhx9+WN27d3e17dq1K02/33//XfHx8a4gvmbNGvn6+qpo0aIKDAy8bu0AkBXMBgAA/99//vMfFSxYUM2bN9cvv/yiPXv2aNmyZerZs6cOHjwoSerVq5feeustzZ07V9u2bVP37t2vOUdqsWLFFBkZqY4dO2ru3LmubX755ZeSpNDQUDkcDs2fP1/Hjx9XXFyc8uXLp/79+6tPnz6aPn26du3apY0bN2r8+PGaPn26JKlr167asWOHBgwYoO3bt+vzzz/XtGnTMnWchw4d0ubNm93+nD59WqVKldL69eu1YMEC/fPPPxo0aJDWrVuXZv2kpCQ999xz+vvvv/XDDz/otdde03//+195eHhkqnYAyBKrT5oFgFvhygussrI8JibGREREmIIFCxqn02lKlChhOnfubM6cOWOMuXRBVa9evYyfn58JCAgwffv2NRERERleYGWMMfHx8aZPnz6mcOHCxtvb25QsWdJMmTLFtXzo0KGmUKFCxuFwmMjISGPMpYvCxo4da8LCwoyXl5cJCgoyDRo0MMuXL3et991335mSJUsap9NpHn30UTNlypRMXWAlKc2fGTNmmISEBBMVFWX8/f1NQECA6datm3n55ZdNpUqV0jxvgwcPNgUKFDC+vr6mc+fOJiEhwdXnerVzgRWArHAYk8FVAQAAAIDFOA0AAAAAtkVYBQAAgG0RVgEAAGBbhFUAAADYFmEVAAAAtkVYBQAAgG0RVgEAAGBbhFUAAADYFmEVAAAAtkVYBQAAgG0RVgEAAGBb/w+JgcTpSIgLAwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Confusion matrix visualization complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "Recall, and F1-Score."
      ],
      "metadata": {
        "id": "PQbwYwYuwEvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# --- 1. Create a dummy dataset for binary classification ---\n",
        "np.random.seed(42) # for reproducibility\n",
        "N_SAMPLES = 500\n",
        "\n",
        "# Numerical features\n",
        "feature_num1 = np.random.rand(N_SAMPLES) * 100\n",
        "feature_num2 = np.random.randn(N_SAMPLES) * 15 + 50\n",
        "feature_num3 = np.random.randint(0, 10, N_SAMPLES)\n",
        "\n",
        "# Categorical features\n",
        "gender = np.random.choice(['Male', 'Female'], N_SAMPLES, p=[0.5, 0.5])\n",
        "city = np.random.choice(['New York', 'London', 'Paris', 'Tokyo'], N_SAMPLES, p=[0.4, 0.3, 0.2, 0.1])\n",
        "\n",
        "# Binary Target variable:\n",
        "linear_combination = (0.5 * feature_num1 + 0.8 * feature_num2 - 0.3 * feature_num3 +\n",
        "                      (1 if gender[0] == 'Male' else 0) * 10 +\n",
        "                      (5 if city[0] == 'New York' else 0) +\n",
        "                      np.random.randn(N_SAMPLES) * 20)\n",
        "binary_target = (linear_combination > np.percentile(linear_combination, 50)).astype(int)\n",
        "\n",
        "# Introduce some missing values for demonstration of preprocessing\n",
        "missing_indices_num1 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.05), replace=False)\n",
        "missing_indices_num2 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.03), replace=False)\n",
        "feature_num1[missing_indices_num1] = np.nan\n",
        "feature_num2[missing_indices_num2] = np.nan\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Feature_Num1': feature_num1,\n",
        "    'Feature_Num2': feature_num2,\n",
        "    'Feature_Num3': feature_num3,\n",
        "    'Gender': gender,\n",
        "    'City': city,\n",
        "    'Target': binary_target # Our binary target\n",
        "})\n",
        "\n",
        "print(\"--- Dummy Dataset Head ---\")\n",
        "print(data.head())\n",
        "print(\"\\n--- Dummy Dataset Info ---\")\n",
        "data.info()\n",
        "print(\"\\n--- Target Variable Distribution ---\")\n",
        "print(data['Target'].value_counts())\n",
        "\n",
        "# --- 2. Separate features (X) and target (y) ---\n",
        "X = data[['Feature_Num1', 'Feature_Num2', 'Feature_Num3', 'Gender', 'City']]\n",
        "y = data['Target'] # Use the binary target\n",
        "\n",
        "# --- 3. Identify numerical and categorical columns for preprocessing ---\n",
        "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# --- 4. Create a preprocessing pipeline ---\n",
        "# This pipeline will handle missing values, scaling, and one-hot encoding.\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')), # Impute missing numerical values with mean\n",
        "    ('scaler', StandardScaler())                  # Scale numerical features\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), # Impute missing categorical values with mode\n",
        "    ('onehot', OneDotEncoder(handle_unknown='ignore'))     # One-hot encode categorical features\n",
        "])\n",
        "\n",
        "# Create a preprocessor using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# --- 5. Split the dataset into training and testing sets ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "print(f\"Training target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Testing target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
        "\n",
        "# --- Practical Question 12: Evaluate performance using Precision, Recall, and F1-Score ---\n",
        "print(\"\\n--- Question 12: Evaluate Performance using Precision, Recall, and F1-Score ---\")\n",
        "\n",
        "# Create a pipeline for the Logistic Regression model\n",
        "model_q12 = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                            ('classifier', LogisticRegression(random_state=42, max_iter=500))])\n",
        "\n",
        "# Train the model\n",
        "print(\"Training Logistic Regression model...\")\n",
        "model_q12.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_q12 = model_q12.predict(X_test)\n",
        "\n",
        "# --- Calculate and Print Evaluation Metrics ---\n",
        "\n",
        "# Accuracy: Overall correctness of the model.\n",
        "accuracy = accuracy_score(y_test, y_pred_q12)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Precision: The proportion of positive identifications that were actually correct.\n",
        "# High precision relates to a low false positive rate.\n",
        "# For binary classification, specify pos_label=1 (default) or the label of the positive class.\n",
        "precision = precision_score(y_test, y_pred_q12, pos_label=1)\n",
        "print(f\"Precision (for positive class): {precision:.4f}\")\n",
        "\n",
        "# Recall (Sensitivity or True Positive Rate): The proportion of actual positives\n",
        "# that were identified correctly. High recall relates to a low false negative rate.\n",
        "recall = recall_score(y_test, y_pred_q12, pos_label=1)\n",
        "print(f\"Recall (for positive class): {recall:.4f}\")\n",
        "\n",
        "# F1-Score: The harmonic mean of Precision and Recall.\n",
        "# It tries to find the balance between precision and recall.\n",
        "f1 = f1_score(y_test, y_pred_q12, pos_label=1)\n",
        "print(f\"F1-Score (for positive class): {f1:.4f}\")\n",
        "\n",
        "# --- Comprehensive Classification Report ---\n",
        "# The classification_report function provides a text report showing the main\n",
        "# classification metrics per class.\n",
        "print(\"\\n--- Full Classification Report ---\")\n",
        "print(classification_report(y_test, y_pred_q12, target_names=['Class 0 (Negative)', 'Class 1 (Positive)']))\n",
        "\n",
        "print(\"\\nEvaluation complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "id": "YF6NveWzwFof",
        "outputId": "01c3c6bd-3d45-4a28-ebcc-051cdf3a919e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dummy Dataset Head ---\n",
            "   Feature_Num1  Feature_Num2  Feature_Num3  Gender      City  Target\n",
            "0     37.454012     55.126340             0  Female  New York       0\n",
            "1     95.071431     78.142563             5  Female     Paris       1\n",
            "2     73.199394     64.256358             7    Male  New York       0\n",
            "3     59.865848     41.346445             2    Male  New York       0\n",
            "4     15.601864     36.523780             7  Female     Tokyo       0\n",
            "\n",
            "--- Dummy Dataset Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 6 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Feature_Num1  475 non-null    float64\n",
            " 1   Feature_Num2  485 non-null    float64\n",
            " 2   Feature_Num3  500 non-null    int64  \n",
            " 3   Gender        500 non-null    object \n",
            " 4   City          500 non-null    object \n",
            " 5   Target        500 non-null    int64  \n",
            "dtypes: float64(2), int64(2), object(2)\n",
            "memory usage: 23.6+ KB\n",
            "\n",
            "--- Target Variable Distribution ---\n",
            "Target\n",
            "0    250\n",
            "1    250\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'OneDotEncoder' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-83b14b3d6600>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m categorical_transformer = Pipeline(steps=[\n\u001b[1;32m     70\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'imputer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'most_frequent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Impute missing categorical values with mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0;34m'onehot'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneDotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# One-hot encode categorical features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m ])\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'OneDotEncoder' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "improve model performance."
      ],
      "metadata": {
        "id": "K79mxEfiwQdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# --- 1. Create a dummy dataset with significant class imbalance ---\n",
        "np.random.seed(42) # for reproducibility\n",
        "N_SAMPLES = 1000 # Increased samples to better demonstrate imbalance\n",
        "\n",
        "# Numerical features\n",
        "feature_num1 = np.random.rand(N_SAMPLES) * 100\n",
        "feature_num2 = np.random.randn(N_SAMPLES) * 15 + 50\n",
        "feature_num3 = np.random.randint(0, 10, N_SAMPLES)\n",
        "\n",
        "# Categorical features\n",
        "gender = np.random.choice(['Male', 'Female'], N_SAMPLES, p=[0.5, 0.5])\n",
        "city = np.random.choice(['New York', 'London', 'Paris', 'Tokyo'], N_SAMPLES, p=[0.4, 0.3, 0.2, 0.1])\n",
        "\n",
        "# Create an imbalanced binary target variable\n",
        "# Let's make class 1 (positive class) very rare, e.g., 10% of samples.\n",
        "imbalanced_target = np.zeros(N_SAMPLES, dtype=int)\n",
        "# Randomly select 10% of indices to be the minority class (class 1)\n",
        "minority_indices = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.10), replace=False)\n",
        "imbalanced_target[minority_indices] = 1\n",
        "\n",
        "# Introduce some missing values for demonstration of preprocessing\n",
        "missing_indices_num1 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.05), replace=False)\n",
        "missing_indices_num2 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.03), replace=False)\n",
        "feature_num1[missing_indices_num1] = np.nan\n",
        "feature_num2[missing_indices_num2] = np.nan\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Feature_Num1': feature_num1,\n",
        "    'Feature_Num2': feature_num2,\n",
        "    'Feature_Num3': feature_num3,\n",
        "    'Gender': gender,\n",
        "    'City': city,\n",
        "    'Target': imbalanced_target # Our imbalanced binary target\n",
        "})\n",
        "\n",
        "print(\"--- Dummy Imbalanced Dataset Head ---\")\n",
        "print(data.head())\n",
        "print(\"\\n--- Dummy Imbalanced Dataset Info ---\")\n",
        "data.info()\n",
        "print(\"\\n--- Imbalanced Target Variable Distribution ---\")\n",
        "print(data['Target'].value_counts())\n",
        "print(f\"Class 0 (Majority): {data['Target'].value_counts()[0]} samples ({data['Target'].value_counts(normalize=True)[0]:.2%})\")\n",
        "print(f\"Class 1 (Minority): {data['Target'].value_counts()[1]} samples ({data['Target'].value_counts(normalize=True)[1]:.2%})\")\n",
        "\n",
        "\n",
        "# --- 2. Separate features (X) and target (y) ---\n",
        "X = data[['Feature_Num1', 'Feature_Num2', 'Feature_Num3', 'Gender', 'City']]\n",
        "y = data['Target'] # Use the imbalanced target\n",
        "\n",
        "# --- 3. Identify numerical and categorical columns for preprocessing ---\n",
        "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# --- 4. Create a preprocessing pipeline ---\n",
        "# This pipeline will handle missing values, scaling, and one-hot encoding.\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')), # Impute missing numerical values with mean\n",
        "    ('scaler', StandardScaler())                  # Scale numerical features\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), # Impute missing categorical values with mode\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encode categorical features\n",
        "])\n",
        "\n",
        "# Create a preprocessor using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# --- 5. Split the dataset into training and testing sets ---\n",
        "# It's crucial to use stratify=y to ensure the rare class is present in both\n",
        "# training and testing sets, maintaining the imbalance ratio.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "print(f\"Training target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Testing target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
        "\n",
        "# --- Practical Question 13: Train Logistic Regression on imbalanced data and apply class weights ---\n",
        "print(\"\\n--- Question 13: Imbalanced Data with Class Weights Comparison ---\")\n",
        "\n",
        "# --- Model 1: Logistic Regression WITHOUT Class Weights ---\n",
        "print(\"\\n--- Training Model WITHOUT Class Weights ---\")\n",
        "model_no_weights = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                   ('classifier', LogisticRegression(random_state=42, max_iter=500))])\n",
        "\n",
        "model_no_weights.fit(X_train, y_train)\n",
        "y_pred_no_weights = model_no_weights.predict(X_test)\n",
        "\n",
        "print(\"\\n--- Performance WITHOUT Class Weights ---\")\n",
        "accuracy_no_weights = accuracy_score(y_test, y_pred_no_weights)\n",
        "precision_no_weights = precision_score(y_test, y_pred_no_weights, pos_label=1, zero_division=0)\n",
        "recall_no_weights = recall_score(y_test, y_pred_no_weights, pos_label=1, zero_division=0)\n",
        "f1_no_weights = f1_score(y_test, y_pred_no_weights, pos_label=1, zero_division=0)\n",
        "\n",
        "print(f\"Accuracy: {accuracy_no_weights:.4f}\")\n",
        "print(f\"Precision (Class 1): {precision_no_weights:.4f}\")\n",
        "print(f\"Recall (Class 1): {recall_no_weights:.4f}\")\n",
        "print(f\"F1-Score (Class 1): {f1_no_weights:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (WITHOUT Class Weights):\")\n",
        "print(confusion_matrix(y_test, y_pred_no_weights))\n",
        "print(\"\\nClassification Report (WITHOUT Class Weights):\")\n",
        "print(classification_report(y_test, y_pred_no_weights, target_names=['Class 0', 'Class 1'], zero_division=0))\n",
        "\n",
        "# --- Model 2: Logistic Regression WITH Class Weights ---\n",
        "print(\"\\n--- Training Model WITH Class Weights (class_weight='balanced') ---\")\n",
        "# Setting class_weight='balanced' automatically adjusts weights inversely proportional\n",
        "# to class frequencies in the input data. This gives more importance to the minority class.\n",
        "model_with_weights = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                     ('classifier', LogisticRegression(class_weight='balanced', random_state=42, max_iter=500))])\n",
        "\n",
        "model_with_weights.fit(X_train, y_train)\n",
        "y_pred_with_weights = model_with_weights.predict(X_test)\n",
        "\n",
        "print(\"\\n--- Performance WITH Class Weights ---\")\n",
        "accuracy_with_weights = accuracy_score(y_test, y_pred_with_weights)\n",
        "precision_with_weights = precision_score(y_test, y_pred_with_weights, pos_label=1, zero_division=0)\n",
        "recall_with_weights = recall_score(y_test, y_pred_with_weights, pos_label=1, zero_division=0)\n",
        "f1_with_weights = f1_score(y_test, y_pred_with_weights, pos_label=1, zero_division=0)\n",
        "\n",
        "print(f\"Accuracy: {accuracy_with_weights:.4f}\")\n",
        "print(f\"Precision (Class 1): {precision_with_weights:.4f}\")\n",
        "print(f\"Recall (Class 1): {recall_with_weights:.4f}\")\n",
        "print(f\"F1-Score (Class 1): {f1_with_weights:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (WITH Class Weights):\")\n",
        "print(confusion_matrix(y_test, y_pred_with_weights))\n",
        "print(\"\\nClassification Report (WITH Class Weights):\")\n",
        "print(classification_report(y_test, y_pred_with_weights, target_names=['Class 0', 'Class 1'], zero_division=0))\n",
        "\n",
        "# --- Comparison ---\n",
        "print(\"\\n--- Comparison of Models ---\")\n",
        "print(f\"Accuracy (No Weights): {accuracy_no_weights:.4f} | Accuracy (With Weights): {accuracy_with_weights:.4f}\")\n",
        "print(f\"Recall (Class 1, No Weights): {recall_no_weights:.4f} | Recall (Class 1, With Weights): {recall_with_weights:.4f}\")\n",
        "print(f\"F1-Score (Class 1, No Weights): {f1_no_weights:.4f} | F1-Score (Class 1, With Weights): {f1_with_weights:.4f}\")\n",
        "\n",
        "print(\"\\nObservation: You'll typically see a trade-off. Accuracy might slightly decrease (as the model is no longer optimizing for overall accuracy alone), but Recall and F1-Score for the minority class often improve significantly, indicating better detection of the rare class.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDQBij9NwSNK",
        "outputId": "0e42ab92-7b2d-480d-e76e-1879161b08aa"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dummy Imbalanced Dataset Head ---\n",
            "   Feature_Num1  Feature_Num2  Feature_Num3  Gender      City  Target\n",
            "0           NaN     52.665515             0  Female     Tokyo       0\n",
            "1     95.071431           NaN             8    Male     Paris       0\n",
            "2           NaN     55.702968             0    Male  New York       0\n",
            "3           NaN     59.158786             2    Male  New York       0\n",
            "4     15.601864           NaN             7    Male  New York       0\n",
            "\n",
            "--- Dummy Imbalanced Dataset Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 6 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Feature_Num1  950 non-null    float64\n",
            " 1   Feature_Num2  970 non-null    float64\n",
            " 2   Feature_Num3  1000 non-null   int64  \n",
            " 3   Gender        1000 non-null   object \n",
            " 4   City          1000 non-null   object \n",
            " 5   Target        1000 non-null   int64  \n",
            "dtypes: float64(2), int64(2), object(2)\n",
            "memory usage: 47.0+ KB\n",
            "\n",
            "--- Imbalanced Target Variable Distribution ---\n",
            "Target\n",
            "0    900\n",
            "1    100\n",
            "Name: count, dtype: int64\n",
            "Class 0 (Majority): 900 samples (90.00%)\n",
            "Class 1 (Minority): 100 samples (10.00%)\n",
            "\n",
            "Training set size: 800 samples\n",
            "Testing set size: 200 samples\n",
            "Training target distribution:\n",
            "Target\n",
            "0    0.9\n",
            "1    0.1\n",
            "Name: proportion, dtype: float64\n",
            "Testing target distribution:\n",
            "Target\n",
            "0    0.9\n",
            "1    0.1\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "--- Question 13: Imbalanced Data with Class Weights Comparison ---\n",
            "\n",
            "--- Training Model WITHOUT Class Weights ---\n",
            "\n",
            "--- Performance WITHOUT Class Weights ---\n",
            "Accuracy: 0.9000\n",
            "Precision (Class 1): 0.0000\n",
            "Recall (Class 1): 0.0000\n",
            "F1-Score (Class 1): 0.0000\n",
            "\n",
            "Confusion Matrix (WITHOUT Class Weights):\n",
            "[[180   0]\n",
            " [ 20   0]]\n",
            "\n",
            "Classification Report (WITHOUT Class Weights):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.90      1.00      0.95       180\n",
            "     Class 1       0.00      0.00      0.00        20\n",
            "\n",
            "    accuracy                           0.90       200\n",
            "   macro avg       0.45      0.50      0.47       200\n",
            "weighted avg       0.81      0.90      0.85       200\n",
            "\n",
            "\n",
            "--- Training Model WITH Class Weights (class_weight='balanced') ---\n",
            "\n",
            "--- Performance WITH Class Weights ---\n",
            "Accuracy: 0.5250\n",
            "Precision (Class 1): 0.1287\n",
            "Recall (Class 1): 0.6500\n",
            "F1-Score (Class 1): 0.2149\n",
            "\n",
            "Confusion Matrix (WITH Class Weights):\n",
            "[[92 88]\n",
            " [ 7 13]]\n",
            "\n",
            "Classification Report (WITH Class Weights):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.93      0.51      0.66       180\n",
            "     Class 1       0.13      0.65      0.21        20\n",
            "\n",
            "    accuracy                           0.53       200\n",
            "   macro avg       0.53      0.58      0.44       200\n",
            "weighted avg       0.85      0.53      0.62       200\n",
            "\n",
            "\n",
            "--- Comparison of Models ---\n",
            "Accuracy (No Weights): 0.9000 | Accuracy (With Weights): 0.5250\n",
            "Recall (Class 1, No Weights): 0.0000 | Recall (Class 1, With Weights): 0.6500\n",
            "F1-Score (Class 1, No Weights): 0.0000 | F1-Score (Class 1, With Weights): 0.2149\n",
            "\n",
            "Observation: You'll typically see a trade-off. Accuracy might slightly decrease (as the model is no longer optimizing for overall accuracy alone), but Recall and F1-Score for the minority class often improve significantly, indicating better detection of the rare class.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "evaluate performance."
      ],
      "metadata": {
        "id": "KvJ_5CPLwgYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# --- 1. Simulate Loading the Titanic Dataset ---\n",
        "# In a real scenario, you would load the actual Titanic dataset, e.g.:\n",
        "# df = pd.read_csv('titanic.csv')\n",
        "# For this program, we will create a synthetic dataset that mimics\n",
        "# the structure and common missing values of the Titanic dataset.\n",
        "\n",
        "np.random.seed(42) # for reproducibility\n",
        "N_SAMPLES = 891 # Typical number of samples in Titanic training set\n",
        "\n",
        "# Simulate 'Survived' (Target)\n",
        "survived = np.random.randint(0, 2, N_SAMPLES)\n",
        "\n",
        "# Simulate 'Pclass' (Categorical/Ordinal)\n",
        "pclass = np.random.choice([1, 2, 3], N_SAMPLES, p=[0.2, 0.3, 0.5])\n",
        "\n",
        "# Simulate 'Sex' (Categorical)\n",
        "sex = np.random.choice(['male', 'female'], N_SAMPLES, p=[0.65, 0.35])\n",
        "\n",
        "# Simulate 'Age' (Numerical, with missing values)\n",
        "age = np.random.normal(30, 15, N_SAMPLES)\n",
        "age[age < 0] = 0 # No negative ages\n",
        "age[age > 80] = 80 # Cap at 80\n",
        "# Introduce missing values in Age\n",
        "age_missing_indices = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.20), replace=False) # ~20% missing\n",
        "age[age_missing_indices] = np.nan\n",
        "\n",
        "# Simulate 'SibSp' (Numerical) - # of siblings/spouses aboard\n",
        "sibsp = np.random.randint(0, 4, N_SAMPLES)\n",
        "\n",
        "# Simulate 'Parch' (Numerical) - # of parents/children aboard\n",
        "parch = np.random.randint(0, 3, N_SAMPLES)\n",
        "\n",
        "# Simulate 'Fare' (Numerical)\n",
        "fare = np.random.lognormal(mean=3, sigma=1, size=N_SAMPLES) * 10 # Skewed distribution\n",
        "fare[fare > 500] = 500 # Cap at 500\n",
        "\n",
        "# Simulate 'Embarked' (Categorical, with some missing values)\n",
        "embarked = np.random.choice(['S', 'C', 'Q', np.nan], N_SAMPLES, p=[0.7, 0.15, 0.1, 0.05]) # ~5% missing\n",
        "\n",
        "# Create the DataFrame\n",
        "titanic_df = pd.DataFrame({\n",
        "    'Pclass': pclass,\n",
        "    'Sex': sex,\n",
        "    'Age': age,\n",
        "    'SibSp': sibsp,\n",
        "    'Parch': parch,\n",
        "    'Fare': fare,\n",
        "    'Embarked': embarked,\n",
        "    'Survived': survived\n",
        "})\n",
        "\n",
        "print(\"--- Simulated Titanic Dataset Head ---\")\n",
        "print(titanic_df.head())\n",
        "print(\"\\n--- Simulated Titanic Dataset Info ---\")\n",
        "titanic_df.info()\n",
        "print(\"\\n--- Missing Values Count ---\")\n",
        "print(titanic_df.isnull().sum())\n",
        "print(\"\\n--- Target Variable Distribution ---\")\n",
        "print(titanic_df['Survived'].value_counts(normalize=True))\n",
        "\n",
        "# --- 2. Separate features (X) and target (y) ---\n",
        "X = titanic_df.drop('Survived', axis=1)\n",
        "y = titanic_df['Survived']\n",
        "\n",
        "# --- 3. Identify numerical and categorical columns for preprocessing ---\n",
        "# Note: 'Pclass' is often treated as categorical in Titanic, but here we'll keep it numerical\n",
        "# for simplicity with the default numerical transformer. If you want to treat it as categorical,\n",
        "# move it to `categorical_cols`.\n",
        "numerical_cols = ['Age', 'Fare', 'SibSp', 'Parch']\n",
        "categorical_cols = ['Pclass', 'Sex', 'Embarked'] # Treating Pclass as categorical here for better modeling\n",
        "\n",
        "# --- 4. Create a preprocessing pipeline for handling missing values and scaling/encoding ---\n",
        "# Numerical transformer: Impute missing values with mean, then scale using StandardScaler.\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Categorical transformer: Impute missing values with the most frequent value, then one-hot encode.\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore')) # handle_unknown='ignore' prevents errors on unseen categories\n",
        "])\n",
        "\n",
        "# Combine transformers using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ],\n",
        "    remainder='passthrough' # Keep any other columns not specified (if any)\n",
        ")\n",
        "\n",
        "# --- 5. Split the dataset into training and testing sets ---\n",
        "# Use stratify=y to ensure the proportion of 'Survived' is similar in both sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "print(f\"Training target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Testing target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
        "\n",
        "# --- Practical Question 14: Train Logistic Regression on Titanic data, handle missing values, evaluate performance ---\n",
        "print(\"\\n--- Question 14: Logistic Regression on Simulated Titanic Dataset ---\")\n",
        "\n",
        "# Create the full pipeline: Preprocessing + Logistic Regression model\n",
        "model_titanic = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                ('classifier', LogisticRegression(random_state=42, max_iter=1000))]) # Increased max_iter for convergence\n",
        "\n",
        "# Train the model\n",
        "print(\"\\nTraining Logistic Regression model on simulated Titanic dataset...\")\n",
        "model_titanic.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_titanic = model_titanic.predict(X_test)\n",
        "\n",
        "# --- Evaluate Model Performance ---\n",
        "print(\"\\n--- Model Performance Evaluation ---\")\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_titanic)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Classification Report (Precision, Recall, F1-Score)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_titanic, target_names=['Not Survived (0)', 'Survived (1)']))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred_titanic)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Visualize Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Predicted Not Survived', 'Predicted Survived'],\n",
        "            yticklabels=['Actual Not Survived', 'Actual Survived'],\n",
        "            linewidths=.5, linecolor='black')\n",
        "plt.title('Confusion Matrix for Titanic Survival Prediction')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# Visualize Confusion Matrix using Scikit-learn's ConfusionMatrixDisplay\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Not Survived', 'Survived'])\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "disp.plot(cmap=plt.cm.Blues, ax=ax, values_format='d')\n",
        "ax.set_title('Confusion Matrix (using ConfusionMatrixDisplay)')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nPerformance evaluation complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ox2U_EIWwiFa",
        "outputId": "dd3b7f06-fe84-4c38-d822-3ab01ae60b1e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Simulated Titanic Dataset Head ---\n",
            "   Pclass     Sex        Age  SibSp  Parch        Fare Embarked  Survived\n",
            "0       3    male  39.834156      1      1  500.000000        C         0\n",
            "1       3    male  52.915854      1      2  156.525822        S         1\n",
            "2       2    male  24.643464      3      2  500.000000        S         0\n",
            "3       3    male        NaN      2      1   25.651918        S         0\n",
            "4       2  female  38.387389      2      0  286.804788        S         0\n",
            "\n",
            "--- Simulated Titanic Dataset Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 891 entries, 0 to 890\n",
            "Data columns (total 8 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   Pclass    891 non-null    int64  \n",
            " 1   Sex       891 non-null    object \n",
            " 2   Age       713 non-null    float64\n",
            " 3   SibSp     891 non-null    int64  \n",
            " 4   Parch     891 non-null    int64  \n",
            " 5   Fare      891 non-null    float64\n",
            " 6   Embarked  891 non-null    object \n",
            " 7   Survived  891 non-null    int64  \n",
            "dtypes: float64(2), int64(4), object(2)\n",
            "memory usage: 55.8+ KB\n",
            "\n",
            "--- Missing Values Count ---\n",
            "Pclass        0\n",
            "Sex           0\n",
            "Age         178\n",
            "SibSp         0\n",
            "Parch         0\n",
            "Fare          0\n",
            "Embarked      0\n",
            "Survived      0\n",
            "dtype: int64\n",
            "\n",
            "--- Target Variable Distribution ---\n",
            "Survived\n",
            "1    0.508418\n",
            "0    0.491582\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Training set size: 712 samples\n",
            "Testing set size: 179 samples\n",
            "Training target distribution:\n",
            "Survived\n",
            "1    0.508427\n",
            "0    0.491573\n",
            "Name: proportion, dtype: float64\n",
            "Testing target distribution:\n",
            "Survived\n",
            "1    0.50838\n",
            "0    0.49162\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "--- Question 14: Logistic Regression on Simulated Titanic Dataset ---\n",
            "\n",
            "Training Logistic Regression model on simulated Titanic dataset...\n",
            "Model training complete.\n",
            "\n",
            "--- Model Performance Evaluation ---\n",
            "Accuracy: 0.4860\n",
            "\n",
            "Classification Report:\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "Not Survived (0)       0.47      0.38      0.42        88\n",
            "    Survived (1)       0.50      0.59      0.54        91\n",
            "\n",
            "        accuracy                           0.49       179\n",
            "       macro avg       0.48      0.48      0.48       179\n",
            "    weighted avg       0.48      0.49      0.48       179\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[33 55]\n",
            " [37 54]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAArcAAAIjCAYAAAAZajMiAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAcVxJREFUeJzt3XlcVdX+//H3AeSATM4iiYio4ESmlTnkkBNmSQ6ZSqnlUGY5NGhWzhpmg6bdtLKc0qul6bVJMws1p1sW6k0jIU1M1NIER1RYvz/8cn4egSPHIDzH17PHfjw4a6+99tonDnz88NlrW4wxRgAAAIAb8CjuCQAAAACFheAWAAAAboPgFgAAAG6D4BYAAABug+AWAAAAboPgFgAAAG6D4BYAAABug+AWAAAAboPgFgAAAG6D4BY3tL1796pdu3YKCgqSxWLRypUrC3X8/fv3y2KxaN68eYU6ritr2bKlWrZsWahjpqamysfHR5s2bSrUcXPMmzdPFotF+/fvL5Lxr3ffffedmjRpIj8/P1ksFiUmJhbq+AkJCbJYLEpISCjUcV3BP/0zoig+f5erWrWq+vbtW2Tjr169Wv7+/vrjjz+K7BxwfQS3KHYpKSl69NFHVa1aNfn4+CgwMFBNmzbVG2+8obNnzxbpufv06aNdu3Zp8uTJWrhwoW699dYiPd8/qW/fvrJYLAoMDMzzfdy7d68sFossFoteffVVp8c/dOiQxo0bV+iBzrWYMGGCGjVqpKZNmxb3VIpMYmKiHnzwQYWGhspqtapMmTJq06aN5s6dq6ysrCI774ULF3T//ffr+PHjmjZtmhYuXKiwsLAiO19Ryvl+79+/f577X3jhBVufP//80+nxP//8c40bN+5vzrLgcj7jOZu/v7+qVaumbt26afny5crOzv7H5vJPiYmJUfXq1RUfH1/cU8H1zADF6NNPPzW+vr6mVKlSZsiQIeadd94xb775punRo4cpUaKEGTBgQJGd+8yZM0aSeeGFF4rsHNnZ2ebs2bPm4sWLRXaO/PTp08d4eXkZT09Ps3Tp0lz7x44da3x8fIwk88orrzg9/nfffWckmblz5zp1XGZmpsnMzHT6fPk5evSoKVGihFm8eHGhjXmlixcvmrNnz5rs7OwiO4cj7777rvH09DQhISFm5MiRZs6cOWbatGnmnnvuMRaLxUyePLnIzr1nzx4jybz77rtFdo6srCxz9uxZk5WVVWTnMMYYScbHx8eUKlUqz+/B8PBw22fijz/+cHr8wYMHG2d/rf6dnxF9+vQxVqvVLFy40CxcuNC888475oUXXjDR0dFGkmnZsqVJT0+3O6awP39XCgsLM3369Cmy8Y0x5q233jIlS5Y0GRkZRXoeuC4ytyg2+/btU48ePRQWFqbdu3frjTfe0IABAzR48GD9+9//1u7du1WnTp0iO3/On7VKlSpVZOewWCzy8fGRp6dnkZ3DEavVqtatW+vf//53rn2LFy9Wx44d/7G5nDlzRpLk7e0tb2/vQhv3gw8+kJeXl+69995CG/NKnp6e8vHxkcViKbJz5Gfr1q167LHH1LhxY/3888+aMmWK+vXrp2HDhumTTz7Rf//7X4WEhBTZ+Y8ePSqpaD8nHh4e8vHxkYdH0f9KiomJUUZGhr744gu79s2bN2vfvn3/2Gfi4sWLOn/+/N/+GeHl5aUHH3xQDz74oAYMGKBJkyZpx44dio+PV0JCggYMGGDXv7A/f8Wha9euyszM1EcffVTcU8H1qrija9y4HnvsMSPJbNq0qUD9L1y4YCZMmGCqVatmvL29TVhYmBk1apQ5d+6cXb+wsDDTsWNHs3HjRnPbbbcZq9VqwsPDzfz58219xo4dayTZbWFhYcaYS9mQnK8vl3PM5b788kvTtGlTExQUZPz8/EzNmjXNqFGjbPv37duXZ3Zz3bp1plmzZqZkyZImKCjIdOrUyezevTvP8+3du9f06dPHBAUFmcDAQNO3b19z+vTpq75fffr0MX5+fmbevHnGarWav/76y7bvv//9r5Fkli9fnitze+zYMfP000+bunXrGj8/PxMQEGBiYmJMYmKirc8333yT6/27/DpbtGhh6tSpY77//ntz5513Gl9fXzN06FDbvhYtWtjG6t27t7Farbmuv127dqZUqVLm999/d3idzZs3Ny1btszVnl8G6crzG2PMjBkzTO3atW1/RWjYsKFZtGiRbf/cuXONJLNv3z678a/2fZZjx44dpnnz5sbHx8fcdNNNZuLEieb999/PNWZeYmJijJeXl/ntt98c9stx6tQp89RTT5nKlSsbb29vU7NmTfPKK6/kyjpLMoMHDzYrVqwwderUMd7e3qZ27drmiy++sPXp06dPrv/HOe9dXu9jzjFXfn7+/e9/mwYNGhh/f38TEBBg6tata6ZPn27bn/P99M0339gd9+GHH5oGDRoYHx8fU7ZsWRMXF2cOHjyY63x+fn7m4MGDJjY21vj5+Zly5cqZp59+Olc2NOeaW7Zsabp372637/HHHzf16tWzfe4uz9xu2LDBdOvWzYSGhhpvb29TuXJlM2zYMHPmzBmH71XOz4ucnwOvvPKKmTZtmqlWrZrx8PAwP/74Y66fEUeOHDHlypUzLVq0sPt/tnfvXlOyZEm7eedce37atWtnLBaLSUpKsrVdy/d/znuyZ88ec//995uAgABTpkwZM2TIEHP27Fm7sa783BXk58nJkydNyZIlzZAhQ3JdQ2pqqvHw8DAvvfSSXfstt9xiOnXqlO+148ZG5hbF5pNPPlG1atXUpEmTAvXv37+/xowZowYNGmjatGlq0aKF4uPj1aNHj1x9k5OT1a1bN7Vt21avvfaaSpcurb59++qnn36SJHXp0kXTpk2TJPXs2VMLFy7U9OnTnZr/Tz/9pHvuuUeZmZmaMGGCXnvtNXXq1OmqNzV99dVXat++vY4ePapx48bpqaee0ubNm9W0adM8b1jq3r27Tp48qfj4eHXv3l3z5s3T+PHjCzzPLl26yGKx6OOPP7a1LV68WFFRUWrQoEGu/r/++qtWrlype+65R6+//rqeffZZ7dq1Sy1atNChQ4ckSbVq1dKECRMkSQMHDtTChQu1cOFCNW/e3DbOsWPH1KFDB9WvX1/Tp09Xq1at8pzfG2+8ofLly6tPnz622tG3335bX375pWbOnOkwK3nhwgV99913eV5HQb377rsaMmSIateurenTp2v8+PGqX7++tm3bdtVjr/Z9Jkm///67WrVqpZ9++kmjRo3S8OHDtWjRIr3xxhtXHf/MmTNat26dmjdvripVqly1vzFGnTp10rRp0xQTE6PXX39dkZGRevbZZ/XUU0/l6v/tt9/q8ccfV48ePTR16lSdO3dOXbt21bFjxyRJjz76qJ5//nlJ0pAhQ7Rw4UK98MILV53H5dauXauePXuqdOnSevnllzVlyhS1bNnyqp+TefPmqXv37vL09FR8fLwGDBigjz/+WM2aNdOJEyfs+mZlZal9+/YqW7asXn31VbVo0UKvvfaa3nnnnTzH7tWrlz755BOdOnVK0qUs6kcffaRevXrl2f+jjz7SmTNnNGjQIM2cOVPt27fXzJkz1bt3b1ufRx99VG3btpUk2+dh4cKFduPMnTtXM2fO1MCBA/Xaa6+pTJkyuc5VoUIFzZo1S+vXr9fMmTMlSdnZ2erbt68CAgL01ltvOXzfLvfQQw/JGKO1a9fm28eZ7//u3bvr3Llzio+P1913360ZM2Zo4MCBDudQkJ8n/v7+6ty5s5YuXZqrfvzf//63jDGKi4uza2/YsKE2b95c0LcCN5piDq5xg0pPTzeSTGxsbIH6JyYmGkmmf//+du3PPPOMkWS+/vprW1tYWJiRZDZs2GBrO3r0qLFarebpp5+2tV2eTblcQTO306ZNu2ptXl6Z2/r165sKFSqYY8eO2dp27NhhPDw8TO/evXOd75FHHrEbs3PnzqZs2bL5nvPy68jJ6nTr1s20bt3aGHOpvjE4ONiMHz8+z/fg3LlzuWof9+3bZ6xWq5kwYYKtzVHNbYsWLYwkM3v27Dz3XZk5WrNmjZFkJk2aZH799Vfj7+9v7rvvvqteY3JyspFkZs6cmWtfQTO3sbGxpk6dOg7Pk1/mtiDfZ08++aSxWCzmxx9/tLUdO3bMlClT5qqZ2x07dhhJtqz31axcudL2Pl6uW7duxmKxmOTkZFubJOPt7W3XlnO+y9/PnKzqRx99ZDdmQTO3Q4cONYGBgQ5rSq/M3J4/f95UqFDB1K1b1y4z+OmnnxpJZsyYMXbnk2T3vWnMpcxew4YN7dr0f5nb48ePG29vb7Nw4UJjjDGfffaZsVgsZv/+/Xlmbi/P0OaIj483FovFLqOeX81tzucsMDDQHD16NM99V36OevbsaUqWLGl++eUX88orrxhJZuXKlXZ9rpa5/fHHH40kM3z4cFvbtXz/57wnV2ZKH3/8cSPJ7Nixw9Z25eeuoD9Pcn4GXP6XA2OMiY6OzvP77KWXXjKSzJEjRxzOHTcmMrcoFhkZGZKkgICAAvX//PPPJSlX9unpp5+WJH322Wd27bVr19add95pe12+fHlFRkbq119/veY5XymnBvE///lPge9KTktLU2Jiovr27WuXtYmOjlbbtm1t13m5xx57zO71nXfeqWPHjtnew4Lo1auXEhISdPjwYX399dc6fPhwvlkqq9Vqq33MysrSsWPH5O/vr8jISP3www8FPqfVatXDDz9coL7t2rXTo48+qgkTJqhLly7y8fHR22+/fdXjcjKMpUuXLvC8rlSqVCkdPHhQ3333ndPHFuT7bPXq1WrcuLHq169vaytTpkyuTFReruVz4unpqSFDhti1P/300zLG5KozbdOmjSIiImyvo6OjFRgYWOifk9OnTzvMHl7p+++/19GjR/X444/Lx8fH1t6xY0dFRUXl+rxLeX9O8ruO0qVLKyYmxlaLvnjxYjVp0iTfVSB8fX1tX58+fVp//vmnmjRpImOMfvzxxwJfV9euXVW+fPkC9X3zzTcVFBSkbt26afTo0XrooYcUGxtb4HNJlzKiknTy5Ml8+zjz/T948GC7108++aQk5flzK0dBf560adNGISEhWrRoka3tf//7n3bu3KkHH3ww17g5n/lrWdUC7o/gFsUiMDBQkuMfupf77bff5OHhoerVq9u1BwcHq1SpUvrtt9/s2vP6E27p0qX1119/XeOMc3vggQfUtGlT9e/fXxUrVlSPHj304YcfOgx0c+YZGRmZa1+tWrX0559/6vTp03btV15Lzg91Z67l7rvvVkBAgJYuXapFixbptttuy/Ve5sjOzta0adNUo0YNWa1WlStXTuXLl9fOnTuVnp5e4HPedNNNTt248uqrr6pMmTJKTEzUjBkzVKFChQIfa4wpcN8rjRw5Uv7+/rr99ttVo0YNDR48uMDr5Rbk++y3337L873O7/2/3LV8TkJCQnIFw7Vq1bLtv9w/8Tl5/PHHVbNmTXXo0EGVK1fWI488otWrVzs8xtHnJCoqKtd1+Pj45Aoar3YdvXr10tq1a3XgwAGtXLky33/sSdKBAwds/yD19/dX+fLl1aJFC0ly6jMRHh5e4L5lypTRjBkztHPnTgUFBWnGjBkFPjZHTtmFo38cOfP9X6NGDbvXERER8vDwcLj+c0F/nnh4eCguLk4rV6603Xy6aNEi+fj46P777881bs5nvjhu8sT1j+AWxSIwMFAhISH63//+59RxBf1Blt+dxwUJgvI7x5W1YL6+vtqwYYO++uorPfTQQ9q5c6ceeOABtW3btlDXHf0715LDarWqS5cumj9/vlasWOHwF/lLL72kp556Ss2bN9cHH3ygNWvWaO3atapTp45T62Zenu0qiB9//NF2Z/6uXbsKdEzZsmUl5R3oF/T/Y61atZSUlKQlS5aoWbNmWr58uZo1a6axY8de9fyF8f/GkerVq8vLy6vA74ez/onPSYUKFZSYmKhVq1apU6dO+uabb9ShQwf16dPH+Qnn41pWGujUqZOsVqv69OmjzMxMde/ePc9+WVlZatu2rT777DONHDlSK1eu1Nq1a20PXSjKz8SaNWskXfr+PnjwoFPHSrL9fHX0D6m/8/1fkJ/Hzvw86d27t06dOqWVK1fKGKPFixfrnnvuUVBQUK5xcz7z5cqVu+occOMhuEWxueeee5SSkqItW7ZctW9YWJiys7O1d+9eu/YjR47oxIkThbqofOnSpXPdsCLlznpJl7INrVu31uuvv67du3dr8uTJ+vrrr/XNN9/kOXbOPJOSknLt+/nnn1WuXDn5+fn9vQvIR69evfTjjz/q5MmTed6El2PZsmVq1aqV3nvvPfXo0UPt2rVTmzZtcr0nhZkxOX36tB5++GHVrl1bAwcO1NSpUwv0Z9IqVarI19dX+/bty7XPmf+Pfn5+euCBBzR37lwdOHBAHTt21OTJk3Xu3Llrup7LhYWFKTk5OVd7Xm1XKlmypO666y5t2LBBqampBTrXoUOHcmV6f/75Z9v+wuLM++vt7a17771Xb731lu2hLQsWLMj3PXD0OUlKSiqU6/D19dV9992nhIQEtW3bNt8gadeuXfrll1/02muvaeTIkYqNjbX9Cf1KhfmZWL16tebMmaMRI0bYbri8ePGiU2MsXLhQFovFdqNbfgr6/X/lz9/k5GRlZ2eratWq+Y5d0J8nklS3bl3dcsstWrRokTZu3KgDBw7ooYceynPcffv22bLAwJUIblFsRowYIT8/P/Xv319HjhzJtT8lJcV2R/ndd98tSblWNHj99dclqVDXpoyIiFB6erp27txpa0tLS9OKFSvs+h0/fjzXsTl1lZmZmXmOXalSJdWvX1/z58+3++H+v//9T19++aXtOotCq1atNHHiRL355psKDg7Ot5+np2euzN1HH32k33//3a4tJwjP65eUs0aOHKkDBw5o/vz5ev3111W1alVbRs2REiVK6NZbb9X333+fa19ERIS2bt2q8+fP29o+/fTTXEFiTt1uDm9vb9WuXVvGGF24cOFvXNUl7du315YtW+ye5Hb8+HG72kJHxo4dK2OMHnroIdufmS+3fft2zZ8/X9Klz0lWVpbefPNNuz7Tpk2TxWJRhw4drv1CrhAREaGff/7Z7jGoO3bsyPUn7SvfXw8PD0VHR0vK/3Ny6623qkKFCpo9e7Zdny+++EJ79uwptM/7M888o7Fjx2r06NH59snJCl/+mTDG5LnaRWF9Jk6cOKH+/fvr9ttv10svvaQ5c+bohx9+0EsvvVTgMaZMmaIvv/xSDzzwQK5ygss58/3/r3/9y+51zmoOjr6vCvrzJMdDDz2kL7/8UtOnT1fZsmXzHXv79u1q3LhxvufFjc2ruCeAG1dERIQWL16sBx54QLVq1VLv3r1Vt25dnT9/Xps3b9ZHH31ke0b5zTffrD59+uidd97RiRMn1KJFC/33v//V/Pnzdd999+W7zNS16NGjh0aOHKnOnTtryJAhOnPmjGbNmqWaNWva3QAxYcIEbdiwQR07dlRYWJiOHj2qt956S5UrV1azZs3yHf+VV15Rhw4d1LhxY/Xr109nz57VzJkzFRQUVKSP7vTw8NCLL7541X733HOPJkyYoIcfflhNmjTRrl27tGjRIlWrVs2uX0REhEqVKqXZs2crICBAfn5+atSokVN1hZL09ddf66233tLYsWNtS3rNnTtXLVu21OjRozV16lSHx8fGxuqFF15QRkaGrUZVurR03LJlyxQTE6Pu3bsrJSVFH3zwgd0NVNKlm9mCg4PVtGlTVaxYUXv27NGbb76pjh07FvhGLkdGjBihDz74QG3bttWTTz4pPz8/zZkzR1WqVNHx48evmu1r0qSJ/vWvf+nxxx9XVFSUHnroIdWoUUMnT55UQkKCVq1apUmTJkmS7r33XrVq1UovvPCC9u/fr5tvvllffvml/vOf/2jYsGG5rv3veOSRR/T666+rffv26tevn44eParZs2erTp06djc79u/fX8ePH9ddd92lypUr67ffftPMmTNVv359Wy3wlUqUKKGXX35ZDz/8sFq0aKGePXvqyJEjeuONN1S1alUNHz68UK7h5ptv1s033+ywT1RUlCIiIvTMM8/o999/V2BgoJYvX55nKUzDhg0lXVo2rX379vL09HT4V5L8DB06VMeOHdNXX30lT09PxcTEqH///po0aZJiY2Pt5nzx4kV98MEHkqRz587pt99+06pVq7Rz5061atUq3+XQcjjz/b9v3z516tRJMTEx2rJliz744AP16tXL4XtY0J8nOXr16qURI0ZoxYoVGjRokEqUKJGrz9GjR7Vz585cN7gBNv/8Ag2AvV9++cUMGDDAVK1a1Xh7e5uAgADTtGlTM3PmTLsHNFy4cMGMHz/ehIeHmxIlSpjQ0FCHD3G40pVL4OS3FJgxlx7OULduXePt7W0iIyPNBx98kGspsHXr1pnY2FgTEhJivL29TUhIiOnZs6f55Zdfcp3jymV+vvrqK9O0aVPj6+trAgMDzb333pvvQxyuXGosr2Wp8nK1ZYLyew/OnTtnnn76aVOpUiXj6+trmjZtarZs2ZLn0k//+c9/TO3atY2Xl1eeD3HIy+XjZGRkmLCwMNOgQQNz4cIFu37Dhw83Hh4eZsuWLQ6v4ciRI8bLy8u2rNPlXnvtNXPTTTcZq9VqmjZtar7//vtc1/H222+b5s2bm7Jlyxqr1WoiIiLMs88+a/fYUkcPcXB0fTl+/PFHc+eddxqr1WoqV65s4uPjzYwZM4wkc/jwYYfXl2P79u2mV69eJiQkxJQoUcKULl3atG7d2syfP99uqaWTJ0+a4cOH2/rVqFHD4UMcrnTlUk75LQVmjDEffPCB7aEq9evXN2vWrMm1FNiyZctMu3btTIUKFYy3t7epUqWKefTRR01aWlquc1z5EIelS5eaW265xVitVlOmTBmHD3G4Ul4PXcnvmvM67vLP3e7du02bNm2Mv7+/KVeunBkwYIBt2bTLP9sXL140Tz75pClfvryxWCx5PsThSlf+jPjPf/5jJJnXXnvNrl/OZ+Xmm28258+ft127LntgRMmSJU3VqlVN165dzbJly/J8nPG1fP/nvCe7d+823bp1MwEBAaZ06dLmiSeeuOpDHJz5eZLj7rvvNpLM5s2b89w/a9YsHr8LhyzGFNKdDwBQTPr166dffvlFGzduLO6pFNiwYcP09ttv69SpU8X2eGagIMaNG6fx48frjz/++Edu4OrcubN27dqVb032LbfcopYtW9oexANciZpbAC5v7Nix+u677wq8hNc/7ezZs3avjx07poULF6pZs2YEtsBl0tLS9Nlnn+V7I9nq1au1d+9ejRo16h+eGVwJNbcAXF6VKlUKZWWDotK4cWO1bNlStWrV0pEjR/Tee+8pIyPD4Y1MwI1k37592rRpk+bMmaMSJUro0UcfzbNfTExMnjdWApcjuAWAInb33Xdr2bJleuedd2SxWNSgQQO99957at68eXFPDbgurF+/Xg8//LCqVKmi+fPnO1zRBbgaam4BAADglJxa7MtFRkba1tVu2bKl1q9fb7f/0Ucf1ezZs/Md0xijsWPH6t1339WJEyfUtGlTzZo1y+Fydnmh5hYAAABOq1OnjtLS0mzbt99+a7d/wIABdvuvtrTj1KlTNWPGDM2ePVvbtm2Tn5+f2rdv73TZGWUJAAAAcJqXl5fDEpKSJUsWuMTEGKPp06frxRdfVGxsrCRpwYIFqlixolauXOnUmtEEtze47OxsHTp0SAEBAYX66EgAANyRMUYnT55USEiIPDz++T+Anzt3zu7Ji4XJGJMrFrBarbJarXn237t3r0JCQuTj46PGjRsrPj5eVapUse1ftGiRPvjgAwUHB+vee+/V6NGjVbJkyTzH2rdvnw4fPqw2bdrY2oKCgtSoUSNt2bKF4BYFd+jQIYWGhhb3NAAAcCmpqamqXLnyP3rOc+fOyTegrHTxTJGM7+/vn2s1irFjx+b59MxGjRpp3rx5ioyMVFpamsaPH68777xT//vf/xQQEKBevXopLCxMISEh2rlzp0aOHKmkpCR9/PHHeZ778OHDkqSKFSvatVesWNG2r6AIbm9wOY9XTN6XqoDLHl0KoPhVaflMcU8BwBVM1nmd3z2/UB7P7azz589LF8/IWruP5OlduINnndep3fOVmppq9yjz/LK2HTp0sH0dHR2tRo0aKSwsTB9++KH69eungQMH2vbXq1dPlSpVUuvWrZWSklKojwLPC8HtDS7nzw8BgYF238wAip+lsH95ASg0xVrK5+VT6D8fjOVSiUXgNcYDpUqVUs2aNfN9slyjRo0kScnJyXkGtzm1uUeOHFGlSpVs7UeOHFH9+vWdmgurJQAAALgSiySLpZC3vzelU6dOKSUlxS4wvVxiYqIk5bs/PDxcwcHBWrduna0tIyND27ZtU+PGjZ2aC8EtAAAAnPLMM89o/fr12r9/vzZv3qzOnTvL09NTPXv2VEpKiiZOnKjt27dr//79WrVqlXr37q3mzZsrOjraNkZUVJRWrFgh6VImfNiwYZo0aZJWrVqlXbt2qXfv3goJCdF9993n1NwoSwAAAHAlFo9LW2GP6YSDBw+qZ8+eOnbsmMqXL69mzZpp69atKl++vM6dO6evvvpK06dP1+nTpxUaGqquXbvqxRdftBsjKSlJ6enpttcjRozQ6dOnNXDgQJ04cULNmjXT6tWr5ePj49yl8ISyG1tGRoaCgoJ05Fg6NbfAdab0bU8U9xQAXMFknVfmrneVnv7P/97M+Z1trT9IFs+8b/S6ViYrU5mJs4rlugobmVsAAABXklMnW9hjuglqbgEAAOA2yNwCAAC4kuug5vZ65j5XAgAAgBsemVsAAABXQs2tQwS3AAAALqUIyhLc6I/57nMlAAAAuOGRuQUAAHAllCU4ROYWAAAAboPMLQAAgCthKTCH3OdKAAAAcMMjcwsAAOBKqLl1iMwtAAAA3AaZWwAAAFdCza1DBLcAAACuhLIEh9wnTAcAAMANj8wtAACAK6EswSH3uRIAAADc8MjcAgAAuBKLpQgyt9TcAgAAANcdMrcAAACuxMNyaSvsMd0EmVsAAAC4DTK3AAAAroTVEhwiuAUAAHAlPMTBIfcJ0wEAAHDDI3MLAADgSihLcMh9rgQAAAA3PDK3AAAAroSaW4fI3AIAAMBtkLkFAABwJdTcOuQ+VwIAAIAbHplbAAAAV0LNrUMEtwAAAK6EsgSH3OdKAAAAcMMjcwsAAOBKKEtwiMwtAAAA3AaZWwAAAJdSBDW3bpTvdJ8rAQAAwA2PzC0AAIAroebWITK3AAAAcBtkbgEAAFyJxVIE69ySuQUAAEBxyHmIQ2FvThg3bpwsFovdFhUVJUk6fvy4nnzySUVGRsrX11dVqlTRkCFDlJ6e7nDMvn375hozJibG6beHzC0AAACcVqdOHX311Ve2115el8LKQ4cO6dChQ3r11VdVu3Zt/fbbb3rsscd06NAhLVu2zOGYMTExmjt3ru211Wp1el4EtwAAAK7kOrmhzMvLS8HBwbna69atq+XLl9teR0REaPLkyXrwwQd18eJFWxCcF6vVmueYzqAsAQAAAJKkjIwMuy0zMzPfvnv37lVISIiqVaumuLg4HThwIN++6enpCgwMdBjYSlJCQoIqVKigyMhIDRo0SMeOHXP6GghuAQAAXEkR1tyGhoYqKCjItsXHx+c5hUaNGmnevHlavXq1Zs2apX379unOO+/UyZMnc/X9888/NXHiRA0cONDhZcXExGjBggVat26dXn75Za1fv14dOnRQVlaWU28PZQkAAACQJKWmpiowMND2Or+a1w4dOti+jo6OVqNGjRQWFqYPP/xQ/fr1s+3LyMhQx44dVbt2bY0bN87huXv06GH7ul69eoqOjlZERIQSEhLUunXrAl8DmVsAAABXklNzW9ibpMDAQLutoDd0lSpVSjVr1lRycrKt7eTJk4qJiVFAQIBWrFihEiVKOHWZ1apVU7ly5ezGLAiCWwAAAPwtp06dUkpKiipVqiTpUsa2Xbt28vb21qpVq+Tj4+P0mAcPHtSxY8dsYxYUwS0AAIAruQ7WuX3mmWe0fv167d+/X5s3b1bnzp3l6empnj172gLb06dP67333lNGRoYOHz6sw4cP29XPRkVFacWKFZIuBcfPPvustm7dqv3792vdunWKjY1V9erV1b59e6fmRs0tAACAK7kOlgI7ePCgevbsqWPHjql8+fJq1qyZtm7dqvLlyyshIUHbtm2TJFWvXt3uuH379qlq1aqSpKSkJNuDHTw9PbVz507Nnz9fJ06cUEhIiNq1a6eJEyc6vdYtwS0AAACcsmTJknz3tWzZUsaYq45xeR9fX1+tWbOmUOZGcAsAAOBCch5NW8iDFu54xYiaWwAAALgNMrcAAAAuhMytY2RuAQAA4DbI3AIAALgSy/9thT2mmyBzCwAAALdB5hYAAMCFUHPrGMEtAACACyG4dYyyBAAAALgNMrcAAAAuhMytY2RuAQAA4DbI3AIAALgQMreOkbkFAACA2yBzCwAA4Ep4iINDZG4BAADgNsjcAgAAuBBqbh0jcwsAAAC3QeYWAADAhVgsKoLMbeEOV5wIbgEAAFyIRUVQluBG0S1lCQAAAHAbZG4BAABcCDeUOUbmFgAAAG6DzC0AAIAr4SEODpG5BQAAgNsgcwsAAOBKiqDm1lBzCwAAAFx/yNwCAAC4kKJYLaHw180tPgS3AAAALoTg1jHKEgAAAOA2yNwCAAC4EpYCc4jMLQAAANwGmVsAAAAXQs2tY2RuAQAA4DbI3AIAALgQMreOkbkFAACA2yBzCwAA4ELI3DpGcAsAAOBCCG4doywBAAAAboPMLQAAgCvhIQ4OkbkFAACA2yBzCwAA4EKouXWMzC0AAADcBsEtAACAC8nJ3Bb25oxx48blOj4qKsq2/9y5cxo8eLDKli0rf39/de3aVUeOHHE4pjFGY8aMUaVKleTr66s2bdpo7969Tr8/BLcAAABwWp06dZSWlmbbvv32W9u+4cOH65NPPtFHH32k9evX69ChQ+rSpYvD8aZOnaoZM2Zo9uzZ2rZtm/z8/NS+fXudO3fOqXlRcwsAAOBCrpeaWy8vLwUHB+dqT09P13vvvafFixfrrrvukiTNnTtXtWrV0tatW3XHHXfkOsYYo+nTp+vFF19UbGysJGnBggWqWLGiVq5cqR49ehR4XmRuAQAAXImliDZJGRkZdltmZma+09i7d69CQkJUrVo1xcXF6cCBA5Kk7du368KFC2rTpo2tb1RUlKpUqaItW7bkOda+fft0+PBhu2OCgoLUqFGjfI/JD8EtAAAAJEmhoaEKCgqybfHx8Xn2a9SokebNm6fVq1dr1qxZ2rdvn+68806dPHlShw8flre3t0qVKmV3TMWKFXX48OE8x8tpr1ixYoGPyQ9lCQAAAC6kKMsSUlNTFRgYaGu3Wq159u/QoYPt6+joaDVq1EhhYWH68MMP5evrW6hzcxaZWwAAAEiSAgMD7bb8gtsrlSpVSjVr1lRycrKCg4N1/vx5nThxwq7PkSNH8qzRlWRrv3JFBUfH5IfgFgAAwIVcD0uBXenUqVNKSUlRpUqV1LBhQ5UoUULr1q2z7U9KStKBAwfUuHHjPI8PDw9XcHCw3TEZGRnatm1bvsfkh+AWAAAATnnmmWe0fv167d+/X5s3b1bnzp3l6empnj17KigoSP369dNTTz2lb775Rtu3b9fDDz+sxo0b262UEBUVpRUrVki6FLAPGzZMkyZN0qpVq7Rr1y717t1bISEhuu+++5yaGzW3/6Bx48Zp5cqVSkxMLNLzVK1aVcOGDdOwYcOK9DwoHu8t26j3l29UatpxSVJUtWA926+D2jatI0ka9tK/tf6/STr8Z7r8fK26PTpc456MVc2qzv1ZB4BzRg64W88NvNuu7Zf9h9Xo/kmSpE9mD1WzhjXs9s9d/q2emrLkH5sj3INFRVBzK+fGO3jwoHr27Kljx46pfPnyatasmbZu3ary5ctLkqZNmyYPDw917dpVmZmZat++vd566y27MZKSkpSenm57PWLECJ0+fVoDBw7UiRMn1KxZM61evVo+Pj7OXYsxxjh1RCHq27ev5s+fr/j4eD333HO29pUrV6pz585yZmoFDeh27Nih0aNHa+vWrcrIyFBwcLAaNWqkmTNnqkKFCtd6KQVy6tQpZWZmqmzZskV6HmeC24yMDAUFBenIsXS7AnJcv77YsEuenh6KCC0vY4z+/dk2zVy4Tus/eE61Iipp3sffqkbVYIUGl9ZfGWc05Z3PtOuX37XjP+Pl6ckfa1xJ6dueKO4pwAkjB9yt2Nb1dd/gmba2ixezdTz9tKRLwW3ygaOKf/tT2/6z5y7o5GnnFqhH8TJZ55W5612lp//zvzdzfmeHPrpUHtaShTp2duYZpb79QLFcV2Er9t90Pj4+evnll/XXX38V+bn++OMPtW7dWmXKlNGaNWu0Z88ezZ07VyEhITp9+vQ1j3v+/PkC9fP39y/ywBbur0PzemrXtI4iqlRQ9bCKGv14J/mVtOr7/+2TJPXt0kxNG1RXlZCyujkqVC8Mule/H/lLB9KOFfPMAfd3MStbR4+dtG05gW2Os+fO2+0nsMW1uB5rbq8nxR7ctmnTRsHBwfmuo5Zj+fLlqlOnjqxWq6pWrarXXnvNtq9ly5b67bffNHz4cIf/gzZt2qT09HTNmTNHt9xyi8LDw9WqVStNmzZN4eHhkqR58+blWpdt5cqVdmOOGzdO9evX15w5cxQeHi4fHx+98847CgkJUXZ2tt2xsbGxeuSRR+yOk6Qvv/xSPj4+ue4kHDp0qO1pHpL07bff6s4775Svr69CQ0M1ZMgQu0D86NGjuvfee+Xr66vw8HAtWrTI4fsI95KVla3lX36vM2fP67Z64bn2nz6bqcWfbFVYSFndVLF0McwQuLFUCy2v3Z9P1o8rx+mdiX1U+YrP3f0xtyp57RRtXvK8xgzuJF9riWKaKVxaET7EwR0Ue3Dr6empl156STNnztTBgwfz7LN9+3Z1795dPXr00K5duzRu3DiNHj1a8+bNkyR9/PHHqly5siZMmGB7vnFegoODdfHiRa1YscKpkoe8JCcna/ny5fr444+VmJio+++/X8eOHdM333xj63P8+HGtXr1acXFxuY5v3bq1SpUqpeXLl9vasrKytHTpUlv/lJQUxcTEqGvXrtq5c6eWLl2qb7/9Vk888f//VNm3b1+lpqbqm2++0bJly/TWW2/p6NGj+c47MzMz19NH4Hp+Sv5dlZs/pYpNh+mp+KVa+MoARVWrZNs/56MNqtz8KVVu/rS+2rxbK/71hLxLUGIPFKXtP+3X4PEf6P4h/9LTU5YqLKSsPn93uPxLXlpKadma7/XomAXq9NgMTZv3pbp3uE1vT+xTzLMG3E+xB7eS1LlzZ9WvX19jx47Nc//rr7+u1q1ba/To0apZs6b69u2rJ554Qq+88ookqUyZMvL09FRAQICCg4PzXQ/tjjvu0PPPP69evXqpXLly6tChg1555ZVca6oVxPnz57VgwQLdcsstio6OVunSpdWhQwctXrzY1mfZsmUqV66cWrVqlet4T09P9ejRw67/unXrdOLECXXt2lWSFB8fr7i4OA0bNkw1atRQkyZNNGPGDC1YsEDnzp3TL7/8oi+++ELvvvuu7rjjDjVs2FDvvfeezp49m++84+Pj7Z48Ehoa6vS1o/jVCKuoDYtG6au5z+iRrs30+LiF+vnX//+Puvs73Kb1HzynT98epogq5fXwqPd1LvNCMc4YcH9fbd6t/6z7UT8lH9LXW/fo/qGzFBTgq/vaNJAkzV+xSV9v3aPdKYf00ervNWjcQt3bqr6q3lSumGcOV0NZgmPXRXArSS+//LLmz5+vPXv25Nq3Z88eNW3a1K6tadOm2rt3r7Kyspw6z+TJk3X48GHNnj1bderU0ezZsxUVFaVdu3Y5NU5YWJjtjsAccXFxWr58ue05zIsWLVKPHj3k4ZH32xwXF6eEhAQdOnTI1r9jx462sogdO3Zo3rx58vf3t23t27dXdna29u3bpz179sjLy0sNGza0jRkVFZWrrOJyo0aNUnp6um1LTU116rpxffAu4aVqoeVVv1YVjX0iVnVr3KTZSxJs+4P8fRVRpYKaNqiu+S/31979R/Rpwo7imzBwA8o4dVbJB46qWmj5PPdv/99+Scp3P4Brc90Et82bN1f79u01atSoIj9X2bJldf/99+vVV1/Vnj17FBISoldffVWS5OHhkatk4cKF3BkvPz+/XG333nuvjDH67LPPlJqaqo0bN+ZZkpDjtttuU0REhJYsWaKzZ89qxYoVdv1PnTqlRx99VImJibZtx44d2rt3ryIiIq7p2q1Wa66nj8D1ZRuj8+cv5rnPGCPjYD+AouHn663wm8rp8J/pee6vV7OyJOlIPvuB/JC5dey6KsKbMmWK6tevr8jISLv2WrVqadOmTXZtmzZtUs2aNeXp6SlJ8vb2djqLm3NcRESE7Sat8uXL6+TJkzp9+rQtgC3ourQ+Pj7q0qWLFi1apOTkZEVGRqpBgwYOj4mLi9OiRYtUuXJleXh4qGPHjrZ9DRo00O7du1W9evU8j42KitLFixe1fft23XbbbZIurRl35U1qcC/j3/yP2jSpo9Dg0jp55pyWrf5e327fq+UzH9f+g3/q47XbddcdtVS2tL8OHTmh6fO/lI9PCds6uACKxoShnbV64y6lph1XpfJBem5gR2VlZ2v5mu2qelM5dYu5VWs3/aTj6adVt8ZNmjy8izb9sFc/JR8q7qkDbuW6Cm7r1aunuLg4zZgxw6796aef1m233aaJEyfqgQce0JYtW/Tmm2/aLQZctWpVbdiwQT169JDValW5crlrmD799FMtWbJEPXr0UM2aNWWM0SeffKLPP/9cc+fOlSQ1atRIJUuW1PPPP68hQ4Zo27ZtthvXCiIuLk733HOPfvrpJz344IMF6j9u3DhNnjxZ3bp1s3uG88iRI3XHHXfoiSeeUP/+/eXn56fdu3dr7dq1evPNNxUZGamYmBg9+uijmjVrlry8vDRs2DD5+voWeL5wPX/+dUqDxi3QkT8zFOjvozrVb9LymY+rVaNaSvvjhLYkpmj2kgSdyDij8mUC1OSW6loz52mVLxNQ3FMH3NpNFUppzqSHVSaopP7865S27fhVbR9+TcdOnJKP1Ustb4/UoB6tVNLXW78f+UuffJ2oV99fU9zThguyWC5thT2mu7iugltJmjBhgpYuXWrX1qBBA3344YcaM2aMJk6cqEqVKmnChAnq27ev3XGPPvqoIiIilJmZmedqCLVr11bJkiX19NNPKzU1VVarVTVq1NCcOXP00EMPSbp0c9oHH3ygZ599Vu+++65at26tcePGaeDAgQWa/1133aUyZcooKSlJvXr1umr/6tWr6/bbb9d///tfTZ8+3W5fdHS01q9frxdeeEF33nmnjDGKiIjQAw88YOszd+5c9e/fXy1atFDFihU1adIkjR49ukBzhWuaOTr/UpdK5Uvpozce/wdnAyBHvxfm5rvv9yMndM+jb/yDswFuXMX6hDIUP55QBly/eEIZcP25Hp5QVu3JZfKw5r735+/IzjytX2d2c4snlF13mVsAAAA4UARlCTzEAQAAALgOkbkFAABwIUWxdJc7LQVG5hYAAABug8wtAACAC2EpMMfI3AIAAMBtkLkFAABwIR4eFnl4FG6q1RTyeMWJzC0AAADcBplbAAAAF0LNrWMEtwAAAC6EpcAcoywBAAAAboPMLQAAgAuhLMExMrcAAABwG2RuAQAAXAg1t46RuQUAAIDbIHMLAADgQsjcOkbmFgAAAG6DzC0AAIALYbUExwhuAQAAXIhFRVCWIPeJbilLAAAAgNsgcwsAAOBCKEtwjMwtAAAA3AaZWwAAABfCUmCOkbkFAACA2yBzCwAA4EKouXWMzC0AAADcBplbAAAAF0LNrWNkbgEAAOA2yNwCAAC4EGpuHSO4BQAAcCGUJThGWQIAAADcBsEtAACAK7H8/9KEwtr0NxO3U6ZMkcVi0bBhwyRJ+/fvt2WYr9w++uijfMfp27dvrv4xMTFOzYWyBAAAAFyz7777Tm+//baio6NtbaGhoUpLS7Pr98477+iVV15Rhw4dHI4XExOjuXPn2l5brVan5kNwCwAA4EKup5rbU6dOKS4uTu+++64mTZpka/f09FRwcLBd3xUrVqh79+7y9/d3OKbVas11rDMoSwAAAIAkKSMjw27LzMx02H/w4MHq2LGj2rRp47Df9u3blZiYqH79+l11DgkJCapQoYIiIyM1aNAgHTt2zKlrIHMLAADgQopyKbDQ0FC79rFjx2rcuHF5HrNkyRL98MMP+u677646/nvvvadatWqpSZMmDvvFxMSoS5cuCg8PV0pKip5//nl16NBBW7ZskaenZ4GuheAWAAAAkqTU1FQFBgbaXudX75qamqqhQ4dq7dq18vHxcTjm2bNntXjxYo0ePfqq5+/Ro4ft63r16ik6OloRERFKSEhQ69atC3QNlCUAAAC4kPxWIfi7myQFBgbabfkFt9u3b9fRo0fVoEEDeXl5ycvLS+vXr9eMGTPk5eWlrKwsW99ly5bpzJkz6t27t9PXWq1aNZUrV07JyckFPobMLQAAgAu5Hp5Q1rp1a+3atcuu7eGHH1ZUVJRGjhxpV0Lw3nvvqVOnTipfvrzT8zp48KCOHTumSpUqFfgYglsAAAA4JSAgQHXr1rVr8/PzU9myZe3ak5OTtWHDBn3++ed5jhMVFaX4+Hh17txZp06d0vjx49W1a1cFBwcrJSVFI0aMUPXq1dW+ffsCz43gFgAAwIVcT0uBXc3777+vypUrq127dnnuT0pKUnp6uqRLy4ft3LlT8+fP14kTJxQSEqJ27dpp4sSJTq11S3ALAACAvy0hISFX20svvaSXXnop32OMMbavfX19tWbNmr89D4JbAAAAF+JKmdviwGoJAAAAcBtkbgEAAFzI9bBawvWMzC0AAADcBplbAAAAF0LNrWMEtwAAAC6EsgTHKEsAAACA2yBzCwAA4EIoS3CMzC0AAADcBplbAAAAF2JREdTcFu5wxYrMLQAAANwGmVsAAAAX4mGxyKOQU7eFPV5xInMLAAAAt0HmFgAAwIWwzq1jBLcAAAAuhKXAHKMsAQAAAG6DzC0AAIAL8bBc2gp7THdB5hYAAABug8wtAACAK7EUQY0smVsAAADg+kPmFgAAwIWwFJhjZG4BAADgNsjcAgAAuBDL//1X2GO6C4JbAAAAF8JSYI5RlgAAAAC3QeYWAADAhfD4XcfI3AIAAMBtkLkFAABwISwF5hiZWwAAALgNMrcAAAAuxMNikUchp1oLe7ziROYWAAAAboPMLQAAgAuh5tYxglsAAAAXwlJgjlGWAAAAALdB5hYAAMCFUJbgWIGC21WrVhV4wE6dOl3zZAAAAIC/o0DB7X333VegwSwWi7Kysv7OfAAAAOAAS4E5VqDgNjs7u6jnAQAAAPxtf+uGsnPnzhXWPAAAAFAAliLa3IXTwW1WVpYmTpyom266Sf7+/vr1118lSaNHj9Z7771X6BMEAAAACsrp4Hby5MmaN2+epk6dKm9vb1t73bp1NWfOnEKdHAAAAOzlrHNb2Ju7cDq4XbBggd555x3FxcXJ09PT1n7zzTfr559/LtTJAQAAwJ6HpWg2d+F0cPv777+revXqudqzs7N14cKFQpkUAAAAXMeUKVNksVg0bNgwW1vLli1zZYcfe+wxh+MYYzRmzBhVqlRJvr6+atOmjfbu3evUXJwObmvXrq2NGzfmal+2bJluueUWZ4cDAACAE663soTvvvtOb7/9tqKjo3PtGzBggNLS0mzb1KlTHY41depUzZgxQ7Nnz9a2bdvk5+en9u3bO7WIgdNPKBszZoz69Omj33//XdnZ2fr444+VlJSkBQsW6NNPP3V2OAAAALioU6dOKS4uTu+++64mTZqUa3/JkiUVHBxcoLGMMZo+fbpefPFFxcbGSrpUDluxYkWtXLlSPXr0KNA4TmduY2Nj9cknn+irr76Sn5+fxowZoz179uiTTz5R27ZtnR0OAAAATsp5BG9hbTkyMjLstszMTIfzGDx4sDp27Kg2bdrkuX/RokUqV66c6tatq1GjRunMmTP5jrVv3z4dPnzYbqygoCA1atRIW7ZsKfB743TmVpLuvPNOrV279loOBQAAwHUqNDTU7vXYsWM1bty4PPsuWbJEP/zwg7777rs89/fq1UthYWEKCQnRzp07NXLkSCUlJenjjz/Os//hw4clSRUrVrRrr1ixom1fQVxTcCtJ33//vfbs2SPpUh1uw4YNr3UoAAAAFFBRLN2VM15qaqoCAwNt7VarNc/+qampGjp0qNauXSsfH588+wwcOND2db169VSpUiW1bt1aKSkpioiIKMTZ23M6uD148KB69uypTZs2qVSpUpKkEydOqEmTJlqyZIkqV65c2HMEAADAPyAwMNAuuM3P9u3bdfToUTVo0MDWlpWVpQ0bNujNN99UZmam3ZKxktSoUSNJUnJycp7BbU5t7pEjR1SpUiVb+5EjR1S/fv0CX4PTNbf9+/fXhQsXtGfPHh0/flzHjx/Xnj17lJ2drf79+zs7HAAAAJxwPaxz27p1a+3atUuJiYm27dZbb1VcXJwSExNzBbaSlJiYKEl2gevlwsPDFRwcrHXr1tnaMjIytG3bNjVu3LjAc3M6c7t+/Xpt3rxZkZGRtrbIyEjNnDlTd955p7PDAQAAwAlFWZZQUAEBAapbt65dm5+fn8qWLau6desqJSVFixcv1t13362yZctq586dGj58uJo3b263ZFhUVJTi4+PVuXNn2zq5kyZNUo0aNRQeHq7Ro0crJCRE9913X4Hn5nRwGxoamufDGrKyshQSEuLscAAAAHAz3t7e+uqrrzR9+nSdPn1aoaGh6tq1q1588UW7fklJSUpPT7e9HjFihE6fPq2BAwfqxIkTatasmVavXp1vXW9enA5uX3nlFT355JP617/+pVtvvVXSpZvLhg4dqldffdXZ4QAAAOAEy/9thT3m35WQkGD7OjQ0VOvXr7/qMcYY+3lYLJowYYImTJhwzfMoUHBbunRpu3T16dOn1ahRI3l5XTr84sWL8vLy0iOPPOJU2hgAAAAoTAUKbqdPn17E0wAAAEBBeFgs8ijkmtvCHq84FSi47dOnT1HPAwAAAPjbrvkhDpJ07tw5nT9/3q6tIGujAQAA4Npc+cjcwhrTXTi9zu3p06f1xBNPqEKFCvLz81Pp0qXtNgAAAKC4OB3cjhgxQl9//bVmzZolq9WqOXPmaPz48QoJCdGCBQuKYo4AAAD4Pznr3Bb25i6cLkv45JNPtGDBArVs2VIPP/yw7rzzTlWvXl1hYWFatGiR4uLiimKeAAAAwFU5nbk9fvy4qlWrJulSfe3x48clSc2aNdOGDRsKd3YAAACwk1NzW9ibu3A6uK1WrZr27dsn6dIj0z788ENJlzK6pUqVKtTJAQAAwF7OUmCFvbkLp4Pbhx9+WDt27JAkPffcc/rXv/4lHx8fDR8+XM8++2yhTxAAAAAoKKdrbocPH277uk2bNvr555+1fft2Va9eXdHR0YU6OQAAANhjKTDH/tY6t5IUFhamsLCwwpgLAAAA8LcUKLidMWNGgQccMmTINU8GAAAAjhXF0l033FJg06ZNK9BgFouF4NZF/XrklPzPOF2CDaAo+ZUq7hkAuNLFzOKeAa6iQMFtzuoIAAAAKF4euoYVAQowprtwp2sBAADADe5v31AGAACAfw41t44R3AIAALgQi0XyYCmwfFGWAAAAALdB5hYAAMCFeBRB5rawxytO15S53bhxox588EE1btxYv//+uyRp4cKF+vbbbwt1cgAAAIAznA5uly9frvbt28vX11c//vijMjMvrfeWnp6ul156qdAnCAAAgP8v54aywt7chdPB7aRJkzR79my9++67KlGihK29adOm+uGHHwp1cgAAAIAznK65TUpKUvPmzXO1BwUF6cSJE4UxJwAAAOSDmlvHnM7cBgcHKzk5OVf7t99+q2rVqhXKpAAAAIBr4XRwO2DAAA0dOlTbtm2TxWLRoUOHtGjRIj3zzDMaNGhQUcwRAAAA/8diKZrNXThdlvDcc88pOztbrVu31pkzZ9S8eXNZrVY988wzevLJJ4tijgAAAPg/HhaLPAo5Gi3s8YqT08GtxWLRCy+8oGeffVbJyck6deqUateuLX9//6KYHwAAAFBg1/wQB29vb9WuXbsw5wIAAICr8FDhP2LWnR5Z63Rw26pVK4droX399dd/a0IAAADAtXI6uK1fv77d6wsXLigxMVH/+9//1KdPn8KaFwAAAPJQFDeAuVHJrfPB7bRp0/JsHzdunE6dOvW3JwQAAABcq0IrsXjwwQf1/vvvF9ZwAAAAyIOHLLYVEwptk/ukbgstuN2yZYt8fHwKazgAAADAaU6XJXTp0sXutTFGaWlp+v777zV69OhCmxgAAAByo+bWMaeD26CgILvXHh4eioyM1IQJE9SuXbtCmxgAAABy87Bc2gp7THfhVHCblZWlhx9+WPXq1VPp0qWLak4AAADANXGq5tbT01Pt2rXTiRMnimg6AAAAcMRiUaHfUOZOZQlO31BWt25d/frrr0UxFwAAAOBvcTq4nTRpkp555hl9+umnSktLU0ZGht0GAACAopNzQ1lhb+6iwDW3EyZM0NNPP627775bktSpUye7x/AaY2SxWJSVlVX4swQAAAAKoMCZ2/Hjx+v06dP65ptvbNvXX39t23JeAwAAoOjkrJZQ2NvfMWXKFFksFg0bNkySdPz4cT355JOKjIyUr6+vqlSpoiFDhig9Pd3hOH379pXFYrHbYmJinJpLgTO3xhhJUosWLZw6AQAAANzXd999p7ffflvR0dG2tkOHDunQoUN69dVXVbt2bf3222967LHHdOjQIS1btszheDExMZo7d67ttdVqdWo+Ti0FZnGnggwAAAAXZPm//wp7zGtx6tQpxcXF6d1339WkSZNs7XXr1tXy5cttryMiIjR58mQ9+OCDunjxory88g9BrVargoODr2k+kpM3lNWsWVNlypRxuAEAAKDoFGVZwpULBWRmZjqcy+DBg9WxY0e1adPmqvNOT09XYGCgw8BWkhISElShQgVFRkZq0KBBOnbsWIHfG8nJzO348eNzPaEMAAAA7iE0NNTu9dixYzVu3Lg8+y5ZskQ//PCDvvvuu6uO++eff2rixIkaOHCgw34xMTHq0qWLwsPDlZKSoueff14dOnTQli1b5OnpWaBrcCq47dGjhypUqODMIQAAAChERfn43dTUVAUGBtra86t3TU1N1dChQ7V27Vr5+Pg4HDsjI0MdO3ZU7dq18w2Uc/To0cP2db169RQdHa2IiAglJCSodevWBbuWAvUS9bYAAADuLjAw0G7LL7jdvn27jh49qgYNGsjLy0teXl5av369ZsyYIS8vL9vSsCdPnlRMTIwCAgK0YsUKlShRwqn5VKtWTeXKlVNycnKBj3F6tQQAAAAUn5wlsgp7TGe0bt1au3btsmt7+OGHFRUVpZEjR8rT01MZGRlq3769rFarVq1addUMb14OHjyoY8eOqVKlSgU+psDBbXZ2ttMTAgAAgPsJCAhQ3bp17dr8/PxUtmxZ1a1bVxkZGWrXrp3OnDmjDz74wO5JtuXLl7fVz0ZFRSk+Pl6dO3fWqVOnNH78eHXt2lXBwcFKSUnRiBEjVL16dbVv377Ac3Oq5hYAAADFqyhrbgvLDz/8oG3btkmSqlevbrdv3759qlq1qiQpKSnJ9mAHT09P7dy5U/Pnz9eJEycUEhKidu3aaeLEiU6tdUtwCwAAgL8tISHB9nXLli0LVNJ6eR9fX1+tWbPmb8+D4BYAAMCFWCyXtsIe010Q3AIAALgQD4tFHoUcjRb2eMXJqSeUAQAAANczMrcAAAAuxBVuKCtOZG4BAADgNsjcAgAAuJIiuKFMZG4BAACA6w+ZWwAAABfiIYs8CjnVWtjjFScytwAAAHAbZG4BAABcCA9xcIzgFgAAwIWwFJhjlCUAAADAbZC5BQAAcCE8ftcxMrcAAABwG2RuAQAAXAg3lDlG5hYAAABug8wtAACAC/FQEdTc8hAHAAAA4PpD5hYAAMCFUHPrGMEtAACAC/FQ4f/p3Z3+lO9O1wIAAIAbHJlbAAAAF2KxWGQp5DqCwh6vOJG5BQAAgNsgcwsAAOBCLP+3FfaY7oLMLQAAANwGmVsAAAAX4mEpgoc4UHMLAAAAXH/I3AIAALgY98mzFj6CWwAAABfCE8ocoywBAAAAboPMLQAAgAvhIQ6OkbkFAACA2yBzCwAA4EI8VPjZSXfKdrrTtQAAAOAGR+YWAADAhVBz6xiZWwAAALgNMrcAAAAuxKLCf4iD++RtydwCAADAjZC5BQAAcCHU3DpGcAsAAOBCWArMMXe6FgAAANzgyNwCAAC4EMoSHCNzCwAAgL9lypQpslgsGjZsmK3t3LlzGjx4sMqWLSt/f3917dpVR44ccTiOMUZjxoxRpUqV5OvrqzZt2mjv3r1OzYXgFgAAwIVYimi7Vt99953efvttRUdH27UPHz5cn3zyiT766COtX79ehw4dUpcuXRyONXXqVM2YMUOzZ8/Wtm3b5Ofnp/bt2+vcuXMFng/BLQAAAK7JqVOnFBcXp3fffVelS5e2taenp+u9997T66+/rrvuuksNGzbU3LlztXnzZm3dujXPsYwxmj59ul588UXFxsYqOjpaCxYs0KFDh7Ry5coCz4ngFgAAwIVYLEWzSVJGRobdlpmZ6XAugwcPVseOHdWmTRu79u3bt+vChQt27VFRUapSpYq2bNmS51j79u3T4cOH7Y4JCgpSo0aN8j0mLwS3AAAAkCSFhoYqKCjItsXHx+fbd8mSJfrhhx/y7HP48GF5e3urVKlSdu0VK1bU4cOH8xwvp71ixYoFPiYvrJYAAADgQjxkkUchPzA3Z7zU1FQFBgba2q1Wa579U1NTNXToUK1du1Y+Pj6FOpe/i8wtAACACynKsoTAwEC7Lb/gdvv27Tp69KgaNGggLy8veXl5af369ZoxY4a8vLxUsWJFnT9/XidOnLA77siRIwoODs5zzJz2K1dUcHRMXghuAQAA4JTWrVtr165dSkxMtG233nqr4uLibF+XKFFC69atsx2TlJSkAwcOqHHjxnmOGR4eruDgYLtjMjIytG3btnyPyQtlCQAAAC7E8n//FfaYzggICFDdunXt2vz8/FS2bFlbe79+/fTUU0+pTJkyCgwM1JNPPqnGjRvrjjvusB0TFRWl+Ph4de7c2bZO7qRJk1SjRg2Fh4dr9OjRCgkJ0X333VfguRHcAgAAoNBNmzZNHh4e6tq1qzIzM9W+fXu99dZbdn2SkpKUnp5uez1ixAidPn1aAwcO1IkTJ9SsWTOtXr3aqbpeizHGFNpVwOVkZGQoKChIW3b/Lv+AwKsfAOAfc1uPKcU9BQBXMBczlbntVaWnp9vdePVPyPmd/dHWZJX0DyjUsc+cOqn776heLNdV2Ki5BQAAgNugLAEAAMCFWIpgKbDCruEtTmRuAQAA4DbI3AIAALiQy9elLcwx3QXBLQAAgAshuHWMsgQAAAC4DTK3AAAALuR6eIjD9YzMLQAAANwGmVsAAAAX4mG5tBX2mO6CzC0AAADcBplbAAAAF0LNrWNkbgEAAOA2yNwCAAC4ENa5dYzgFgAAwIVYVPhlBG4U21KWAAAAAPdB5hYAAMCFsBSYY2RuAQAA4DbI3AIAALgQlgJzjMwtAAAA3AaZ239IQkKCWrVqpb/++kulSpUqsvP07dtXJ06c0MqVK4vsHCheyz7fquVfbFXakb8kSdWqVFS/Hq3V9NZIHTpyXLH9p+Z5XPzIXmrTLPqfnCpwQxnZ9y491/cuu7ZfDvyhRr3fyNX3o5d7q02jmop7cZE+/3bPPzVFuAmWAnPshgtu//jjD40ZM0afffaZjhw5otKlS+vmm2/WmDFj1LRp0yI7b5MmTZSWlqagoKAiOwduDBXKBeqJPjEKDSknY4w+W/eDnpm8QB9MH6KqlcvriwUv2PVfsXqbPlixQU0aRhbTjIEbx559R3Tf03Ntry9mZefqM6hbExlj/slpATeUGy647dq1q86fP6/58+erWrVqOnLkiNatW6djx45d03jGGGVlZcnLy/Fb6e3treDg4Gs6B3C55rfXtnv9eO/2Wv7FVv0v6YAiwiqqXOkAu/0JW39Sm2bRKulr/SenCdyQLmZl6+jxU/nur1s9WIMfaKq7Hp2lpI+f+wdnBndiUeGvS+tGidsbq+b2xIkT2rhxo15++WW1atVKYWFhuv322zVq1Ch16tRJ+/fvl8ViUWJiot0xFotFCQkJki6VF1gsFn3xxRdq2LChrFar3n//fVksFv38889255s2bZoiIiLsjjtx4oQyMjLk6+urL774wq7/ihUrFBAQoDNnzkiSUlNT1b17d5UqVUplypRRbGys9u/fb+uflZWlp556SqVKlVLZsmU1YsQIsgE3mKysbH25YYfOnjuvelFVcu3fk3xQv/yapk5tbyuG2QE3nmo3ldXuZSP04+Kn9M4L96tyhf//1zpfawm9+2J3PTv9E4cBMHA1HrLIw1LImxuFtzdUcOvv7y9/f3+tXLlSmZmZf2us5557TlOmTNGePXvUrVs33XrrrVq0aJFdn0WLFqlXr165jg0MDNQ999yjxYsX5+p/3333qWTJkrpw4YLat2+vgIAAbdy4UZs2bZK/v79iYmJ0/vx5SdJrr72mefPm6f3339e3336r48ePa8WKFQ7nnZmZqYyMDLsNrid5/2E1v3+MmnZ5UfFvrdArLzykalUq5ur3ny+/V3hoBd1cK6wYZgncWLbvTtXgKct1/4j5enraKoVVKq3PZwyQv6+3JOmlwXfrvz8d0Bebfr7KSAD+jhsquPXy8tK8efM0f/58lSpVSk2bNtXzzz+vnTt3Oj3WhAkT1LZtW0VERKhMmTKKi4vTv//9b9v+X375Rdu3b1dcXFyex8fFxWnlypW2LG1GRoY+++wzW/+lS5cqOztbc+bMUb169VSrVi3NnTtXBw4csGWRp0+frlGjRqlLly6qVauWZs+efdWa3vj4eAUFBdm20NBQp68dxS/spnJa9MYQzX3tcXXtcIfGTftIvx44YtfnXOYFrdmQqE5tby2mWQI3lq/+u1f/Wf+Tfvr1iL7+Lln3P7dAQf4+uq9VPXVoEqU7G4Tr+Tc/L+5pwg1YimhzFzdUcCtdqrk9dOiQVq1apZiYGCUkJKhBgwaaN2+eU+Pceqt9wNCjRw/t379fW7dulXQpC9ugQQNFRUXlefzdd9+tEiVKaNWqVZKk5cuXKzAwUG3atJEk7dixQ8nJyQoICLBlnMuUKaNz584pJSVF6enpSktLU6NGjWxjenl55ZrXlUaNGqX09HTblpqa6tR14/pQooSXQkPKqVb1ynqiT4xqhFfSklWb7Pp8vWmXzmVeUMe7GhTTLIEbW8apc0o++Keq3VRGdzaopvCQMtr/6Qv6Y914/bFuvCRpwfie+mR6v2KeKeBebrgbyiTJx8dHbdu2Vdu2bTV69Gj1799fY8eO1caNGyXJrm71woULeY7h5+dn9zo4OFh33XWXFi9erDvuuEOLFy/WoEGD8p2Dt7e3unXrpsWLF6tHjx5avHixHnjgAduNaadOnVLDhg1zlTpIUvny5Z2+5hxWq1VWKzcWuRtjsnX+wkW7tv+s/U7Nb6+l0kH+xTQr4Mbm5+ut8JAyWvplolYm/E8LP/vebv/muUP0/L8+1+rNScU0Q7gs7ihz6IbL3Oaldu3aOn36tC1oTEtLs+27/Oayq4mLi9PSpUu1ZcsW/frrr+rRo8dV+69evVo//fSTvv76a7sShgYNGmjv3r2qUKGCqlevbrfllBRUqlRJ27Ztsx1z8eJFbd++vcDzhWt6c/5q/fC/X3XoyHEl7z+sN+ev1vZd+9Sh5S22PqmH/tSPP+1XbDtuJAP+KRMGxajJzVUVGlxKt9cJ1cKJvZSVbbR83U4dPX5Ke/Ydtdsk6eDRdB04/FcxzxxwLzdU5vbYsWO6//779cgjjyg6OloBAQH6/vvvNXXqVMXGxsrX11d33HGHpkyZovDwcB09elQvvvhigcfv0qWLBg0apEGDBqlVq1YKCQlx2L958+YKDg5WXFycwsPD7UoM4uLi9Morryg2NlYTJkxQ5cqV9dtvv+njjz/WiBEjVLlyZQ0dOlRTpkxRjRo1FBUVpddff10nTpy41rcHLuKv9FMaN+1D/Xn8pPz9fFS9aiXNHP+IGt1Sw9Zn1Vffq0LZQN1xWRuAonVT+UDNGd1dZQJL6s/009q26ze1ffxtHUs/U9xTg5vh8buO3VDBrb+/vxo1aqRp06YpJSVFFy5cUGhoqAYMGKDnn39ekvT++++rX79+atiwoSIjIzV16lS1a9euQOMHBATo3nvv1Ycffqj333//qv0tFot69uypqVOnasyYMXb7SpYsqQ0bNmjkyJHq0qWLTp48qZtuukmtW7dWYGCgJOnpp59WWlqa+vTpIw8PDz3yyCPq3Lmz0tPTnXxn4EpGD+l21T6De8docO+Yf2A2AHL0m/ChU/1Ltyx48gRAwVkMC6Pe0DIyMhQUFKQtu3+Xf0BgcU8HwGVu6zGluKcA4ArmYqYyt72q9PR0W7Lpn5LzO3td4oFC/5196mSGWtevUizXVdhuqMwtAACAq+N+Mse4oQwAAABug8wtAACAKyF16xCZWwAAALgNMrcAAAAuhKXAHCNzCwAAALdB5hYAAMCFWCyXtsIe012QuQUAAIDbIHMLAADgQlgswTGCWwAAAFdCdOsQZQkAAABwyqxZsxQdHa3AwEAFBgaqcePG+uKLLyRJ+/fvl8ViyXP76KOP8h2zb9++ufrHxMQ4PTcytwAAAC7kelgKrHLlypoyZYpq1KghY4zmz5+v2NhY/fjjj4qKilJaWppd/3feeUevvPKKOnTo4HDcmJgYzZ071/baarU6NS+J4BYAAABOuvfee+1eT548WbNmzdLWrVtVp04dBQcH2+1fsWKFunfvLn9/f4fjWq3WXMc6i7IEAAAAF5KzFFhhb5KUkZFht2VmZl51PllZWVqyZIlOnz6txo0b59q/fft2JSYmql+/flcdKyEhQRUqVFBkZKQGDRqkY8eOOf3+ENwCAABAkhQaGqqgoCDbFh8fn2/fXbt2yd/fX1arVY899phWrFih2rVr5+r33nvvqVatWmrSpInDc8fExGjBggVat26dXn75Za1fv14dOnRQVlaWU9dAWQIAAIALKcrFElJTUxUYGGhrd1TzGhkZqcTERKWnp2vZsmXq06eP1q9fbxfgnj17VosXL9bo0aOvOocePXrYvq5Xr56io6MVERGhhIQEtW7dusDXQuYWAAAAkmRb/SBncxTcent7q3r16mrYsKHi4+N1880364033rDrs2zZMp05c0a9e/d2ei7VqlVTuXLllJyc7NRxZG4BAABcyXW6zm12dnauGt333ntPnTp1Uvny5Z0e7+DBgzp27JgqVark1HFkbgEAAFyIpYj+c8aoUaO0YcMG7d+/X7t27dKoUaOUkJCguLg4W5/k5GRt2LBB/fv3z3OMqKgorVixQpJ06tQpPfvss9q6dav279+vdevWKTY2VtWrV1f79u2dmhuZWwAAADjl6NGj6t27t9LS0hQUFKTo6GitWbNGbdu2tfV5//33VblyZbVr1y7PMZKSkpSeni5J8vT01M6dOzV//nydOHFCISEhateunSZOnOj0WrcEtwAAAC7k8qW7CnNMZ7z33ntX7fPSSy/ppZdeyne/Mcb2ta+vr9asWePcJPJBWQIAAADcBplbAAAAF3Kd3k923SBzCwAAALdB5hYAAMCVkLp1iMwtAAAA3AaZWwAAABdyLevSFmRMd0HmFgAAAG6DzC0AAIALuR7Wub2eEdwCAAC4EO4nc4yyBAAAALgNMrcAAACuhNStQ2RuAQAA4DbI3AIAALgQlgJzjMwtAAAA3AaZWwAAABfCUmCOkbkFAACA2yBzCwAA4EJYLMExglsAAABXQnTrEGUJAAAAcBtkbgEAAFwIS4E5RuYWAAAAboPMLQAAgCspgqXA3ChxS+YWAAAA7oPMLQAAgAthsQTHyNwCAADAbZC5BQAAcCWkbh0iuAUAAHAhLAXmGGUJAAAAcBtkbgEAAFyIpQiWAiv0pcWKEZlbAAAAuA0ytwAAAC6E+8kcI3MLAAAAt0HmFgAAwJWQunWIzC0AAADcBplbAAAAF8I6t44R3AIAALgQi4pgKbDCHa5YUZYAAAAAt0HmFgAAwIVwP5ljZG4BAADgNsjcAgAAuBAev+sYmVsAAAC4DTK3AAAALoWqW0fI3AIAAMAps2bNUnR0tAIDAxUYGKjGjRvriy++sO1v2bKlLBaL3fbYY485HNMYozFjxqhSpUry9fVVmzZttHfvXqfnRnALAADgQnJqbgt7c0blypU1ZcoUbd++Xd9//73uuusuxcbG6qeffrL1GTBggNLS0mzb1KlTHY45depUzZgxQ7Nnz9a2bdvk5+en9u3b69y5c07NjbIEAAAAF3I9FCXce++9dq8nT56sWbNmaevWrapTp44kqWTJkgoODi7QeMYYTZ8+XS+++KJiY2MlSQsWLFDFihW1cuVK9ejRo8BzI3MLAAAASVJGRobdlpmZedVjsrKytGTJEp0+fVqNGze2tS9atEjlypVT3bp1NWrUKJ05cybfMfbt26fDhw+rTZs2tragoCA1atRIW7ZsceoayNwCAAC4kKJcCiw0NNSufezYsRo3blyex+zatUuNGzfWuXPn5O/vrxUrVqh27dqSpF69eiksLEwhISHauXOnRo4cqaSkJH388cd5jnX48GFJUsWKFe3aK1asaNtXUAS3AAAAkCSlpqYqMDDQ9tpqtebbNzIyUomJiUpPT9eyZcvUp08frV+/XrVr19bAgQNt/erVq6dKlSqpdevWSklJUURERJFeA2UJAAAALsRSRP9Jsq1+kLM5Cm69vb1VvXp1NWzYUPHx8br55pv1xhtv5Nm3UaNGkqTk5OQ89+fU5h45csSu/ciRIwWu281BcAsAAIC/LTs7O98a3cTERElSpUqV8twfHh6u4OBgrVu3ztaWkZGhbdu22dXxFgRlCQAAAK7kOlguYdSoUerQoYOqVKmikydPavHixUpISNCaNWuUkpKixYsX6+6771bZsmW1c+dODR8+XM2bN1d0dLRtjKioKMXHx6tz586yWCwaNmyYJk2apBo1aig8PFyjR49WSEiI7rvvPqfmRnALAAAApxw9elS9e/dWWlqagoKCFB0drTVr1qht27ZKTU3VV199penTp+v06dMKDQ1V165d9eKLL9qNkZSUpPT0dNvrESNG6PTp0xo4cKBOnDihZs2aafXq1fLx8XFqbhZjjCmUq4RLysjIUFBQkLbs/l3+AYFXPwDAP+a2HlOKewoArmAuZipz26tKT0+3u/Hqn5DzO3tv6p8KKORzn8zIUI3QcsVyXYWNzC0AAIALKcqlwNwBN5QBAADAbZC5BQAAcCGXL91VmGO6CzK3AAAAcBtkbgEAAFzJdbAU2PWMzC0AAADcBplbAAAAF0Li1jEytwAAAHAbZG4BAABcCOvcOkZwCwAA4FIKfykwdypMoCwBAAAAboPMLQAAgAuhLMExMrcAAABwGwS3AAAAcBsEtwAAAHAb1NwCAAC4EGpuHSNzCwAAALdB5hYAAMCFWIpgndvCXze3+BDcAgAAuBDKEhyjLAEAAABug8wtAACAC7Go8B+W60aJWzK3AAAAcB9kbgEAAFwJqVuHyNwCAADAbZC5BQAAcCEsBeYYmVsAAAC4DTK3AAAALoR1bh0jcwsAAAC3QeYWAADAhbBYgmMEtwAAAK6E6NYhyhIAAADgNsjcAgAAuBCWAnOMzC0AAADcBplbAAAAF8JSYI4R3N7gjDGSpNOnThbzTABcyVzMLO4pALhCzucy5/dnccjIyHCJMYsLwe0N7uTJS0Ftm9ujinkmAAC4jpMnTyooKOgfPae3t7eCg4NVIzy0SMYPDg6Wt7d3kYz9T7KY4vynB4pddna2Dh06pICAAFnc6W8SN6iMjAyFhoYqNTVVgYGBxT0dAP+Hz6b7MMbo5MmTCgkJkYfHP3/r0rlz53T+/PkiGdvb21s+Pj5FMvY/icztDc7Dw0OVK1cu7mmgkAUGBvILFLgO8dl0D/90xvZyPj4+bhGAFiVWSwAAAIDbILgFAACA2yC4BdyI1WrV2LFjZbVai3sqAC7DZxP453BDGQAAANwGmVsAAAC4DYJbAAAAuA2CWwAAALgNglsAdsaNG6f69esX+XmqVq2q6dOnF/l5gOtJQkKCLBaLTpw4UaTn6du3r+67774iPQdwvSK4Ba6ib9++slgsmjJlil37ypUrnX6qW0EDuh07dqhTp06qUKGCfHx8VLVqVT3wwAM6evSoU+e7Fs8884zWrVtX5OcBitMff/yhQYMGqUqVKrJarQoODlb79u21adOmIj1vkyZNlJaWVqwPAQDcHcEtUAA+Pj56+eWX9ddffxX5uf744w+1bt1aZcqU0Zo1a7Rnzx7NnTtXISEhOn369DWPW9DHNfr7+6ts2bLXfB7AFXTt2lU//vij5s+fr19++UWrVq1Sy5YtdezYsWsazxijixcvXrWft7e3goODedw5UIQIboECaNOmjYKDgxUfH++w3/Lly1WnTh1ZrVZVrVpVr732mm1fy5Yt9dtvv2n48OGyWCz5/nLbtGmT0tPTNWfOHN1yyy0KDw9Xq1atNG3aNIWHh0uS5s2bp1KlStkdd2UmOae8YM6cOQoPD5ePj4/eeecdhYSEKDs72+7Y2NhYPfLII3bHSdKXX34pHx+fXH9CHTp0qO666y7b62+//VZ33nmnfH19FRoaqiFDhtgF4kePHtW9994rX19fhYeHa9GiRQ7fR6AonThxQhs3btTLL7+sVq1aKSwsTLfffrtGjRqlTp06af/+/bJYLEpMTLQ7xmKxKCEhQdL/Ly/44osv1LBhQ1mtVr3//vuyWCz6+eef7c43bdo0RURE2B134sQJZWRkyNfXV1988YVd/xUrViggIEBnzpyRJKWmpqp79+4qVaqUypQpo9jYWO3fv9/WPysrS0899ZRKlSqlsmXLasSIEWKVT9zICG6BAvD09NRLL72kmTNn6uDBg3n22b59u7p3764ePXpo165dGjdunEaPHq158+ZJkj7++GNVrlxZEyZMUFpamtLS0vIcJzg4WBcvXtSKFSv+9i+o5ORkLV++XB9//LESExN1//3369ixY/rmm29sfY4fP67Vq1crLi4u1/GtW7dWqVKltHz5cltbVlaWli5dauufkpKimJgYde3aVTt37tTSpUv17bff6oknnrAd07dvX6Wmpuqbb77RsmXL9NZbb/0jJRZAXvz9/eXv76+VK1cqMzPzb4313HPPacqUKdqzZ4+6deumW2+9Ndc/3hYtWqRevXrlOjYwMFD33HOPFi9enKv/fffdp5IlS+rChQtq3769AgICtHHjRm3atEn+/v6KiYmx/TXmtdde07x58/T+++/r22+/1fHjx7VixYq/dV2ASzMAHOrTp4+JjY01xhhzxx13mEceecQYY8yKFSvM5R+hXr16mbZt29od++yzz5ratWvbXoeFhZlp06Zd9ZzPP/+88fLyMmXKlDExMTFm6tSp5vDhw7b9c+fONUFBQXbHXDmfsWPHmhIlSpijR4/a9YuNjbVdgzHGvP322yYkJMRkZWXZjrv55ptt+4cOHWruuusu2+s1a9YYq9Vq/vrrL2OMMf369TMDBw60O8fGjRuNh4eHOXv2rElKSjKSzH//+1/b/j179hhJBXovgKKwbNkyU7p0aePj42OaNGliRo0aZXbs2GGMMWbfvn1Gkvnxxx9t/f/66y8jyXzzzTfGGGO++eYbI8msXLnSbtxp06aZiIgI2+uc7/89e/bYHZfz+VmxYoXx9/c3p0+fNsYYk56ebnx8fMwXX3xhjDFm4cKFJjIy0mRnZ9vGzMzMNL6+vmbNmjXGGGMqVapkpk6datt/4cIFU7lyZdvPLeBGQ+YWcMLLL7+s+fPna8+ePbn27dmzR02bNrVra9q0qfbu3ausrCynzjN58mQdPnxYs2fPVp06dTR79mxFRUVp165dTo0TFham8uXL27XFxcVp+fLltozVokWL1KNHD3l45P3jIC4uTgkJCTp06JCtf8eOHW1lETt27NC8efNs2TB/f3+1b99e2dnZ2rdvn/bs2SMvLy81bNjQNmZUVFSusgrgn9S1a1cdOnRIq1atUkxMjBISEtSgQQPbX1oK6tZbb7V73aNHD+3fv19bt26VdOnz0qBBA0VFReV5/N13360SJUpo1apVki6VNgUGBqpNmzaSLn2+kpOTFRAQYPt8lSlTRufOnVNKSorS09OVlpamRo0a2cb08vLKNS/gRkJwCzihefPmat++vUaNGlXk5ypbtqzuv/9+vfrqq9qzZ49CQkL06quvSpI8PDxylSxcuHAh1xh+fn652u69914ZY/TZZ58pNTVVGzduzLMkIcdtt92miIgILVmyRGfPntWKFSvs+p86dUqPPvqoEhMTbduOHTu0d+9eW50hcD3y8fFR27ZtNXr0aG3evFl9+/bV2LFjbf/Qu/wzltfnS8r9GQsODtZdd91lKzVYvHixw8+Xt7e3unXrZtf/gQcekJeXl6RLn6+GDRvafb4SExP1yy+/5FnqAEDyKu4JAK5mypQpql+/viIjI+3aa9WqlWsZoU2bNqlmzZry9PSUdOkXmbNZ3JzjIiIibDdplS9fXidPntTp06dtv1wvv/nFER8fH3Xp0kWLFi1ScnKyIiMj1aBBA4fHxMXFadGiRapcubI8PDzUsWNH274GDRpo9+7dql69ep7HRkVF6eLFi9q+fbtuu+02SVJSUlKRr/MJOKt27dpauXKl7a8daWlpuuWWWyQV/PMlXfq8jBgxQj179tSvv/6qHj16XLV/27Zt9dNPP+nrr7/WpEmTbPsaNGigpUuXqkKFCgoMDMzz+EqVKmnbtm1q3ry5JNk+b1f7XAPuiswt4KR69eopLi5OM2bMsGt/+umntW7dOk2cOFG//PKL5s+frzfffFPPPPOMrU/VqlW1YcMG/f777/rzzz/zHP/TTz/Vgw8+qE8//VS//PKLkpKS9Oqrr+rzzz9XbGysJKlRo0YqWbKknn/+eaWkpGjx4sVO/Tk1Li5On332md5//32HWaXL+//www+aPHmyunXrJqvVats3cuRIbd68WU888YQSExO1d+9e/ec//7HdUBYZGamYmBg9+uij2rZtm7Zv367+/fvL19e3wPMFCtOxY8d011136YMPPtDOnTu1b98+ffTRR5o6dapiY2Pl6+urO+64w3aj2Pr16/Xiiy8WePwuXbro5MmTGjRokFq1aqWQkBCH/Zs3b67g4GDFxcUpPDzcrsQgLi5O5cqVU2xsrDZu3Kh9+/YpISFBQ4YMsd3cOnToUE2ZMkUrV67Uzz//rMcff5x/POLGVsw1v8B17/IbynLs27fPeHt7mys/QsuWLTO1a9c2JUqUMFWqVDGvvPKK3f4tW7aY6OhoY7Vacx2bIyUlxQwYMMDUrFnT+Pr6mlKlSpnbbrvNzJ07167fihUrTPXq1Y2vr6+55557zDvvvJPrhrLLbwy7XFZWlqlUqZKRZFJSUuz25Xfc7bffbiSZr7/+Ote+//73v6Zt27bG39/f+Pn5mejoaDN58mTb/rS0NNOxY0djtVpNlSpVzIIFCwp8cx1Q2M6dO2eee+4506BBAxMUFGRKlixpIiMjzYsvvmjOnDljjDFm9+7dpnHjxsbX19fUr1/ffPnll3neUJZzY9iVunfvbiSZ999/3649v+NGjBhhJJkxY8bkGistLc307t3blCtXzlitVlOtWjUzYMAAk56eboy5dAPZ0KFDTWBgoClVqpR56qmnTO/evbmhDDcsizEshgcAAAD3QFkCAAAA3AbBLQAAANwGwS0AAADcBsEtAAAA3AbBLQAAANwGwS0AAADcBsEtAAAA3AbBLQAAANwGwS0AXKO+ffvqvvvus71u2bKlhg0b9o/PIyEhQRaLxeEjVy0Wi1auXFngMceNG6f69ev/rXnt379fFotFiYmJf2scAHAGwS0At9K3b19ZLBZZLBZ5e3urevXqmjBhgi5evFjk5/744481ceLEAvUtSEAKAHCeV3FPAAAKW0xMjObOnavMzEx9/vnnGjx4sEqUKKFRo0bl6nv+/Hl5e3sXynnLlClTKOMAAK4dmVsAbsdqtSo4OFhhYWEaNGiQ2rRpo1WrVkn6/6UEkydPVkhIiCIjIyVJqamp6t69u0qVKqUyZcooNjZW+/fvt42ZlZWlp556SqVKlVLZsmU1YsQIGWPszntlWUJmZqZGjhyp0NBQWa1WVa9eXe+9957279+vVq1aSZJKly4ti8Wivn37SpKys7MVHx+v8PBw+fr66uabb9ayZcvszvP555+rZs2a8vX1VatWrezmWVAjR45UzZo1VbJkSVWrVk2jR4/WhQsXcvV7++23FRoaqpIlS6p79+5KT0+32z9nzhzVqlVLPj4+ioqK0ltvveX0XACgMBHcAnB7vr6+On/+vO31unXrlJSUpLVr1+rTTz/VhQsX1L59ewUEBGjjxo3atGmT/P39FRMTYzvutdde07x58/T+++/r22+/1fHjx7VixQqH5+3du7f+/e9/a8aMGdqzZ4/efvtt+fv7KzQ0VMuXL5ckJSUlKS0tTW+88YYkKT4+XgsWLNDs2bP1008/afjw4XrwwQe1fv16SZeC8C5duujee+9VYmKi+vfvr+eee87p9yQgIEDz5s3T7t279cYbb+jdd9/VtGnT7PokJyfrww8/1CeffKLVq1frxx9/1OOPP27bv2jRIo0ZM0aTJ0/Wnj179NJLL2n06NGaP3++0/MBgEJjAMCN9OnTx8TGxhpjjMnOzjZr1641VqvVPPPMM7b9FStWNJmZmbZjFi5caCIjI012dratLTMz0/j6+po1a9YYY4ypVKmSmTp1qm3/hQsXTOXKlW3nMsaYFi1amKFDhxpjjElKSjKSzNq1a/Oc5zfffGMkmb/++svWdu7cOVOyZEmzefNmu779+vUzPXv2NMYYM2rUKFO7dm27/SNHjsw11pUkmRUrVuS7/5VXXjENGza0vR47dqzx9PQ0Bw8etLV98cUXxsPDw6SlpRljjImIiDCLFy+2G2fixImmcePGxhhj9u3bZySZH3/8Md/zAkBho+YWgNv59NNP5e/vrwsXLig7O1u9evXSuHHjbPvr1atnV2e7Y8cOJScnKyAgwG6cc+fOKSUlRenp6UpLS1OjRo1s+7y8vHTrrbfmKk3IkZiYKE9PT7Vo0aLA805OTtaZM2fUtm1bu/bz58/rlltukSTt2bPHbh6S1Lhx4wKfI8fSpUs1Y8YMpaSk6NSpU7p48aICAwPt+lSpUkU33XST3Xmys7OVlJSkgIAApaSkqF+/fhowYICtz8WLFxUUFOT0fACgsBDcAnA7rVq10qxZs+Tt7a2QkBB5edn/qPPz87N7ferUKTVs2FCLFi3KNVb58uWvaQ6+vr5OH3Pq1ClJ0meffWYXVEqX6ogLy5YtWxQXF6fx48erffv2CgoK0pIlS/Taa685Pdd33303V7Dt6elZaHMFAGcR3AJwO35+fqpevXqB+zdo0EBLly5VhQoVcmUvc1SqVEnbtm1T8+bNJV3KUG7fvl0NGjTIs3+9evWUnZ2t9evXq02bNrn252SOs7KybG21a9eW1WrVgQMH8s341qpVy3ZzXI6tW7de/SIvs3nzZoWFhemFF16wtf3222+5+h04cECHDh1SSEiI7TweHh6KjIxUxYoVFRISol9//VVxcXFOnR8AihI3lAG44cXFxalcuXKKjY3Vxo0btW/fPiUkJGjIkCE6ePCgJGno0KGaMmWKVq5cqZ9//lmPP/64wzVqq1atqj59+uiRRx7RypUrbWN++OGHkqSwsDBZLBZ9+umn+uOPP3Tq1CkFBATomWee0fDhwzV//nylpKTohx9+0MyZM203aT322GPau3evnn32WSUlJWnx4sWaN2+eU9dbo0YNHThwQEuWLFFKSopmzJiR581xPj4+6tOnj3bs2KGNGzdqyJAh6t69u4KDgyVJ48ePV3x8vGbMmKFffvlFu3bt0ty5c/X66687NR8AKEwEtwBueCVLltSGDRtUpUoVdenSRbVq1VK/fv107tw5Wyb36aef1kMPPaQ+ffqocePGCggIUOfOnR2OO2vWLHXr1k2PP/64oqKiNGDAAJ0+fVqSdNNNN2n8+PF67rnnVLFiRT3xxBOSpIkTJ2r06NGKj49XrVq1FBMTo88++0zh4eGSLtXBLl++XCtXrtTNN9+s2bNn66WXXnLqejt16qThw4friSeeUP369bV582aNHj06V7/q1aurS5cuuvvuu9WuXTtFR0fbLfXVv39/zZkzR3PnzlW9evXUokULzZs3zzZXACgOFpPf3RAAAACAiyFzCwAAALdBcAsAAAC3QXALAAAAt0FwCwAAALdBcAsAAAC3QXALAAAAt0FwCwAAALdBcAsAAAC3QXALAAAAt0FwCwAAALdBcAsAAAC38f8AI4E6vb75E+AAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Performance evaluation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "model. Evaluate its accuracy and compare results with and without scaling."
      ],
      "metadata": {
        "id": "4iIh7r32wuvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# --- 1. Create a dummy dataset with varying scales for demonstration ---\n",
        "# This dataset is designed to clearly show the impact of feature scaling.\n",
        "np.random.seed(42) # for reproducibility\n",
        "N_SAMPLES = 500\n",
        "\n",
        "# Numerical features with different scales\n",
        "feature_small_scale = np.random.rand(N_SAMPLES) * 10 # Values from 0-10\n",
        "feature_medium_scale = np.random.normal(50, 10, N_SAMPLES) # Values around 50 +/- 10\n",
        "feature_large_scale = np.random.randint(1000, 5000, N_SAMPLES) # Values from 1000-5000\n",
        "\n",
        "# Categorical features (will be one-hot encoded)\n",
        "gender = np.random.choice(['Male', 'Female'], N_SAMPLES, p=[0.5, 0.5])\n",
        "city = np.random.choice(['New York', 'London', 'Paris'], N_SAMPLES, p=[0.4, 0.3, 0.3])\n",
        "\n",
        "# Binary Target variable: A relationship that might benefit from scaling\n",
        "linear_combination = (0.2 * feature_small_scale + 0.05 * feature_medium_scale +\n",
        "                      0.001 * feature_large_scale +\n",
        "                      (1 if gender[0] == 'Male' else 0) * 0.5 +\n",
        "                      (1 if city[0] == 'New York' else 0) * 0.8 +\n",
        "                      np.random.randn(N_SAMPLES) * 0.5) # Add some noise\n",
        "\n",
        "binary_target = (linear_combination > np.median(linear_combination)).astype(int)\n",
        "\n",
        "# Introduce some missing values for demonstration of preprocessing\n",
        "missing_indices_small = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.02), replace=False)\n",
        "missing_indices_medium = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.03), replace=False)\n",
        "feature_small_scale[missing_indices_small] = np.nan\n",
        "feature_medium_scale[missing_indices_medium] = np.nan\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Feature_Small': feature_small_scale,\n",
        "    'Feature_Medium': feature_medium_scale,\n",
        "    'Feature_Large': feature_large_scale,\n",
        "    'Gender': gender,\n",
        "    'City': city,\n",
        "    'Target': binary_target # Our binary target\n",
        "})\n",
        "\n",
        "print(\"--- Dummy Dataset Head (with varying scales) ---\")\n",
        "print(data.head())\n",
        "print(\"\\n--- Dummy Dataset Info ---\")\n",
        "data.info()\n",
        "print(\"\\n--- Target Variable Distribution ---\")\n",
        "print(data['Target'].value_counts())\n",
        "\n",
        "# --- 2. Separate features (X) and target (y) ---\n",
        "X = data.drop('Target', axis=1)\n",
        "y = data['Target']\n",
        "\n",
        "# --- 3. Identify numerical and categorical columns for preprocessing ---\n",
        "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# --- 4. Define Preprocessing Pipelines (With and Without Scaling) ---\n",
        "\n",
        "# Preprocessor for models *WITHOUT* explicit numerical scaling\n",
        "# It still handles imputation for numerical features and encoding for categorical.\n",
        "numerical_transformer_no_scale = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')) # Only impute, no scaling\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor_no_scale = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer_no_scale, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Preprocessor for models *WITH* numerical scaling (Standardization)\n",
        "numerical_transformer_with_scale = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler()) # Impute then standardize\n",
        "])\n",
        "\n",
        "preprocessor_with_scale = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer_with_scale, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# --- 5. Split the dataset into training and testing sets ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "print(f\"Training target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Testing target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
        "\n",
        "\n",
        "# --- Practical Question 15 & 23: Apply Feature Scaling (Standardization) and Compare Results ---\n",
        "print(\"\\n--- Question 15 & 23: Feature Scaling (Standardization) Comparison ---\")\n",
        "\n",
        "# --- Model 1: Logistic Regression WITHOUT Feature Scaling ---\n",
        "print(\"\\n--- Training Model WITHOUT Feature Scaling ---\")\n",
        "model_no_scaling = Pipeline(steps=[('preprocessor', preprocessor_no_scale),\n",
        "                                   ('classifier', LogisticRegression(random_state=42, max_iter=1000))])\n",
        "\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "print(f\"Accuracy WITHOUT Feature Scaling: {accuracy_no_scaling:.4f}\")\n",
        "print(\"\\nClassification Report (WITHOUT Scaling):\")\n",
        "print(classification_report(y_test, y_pred_no_scaling))\n",
        "\n",
        "\n",
        "# --- Model 2: Logistic Regression WITH Feature Scaling (Standardization) ---\n",
        "print(\"\\n--- Training Model WITH Feature Scaling (Standardization) ---\")\n",
        "model_with_scaling = Pipeline(steps=[('preprocessor', preprocessor_with_scale),\n",
        "                                     ('classifier', LogisticRegression(random_state=42, max_iter=1000))])\n",
        "\n",
        "model_with_scaling.fit(X_train, y_train)\n",
        "y_pred_with_scaling = model_with_scaling.predict(X_test)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "\n",
        "print(f\"Accuracy WITH Feature Scaling (Standardization): {accuracy_with_scaling:.4f}\")\n",
        "print(\"\\nClassification Report (WITH Scaling):\")\n",
        "print(classification_report(y_test, y_pred_with_scaling))\n",
        "\n",
        "\n",
        "# --- Comparison of Results ---\n",
        "print(\"\\n--- Comparison of Model Accuracies ---\")\n",
        "print(f\"Accuracy WITHOUT scaling: {accuracy_no_scaling:.4f}\")\n",
        "print(f\"Accuracy WITH scaling:    {accuracy_with_scaling:.4f}\")\n",
        "\n",
        "if accuracy_with_scaling > accuracy_no_scaling:\n",
        "    print(\"\\nObservation: Feature scaling (Standardization) IMPROVED the model's accuracy.\")\n",
        "    print(\"This often happens when features have vastly different scales, as Logistic Regression's\")\n",
        "    print(\"optimization algorithm (like gradient descent) converges more efficiently on scaled data.\")\n",
        "elif accuracy_with_scaling < accuracy_no_scaling:\n",
        "    print(\"\\nObservation: Feature scaling (Standardization) slightly DECREASED the model's accuracy.\")\n",
        "    print(\"This can happen if the original scales were already optimal or if the dataset is very simple,\")\n",
        "    print(\"or if the scaling introduced some noise for this particular random split/dataset.\")\n",
        "else:\n",
        "    print(\"\\nObservation: Feature scaling (Standardization) had NO SIGNIFICANT IMPACT on accuracy.\")\n",
        "    print(\"This might occur if the features were already on a similar scale or if the model is robust to scale differences.\")\n",
        "\n",
        "print(\"\\nFeature scaling is generally recommended for Logistic Regression, especially when using solvers that are sensitive to feature scales (e.g., 'lbfgs', 'saga').\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sh2DknmXwwhB",
        "outputId": "1a771e61-57b5-484c-8e20-1605a0289d1a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dummy Dataset Head (with varying scales) ---\n",
            "   Feature_Small  Feature_Medium  Feature_Large  Gender      City  Target\n",
            "0       3.745401       53.417560           2772    Male    London       0\n",
            "1       9.507143       68.761708           2312    Male  New York       1\n",
            "2       7.319939       59.504238           2285    Male     Paris       0\n",
            "3       5.986585       44.230963           1215  Female  New York       0\n",
            "4       1.560186       41.015853           4218  Female     Paris       1\n",
            "\n",
            "--- Dummy Dataset Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 6 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   Feature_Small   490 non-null    float64\n",
            " 1   Feature_Medium  485 non-null    float64\n",
            " 2   Feature_Large   500 non-null    int64  \n",
            " 3   Gender          500 non-null    object \n",
            " 4   City            500 non-null    object \n",
            " 5   Target          500 non-null    int64  \n",
            "dtypes: float64(2), int64(2), object(2)\n",
            "memory usage: 23.6+ KB\n",
            "\n",
            "--- Target Variable Distribution ---\n",
            "Target\n",
            "0    250\n",
            "1    250\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training set size: 400 samples\n",
            "Testing set size: 100 samples\n",
            "Training target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "Testing target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "--- Question 15 & 23: Feature Scaling (Standardization) Comparison ---\n",
            "\n",
            "--- Training Model WITHOUT Feature Scaling ---\n",
            "Accuracy WITHOUT Feature Scaling: 0.8900\n",
            "\n",
            "Classification Report (WITHOUT Scaling):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.88      0.89        50\n",
            "           1       0.88      0.90      0.89        50\n",
            "\n",
            "    accuracy                           0.89       100\n",
            "   macro avg       0.89      0.89      0.89       100\n",
            "weighted avg       0.89      0.89      0.89       100\n",
            "\n",
            "\n",
            "--- Training Model WITH Feature Scaling (Standardization) ---\n",
            "Accuracy WITH Feature Scaling (Standardization): 0.8900\n",
            "\n",
            "Classification Report (WITH Scaling):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.88      0.89        50\n",
            "           1       0.88      0.90      0.89        50\n",
            "\n",
            "    accuracy                           0.89       100\n",
            "   macro avg       0.89      0.89      0.89       100\n",
            "weighted avg       0.89      0.89      0.89       100\n",
            "\n",
            "\n",
            "--- Comparison of Model Accuracies ---\n",
            "Accuracy WITHOUT scaling: 0.8900\n",
            "Accuracy WITH scaling:    0.8900\n",
            "\n",
            "Observation: Feature scaling (Standardization) had NO SIGNIFICANT IMPACT on accuracy.\n",
            "This might occur if the features were already on a similar scale or if the model is robust to scale differences.\n",
            "\n",
            "Feature scaling is generally recommended for Logistic Regression, especially when using solvers that are sensitive to feature scales (e.g., 'lbfgs', 'saga').\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. M Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score."
      ],
      "metadata": {
        "id": "--0lTumSw9wJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns # For potential future visualizations, though not strictly needed for ROC-AUC plot\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, RocCurveDisplay\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# --- 1. Create a dummy dataset for binary classification ---\n",
        "# This dataset will be used to demonstrate ROC-AUC evaluation.\n",
        "np.random.seed(42) # for reproducibility\n",
        "N_SAMPLES = 500\n",
        "\n",
        "# Numerical features\n",
        "feature_num1 = np.random.rand(N_SAMPLES) * 100\n",
        "feature_num2 = np.random.randn(N_SAMPLES) * 15 + 50\n",
        "feature_num3 = np.random.randint(0, 10, N_SAMPLES)\n",
        "\n",
        "# Categorical features\n",
        "gender = np.random.choice(['Male', 'Female'], N_SAMPLES, p=[0.5, 0.5])\n",
        "city = np.random.choice(['New York', 'London', 'Paris'], N_SAMPLES, p=[0.4, 0.3, 0.3])\n",
        "\n",
        "# Binary Target variable: Create a target with some separation but also some overlap\n",
        "# to make the ROC curve interesting (not perfectly separable).\n",
        "linear_combination = (0.5 * feature_num1 + 0.8 * feature_num2 - 0.3 * feature_num3 +\n",
        "                      (1 if gender[0] == 'Male' else 0) * 10 +\n",
        "                      (5 if city[0] == 'New York' else 0) +\n",
        "                      np.random.randn(N_SAMPLES) * 15) # Increased noise for less perfect separation\n",
        "\n",
        "binary_target = (linear_combination > np.median(linear_combination)).astype(int)\n",
        "\n",
        "# Introduce some missing values for demonstration of preprocessing\n",
        "missing_indices_num1 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.05), replace=False)\n",
        "missing_indices_num2 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.03), replace=False)\n",
        "feature_num1[missing_indices_num1] = np.nan\n",
        "feature_num2[missing_indices_num2] = np.nan\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Feature_Num1': feature_num1,\n",
        "    'Feature_Num2': feature_num2,\n",
        "    'Feature_Num3': feature_num3,\n",
        "    'Gender': gender,\n",
        "    'City': city,\n",
        "    'Target': binary_target # Our binary target\n",
        "})\n",
        "\n",
        "print(\"--- Dummy Dataset Head ---\")\n",
        "print(data.head())\n",
        "print(\"\\n--- Dummy Dataset Info ---\")\n",
        "data.info()\n",
        "print(\"\\n--- Target Variable Distribution ---\")\n",
        "print(data['Target'].value_counts())\n",
        "\n",
        "# --- 2. Separate features (X) and target (y) ---\n",
        "X = data.drop('Target', axis=1)\n",
        "y = data['Target'] # Use the binary target\n",
        "\n",
        "# --- 3. Identify numerical and categorical columns for preprocessing ---\n",
        "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# --- 4. Create a preprocessing pipeline ---\n",
        "# This pipeline will handle missing values, scaling (Standardization), and one-hot encoding.\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')), # Impute missing numerical values with mean\n",
        "    ('scaler', StandardScaler())                  # Scale numerical features\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), # Impute missing categorical values with mode\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encode categorical features\n",
        "])\n",
        "\n",
        "# Create a preprocessor using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# --- 5. Split the dataset into training and testing sets ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "print(f\"Training target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Testing target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
        "\n",
        "# --- Practical Question 16: Train Logistic Regression and evaluate its performance using ROC-AUC score ---\n",
        "print(\"\\n--- Question 16: Evaluate Performance using ROC-AUC Score ---\")\n",
        "\n",
        "# Create a pipeline for the Logistic Regression model\n",
        "model_q16 = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                            ('classifier', LogisticRegression(random_state=42, max_iter=500))])\n",
        "\n",
        "# Train the model\n",
        "print(\"Training Logistic Regression model...\")\n",
        "model_q16.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# --- Predict probabilities for the positive class ---\n",
        "# ROC-AUC requires probability estimates for the positive class (class 1).\n",
        "# model.predict_proba() returns probabilities for all classes,\n",
        "# y_proba[:, 1] extracts probabilities for the positive class.\n",
        "y_proba_q16 = model_q16.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# --- Calculate ROC-AUC Score ---\n",
        "# The ROC-AUC score (Receiver Operating Characteristic - Area Under the Curve)\n",
        "# measures the ability of a classifier to distinguish between classes.\n",
        "# An AUC of 1.0 represents a perfect classifier, and 0.5 represents a random classifier.\n",
        "roc_auc = roc_auc_score(y_test, y_proba_q16)\n",
        "print(f\"\\nROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# --- Visualize the ROC Curve ---\n",
        "# The ROC curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR)\n",
        "# at various threshold settings.\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Using RocCurveDisplay for a convenient way to plot ROC curve\n",
        "# It directly takes the estimator, X_test, and y_test.\n",
        "disp = RocCurveDisplay.from_estimator(\n",
        "    model_q16, X_test, y_test, name='Logistic Regression', ax=plt.gca()\n",
        ")\n",
        "\n",
        "# Plot the random classifier line (AUC = 0.5)\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.5)')\n",
        "\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.grid(True)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nROC-AUC score calculation and visualization complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NtiA7blWw-9b",
        "outputId": "480ebcab-eeb1-4107-f5ef-2d66c78c8a6c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dummy Dataset Head ---\n",
            "   Feature_Num1  Feature_Num2  Feature_Num3  Gender      City  Target\n",
            "0     37.454012     55.126340             0  Female  New York       0\n",
            "1     95.071431     78.142563             5  Female     Paris       1\n",
            "2     73.199394     64.256358             7    Male  New York       1\n",
            "3     59.865848     41.346445             2    Male  New York       0\n",
            "4     15.601864     36.523780             7  Female     Paris       0\n",
            "\n",
            "--- Dummy Dataset Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 6 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Feature_Num1  475 non-null    float64\n",
            " 1   Feature_Num2  485 non-null    float64\n",
            " 2   Feature_Num3  500 non-null    int64  \n",
            " 3   Gender        500 non-null    object \n",
            " 4   City          500 non-null    object \n",
            " 5   Target        500 non-null    int64  \n",
            "dtypes: float64(2), int64(2), object(2)\n",
            "memory usage: 23.6+ KB\n",
            "\n",
            "--- Target Variable Distribution ---\n",
            "Target\n",
            "0    250\n",
            "1    250\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training set size: 400 samples\n",
            "Testing set size: 100 samples\n",
            "Training target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "Testing target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "--- Question 16: Evaluate Performance using ROC-AUC Score ---\n",
            "Training Logistic Regression model...\n",
            "Model training complete.\n",
            "\n",
            "ROC-AUC Score: 0.8488\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ROC-AUC score calculation and visualization complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "accuracy."
      ],
      "metadata": {
        "id": "BpOueu6FxJdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# --- 1. Create a dummy dataset for binary classification ---\n",
        "np.random.seed(42) # for reproducibility\n",
        "N_SAMPLES = 500\n",
        "\n",
        "# Numerical features\n",
        "feature_num1 = np.random.rand(N_SAMPLES) * 100\n",
        "feature_num2 = np.random.randn(N_SAMPLES) * 15 + 50\n",
        "feature_num3 = np.random.randint(0, 10, N_SAMPLES)\n",
        "\n",
        "# Categorical features\n",
        "gender = np.random.choice(['Male', 'Female'], N_SAMPLES, p=[0.5, 0.5])\n",
        "city = np.random.choice(['New York', 'London', 'Paris'], N_SAMPLES, p=[0.4, 0.3, 0.3])\n",
        "\n",
        "# Binary Target variable:\n",
        "linear_combination = (0.5 * feature_num1 + 0.8 * feature_num2 - 0.3 * feature_num3 +\n",
        "                      (1 if gender[0] == 'Male' else 0) * 10 +\n",
        "                      (5 if city[0] == 'New York' else 0) +\n",
        "                      np.random.randn(N_SAMPLES) * 20)\n",
        "\n",
        "binary_target = (linear_combination > np.percentile(linear_combination, 50)).astype(int)\n",
        "\n",
        "# Introduce some missing values for demonstration of preprocessing\n",
        "missing_indices_num1 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.05), replace=False)\n",
        "missing_indices_num2 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.03), replace=False)\n",
        "feature_num1[missing_indices_num1] = np.nan\n",
        "feature_num2[missing_indices_num2] = np.nan\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Feature_Num1': feature_num1,\n",
        "    'Feature_Num2': feature_num2,\n",
        "    'Feature_Num3': feature_num3,\n",
        "    'Gender': gender,\n",
        "    'City': city,\n",
        "    'Target': binary_target # Our binary target\n",
        "})\n",
        "\n",
        "print(\"--- Dummy Dataset Head ---\")\n",
        "print(data.head())\n",
        "print(\"\\n--- Dummy Dataset Info ---\")\n",
        "data.info()\n",
        "print(\"\\n--- Target Variable Distribution ---\")\n",
        "print(data['Target'].value_counts())\n",
        "\n",
        "# --- 2. Separate features (X) and target (y) ---\n",
        "X = data.drop('Target', axis=1)\n",
        "y = data['Target'] # Use the binary target\n",
        "\n",
        "# --- 3. Identify numerical and categorical columns for preprocessing ---\n",
        "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# --- 4. Create a preprocessing pipeline ---\n",
        "# This pipeline will handle missing values, scaling, and one-hot encoding.\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')), # Impute missing numerical values with mean\n",
        "    ('scaler', StandardScaler())                  # Scale numerical features\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), # Impute missing categorical values with mode\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encode categorical features\n",
        "])\n",
        "\n",
        "# Create a preprocessor using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# --- 5. Split the dataset into training and testing sets ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "print(f\"Training target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Testing target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
        "\n",
        "# --- Practical Question 17: Train Logistic Regression using a custom C (regularization strength) ---\n",
        "print(\"\\n--- Question 17: Train Logistic Regression with Custom C (C=0.5) ---\")\n",
        "\n",
        "# Create a pipeline for the Logistic Regression model.\n",
        "# The 'C' parameter is the inverse of the regularization strength.\n",
        "# Smaller values of C specify stronger regularization.\n",
        "# C=0.5 means a moderate amount of regularization.\n",
        "model_custom_c = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                 ('classifier', LogisticRegression(C=0.5, random_state=42, max_iter=500))])\n",
        "\n",
        "# Train the model\n",
        "print(f\"Training Logistic Regression model with C={model_custom_c.named_steps['classifier'].C}...\")\n",
        "model_custom_c.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_custom_c = model_custom_c.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy_custom_c = accuracy_score(y_test, y_pred_custom_c)\n",
        "print(f\"Model Accuracy with C=0.5: {accuracy_custom_c:.4f}\")\n",
        "\n",
        "# Optional: Compare with default C (usually 1.0)\n",
        "print(\"\\n--- Comparison with Default C (C=1.0) ---\")\n",
        "model_default_c = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                  ('classifier', LogisticRegression(random_state=42, max_iter=500))])\n",
        "model_default_c.fit(X_train, y_train)\n",
        "y_pred_default_c = model_default_c.predict(X_test)\n",
        "accuracy_default_c = accuracy_score(y_test, y_pred_default_c)\n",
        "print(f\"Model Accuracy with Default C (C=1.0): {accuracy_default_c:.4f}\")\n",
        "\n",
        "if accuracy_custom_c > accuracy_default_c:\n",
        "    print(f\"\\nObservation: Custom C=0.5 resulted in slightly higher accuracy ({accuracy_custom_c:.4f} vs {accuracy_default_c:.4f}).\")\n",
        "elif accuracy_custom_c < accuracy_default_c:\n",
        "    print(f\"\\nObservation: Custom C=0.5 resulted in slightly lower accuracy ({accuracy_custom_c:.4f} vs {accuracy_default_c:.4f}).\")\n",
        "else:\n",
        "    print(\"\\nObservation: Custom C=0.5 had similar accuracy to the default C.\")\n",
        "\n",
        "print(\"\\nChoosing the optimal C value is typically done through hyperparameter tuning (e.g., GridSearchCV or RandomizedSearchCV).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ly9kwcVRxKus",
        "outputId": "cb06e2e9-4561-46b5-87fa-74325e24a992"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dummy Dataset Head ---\n",
            "   Feature_Num1  Feature_Num2  Feature_Num3  Gender      City  Target\n",
            "0     37.454012     55.126340             0  Female  New York       0\n",
            "1     95.071431     78.142563             5  Female     Paris       1\n",
            "2     73.199394     64.256358             7    Male  New York       0\n",
            "3     59.865848     41.346445             2    Male  New York       0\n",
            "4     15.601864     36.523780             7  Female     Paris       0\n",
            "\n",
            "--- Dummy Dataset Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 6 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Feature_Num1  475 non-null    float64\n",
            " 1   Feature_Num2  485 non-null    float64\n",
            " 2   Feature_Num3  500 non-null    int64  \n",
            " 3   Gender        500 non-null    object \n",
            " 4   City          500 non-null    object \n",
            " 5   Target        500 non-null    int64  \n",
            "dtypes: float64(2), int64(2), object(2)\n",
            "memory usage: 23.6+ KB\n",
            "\n",
            "--- Target Variable Distribution ---\n",
            "Target\n",
            "0    250\n",
            "1    250\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training set size: 400 samples\n",
            "Testing set size: 100 samples\n",
            "Training target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "Testing target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "--- Question 17: Train Logistic Regression with Custom C (C=0.5) ---\n",
            "Training Logistic Regression model with C=0.5...\n",
            "Model training complete.\n",
            "Model Accuracy with C=0.5: 0.7500\n",
            "\n",
            "--- Comparison with Default C (C=1.0) ---\n",
            "Model Accuracy with Default C (C=1.0): 0.7500\n",
            "\n",
            "Observation: Custom C=0.5 had similar accuracy to the default C.\n",
            "\n",
            "Choosing the optimal C value is typically done through hyperparameter tuning (e.g., GridSearchCV or RandomizedSearchCV).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Write a Python program to train Logistic Regression and identify important features based on model\n",
        "coefficients."
      ],
      "metadata": {
        "id": "ApW-I-rcxgpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# --- 1. Create a dummy dataset for binary classification ---\n",
        "# This dataset is designed to have some features that are more \"important\"\n",
        "# (i.e., have a stronger correlation with the target) to demonstrate coefficient interpretation.\n",
        "np.random.seed(42) # for reproducibility\n",
        "N_SAMPLES = 500\n",
        "\n",
        "# Numerical features\n",
        "# Feature_Strong_Num: strong positive impact\n",
        "feature_strong_num = np.random.rand(N_SAMPLES) * 50 + 10 # Range 10-60\n",
        "# Feature_Weak_Num: weak negative impact\n",
        "feature_weak_num = np.random.randn(N_SAMPLES) * 5 + 20 # Around 20 +/- 5\n",
        "# Feature_Random_Num: very little impact\n",
        "feature_random_num = np.random.rand(N_SAMPLES) * 100\n",
        "\n",
        "# Categorical features\n",
        "# Gender_Strong: strong impact (e.g., 'Female' having higher odds)\n",
        "gender = np.random.choice(['Male', 'Female'], N_SAMPLES, p=[0.5, 0.5])\n",
        "# City_Moderate: moderate impact\n",
        "city = np.random.choice(['New York', 'London', 'Paris'], N_SAMPLES, p=[0.4, 0.3, 0.3])\n",
        "\n",
        "# Binary Target variable: Define target based on a weighted sum of features\n",
        "# Coefficients chosen to simulate importance:\n",
        "# Feature_Strong_Num (positive), Gender (Female positive), City (New York positive)\n",
        "target_linear_combination = (\n",
        "    0.8 * feature_strong_num +          # Strong positive coefficient\n",
        "    -0.1 * feature_weak_num +           # Weak negative coefficient\n",
        "    0.01 * feature_random_num +         # Very weak coefficient\n",
        "    (1 if gender[0] == 'Female' else 0) * 50 + # Strong positive for Female\n",
        "    (1 if city[0] == 'New York' else 0) * 20 + # Moderate positive for New York\n",
        "    np.random.randn(N_SAMPLES) * 10     # Add some noise\n",
        ")\n",
        "\n",
        "binary_target = (target_linear_combination > np.median(target_linear_combination)).astype(int)\n",
        "\n",
        "# Introduce some missing values for demonstration of preprocessing\n",
        "missing_indices_strong = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.03), replace=False)\n",
        "missing_indices_weak = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.02), replace=False)\n",
        "feature_strong_num[missing_indices_strong] = np.nan\n",
        "feature_weak_num[missing_indices_weak] = np.nan\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Feature_Strong_Num': feature_strong_num,\n",
        "    'Feature_Weak_Num': feature_weak_num,\n",
        "    'Feature_Random_Num': feature_random_num,\n",
        "    'Gender': gender,\n",
        "    'City': city,\n",
        "    'Target': binary_target # Our binary target\n",
        "})\n",
        "\n",
        "print(\"--- Dummy Dataset Head ---\")\n",
        "print(data.head())\n",
        "print(\"\\n--- Dummy Dataset Info ---\")\n",
        "data.info()\n",
        "print(\"\\n--- Target Variable Distribution ---\")\n",
        "print(data['Target'].value_counts())\n",
        "\n",
        "# --- 2. Separate features (X) and target (y) ---\n",
        "X = data.drop('Target', axis=1)\n",
        "y = data['Target'] # Use the binary target\n",
        "\n",
        "# --- 3. Identify numerical and categorical columns for preprocessing ---\n",
        "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# --- 4. Create a preprocessing pipeline ---\n",
        "# This pipeline will handle missing values, scaling (Standardization), and one-hot encoding.\n",
        "# Scaling is crucial for interpreting coefficients, as it puts all numerical features\n",
        "# on a comparable scale, making their coefficients directly comparable in magnitude.\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')), # Impute missing numerical values with mean\n",
        "    ('scaler', StandardScaler())                  # Scale numerical features to mean 0, std dev 1\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), # Impute missing categorical values with mode\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encode categorical features\n",
        "])\n",
        "\n",
        "# Create a preprocessor using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# --- 5. Split the dataset into training and testing sets ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "print(f\"Training target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Testing target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
        "\n",
        "# --- Practical Question 18: Identify important features based on model coefficients ---\n",
        "print(\"\\n--- Question 18: Identifying Important Features based on Model Coefficients ---\")\n",
        "\n",
        "# Create a pipeline for the Logistic Regression model\n",
        "# It's important to use a solver that works well with L2 regularization (default)\n",
        "# and scaled data, like 'lbfgs'.\n",
        "model_q18 = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                            ('classifier', LogisticRegression(random_state=42, max_iter=500))])\n",
        "\n",
        "# Train the model\n",
        "print(\"Training Logistic Regression model...\")\n",
        "model_q18.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# Make predictions and evaluate accuracy (optional, but good practice)\n",
        "y_pred_q18 = model_q18.predict(X_test)\n",
        "accuracy_q18 = accuracy_score(y_test, y_pred_q18)\n",
        "print(f\"\\nModel Accuracy: {accuracy_q18:.4f}\")\n",
        "\n",
        "# --- Extract Coefficients and Map to Feature Names ---\n",
        "\n",
        "# Get the trained LogisticRegression classifier from the pipeline\n",
        "logistic_reg_classifier = model_q18.named_steps['classifier']\n",
        "\n",
        "# Get the coefficients\n",
        "coefficients = logistic_reg_classifier.coef_[0] # For binary classification, coef_ is (1, n_features)\n",
        "\n",
        "# Get the fitted preprocessor\n",
        "fitted_preprocessor = model_q18.named_steps['preprocessor']\n",
        "\n",
        "# Get feature names after preprocessing\n",
        "# 1. Get names of numerical features (they remain the same)\n",
        "processed_numerical_features = numerical_cols\n",
        "\n",
        "# 2. Get names of one-hot encoded categorical features\n",
        "# Access the 'onehot' step within the 'cat' transformer\n",
        "onehot_encoder = fitted_preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
        "processed_categorical_features = list(onehot_encoder.get_feature_names_out(categorical_cols))\n",
        "\n",
        "# Combine all processed feature names in the correct order\n",
        "# The order must match the order of columns produced by the preprocessor\n",
        "all_processed_feature_names = processed_numerical_features + processed_categorical_features\n",
        "\n",
        "# Create a DataFrame to display coefficients and their absolute values\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': all_processed_feature_names,\n",
        "    'Coefficient': coefficients,\n",
        "    'Absolute_Coefficient': np.abs(coefficients)\n",
        "})\n",
        "\n",
        "# Sort by absolute coefficient value in descending order to identify most important features\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Absolute_Coefficient', ascending=False)\n",
        "\n",
        "print(\"\\n--- Feature Importance based on Model Coefficients ---\")\n",
        "print(\"Features are sorted by the absolute value of their coefficients. Larger absolute values\")\n",
        "print(\"indicate a stronger impact on the log-odds of the target variable.\")\n",
        "print(\"Positive coefficients increase the log-odds (and thus probability) of the positive class.\")\n",
        "print(\"Negative coefficients decrease the log-odds (and thus probability) of the positive class.\")\n",
        "print(\"\\n\", feature_importance_df.to_string(index=False)) # .to_string() to print full DataFrame without truncation\n",
        "\n",
        "print(\"\\nInterpretation Notes:\")\n",
        "print(\"- The magnitude of the coefficient (absolute value) indicates the strength of the relationship.\")\n",
        "print(\"- The sign (+/-) indicates the direction of the relationship.\")\n",
        "print(\"- For one-hot encoded categorical features (e.g., Gender_Female), the coefficient is relative to the omitted category (e.g., Gender_Male).\")\n",
        "print(\"- This interpretation assumes features are scaled (standardized) for comparability.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSePGISNxhxC",
        "outputId": "3612edc1-e5c5-43d0-d553-774235b6ae99"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dummy Dataset Head ---\n",
            "   Feature_Strong_Num  Feature_Weak_Num  Feature_Random_Num  Gender      City  \\\n",
            "0                 NaN         21.708780           26.702827    Male    London   \n",
            "1           57.535715         29.380854           87.862999  Female     Paris   \n",
            "2           46.599697         24.752119           79.742602    Male    London   \n",
            "3           39.932924         17.115482           65.845183  Female  New York   \n",
            "4           17.800932         15.507927           85.058173  Female    London   \n",
            "\n",
            "   Target  \n",
            "0       0  \n",
            "1       1  \n",
            "2       1  \n",
            "3       1  \n",
            "4       0  \n",
            "\n",
            "--- Dummy Dataset Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 6 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   Feature_Strong_Num  485 non-null    float64\n",
            " 1   Feature_Weak_Num    490 non-null    float64\n",
            " 2   Feature_Random_Num  500 non-null    float64\n",
            " 3   Gender              500 non-null    object \n",
            " 4   City                500 non-null    object \n",
            " 5   Target              500 non-null    int64  \n",
            "dtypes: float64(3), int64(1), object(2)\n",
            "memory usage: 23.6+ KB\n",
            "\n",
            "--- Target Variable Distribution ---\n",
            "Target\n",
            "0    250\n",
            "1    250\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training set size: 400 samples\n",
            "Testing set size: 100 samples\n",
            "Training target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "Testing target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "--- Question 18: Identifying Important Features based on Model Coefficients ---\n",
            "Training Logistic Regression model...\n",
            "Model training complete.\n",
            "\n",
            "Model Accuracy: 0.7700\n",
            "\n",
            "--- Feature Importance based on Model Coefficients ---\n",
            "Features are sorted by the absolute value of their coefficients. Larger absolute values\n",
            "indicate a stronger impact on the log-odds of the target variable.\n",
            "Positive coefficients increase the log-odds (and thus probability) of the positive class.\n",
            "Negative coefficients decrease the log-odds (and thus probability) of the positive class.\n",
            "\n",
            "            Feature  Coefficient  Absolute_Coefficient\n",
            "Feature_Strong_Num     2.005380              2.005380\n",
            "Feature_Random_Num     0.177775              0.177775\n",
            "  Feature_Weak_Num    -0.126753              0.126753\n",
            "     City_New York    -0.090924              0.090924\n",
            "       City_London     0.070950              0.070950\n",
            "       Gender_Male    -0.038630              0.038630\n",
            "     Gender_Female     0.034458              0.034458\n",
            "        City_Paris     0.015801              0.015801\n",
            "\n",
            "Interpretation Notes:\n",
            "- The magnitude of the coefficient (absolute value) indicates the strength of the relationship.\n",
            "- The sign (+/-) indicates the direction of the relationship.\n",
            "- For one-hot encoded categorical features (e.g., Gender_Female), the coefficient is relative to the omitted category (e.g., Gender_Male).\n",
            "- This interpretation assumes features are scaled (standardized) for comparability.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. M Write a Python program to train Logistic Regression and evaluate its performance using Cohenâs Kappa\n",
        "Score."
      ],
      "metadata": {
        "id": "V8FMx0pLxsP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, cohen_kappa_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# --- 1. Create a dummy dataset for binary classification ---\n",
        "# This dataset will be used to demonstrate Cohen's Kappa score evaluation.\n",
        "np.random.seed(42) # for reproducibility\n",
        "N_SAMPLES = 500\n",
        "\n",
        "# Numerical features\n",
        "feature_num1 = np.random.rand(N_SAMPLES) * 100\n",
        "feature_num2 = np.random.randn(N_SAMPLES) * 15 + 50\n",
        "feature_num3 = np.random.randint(0, 10, N_SAMPLES)\n",
        "\n",
        "# Categorical features\n",
        "gender = np.random.choice(['Male', 'Female'], N_SAMPLES, p=[0.5, 0.5])\n",
        "city = np.random.choice(['New York', 'London', 'Paris'], N_SAMPLES, p=[0.4, 0.3, 0.3])\n",
        "\n",
        "# Binary Target variable:\n",
        "linear_combination = (0.5 * feature_num1 + 0.8 * feature_num2 - 0.3 * feature_num3 +\n",
        "                      (1 if gender[0] == 'Male' else 0) * 10 +\n",
        "                      (5 if city[0] == 'New York' else 0) +\n",
        "                      np.random.randn(N_SAMPLES) * 20)\n",
        "\n",
        "binary_target = (linear_combination > np.percentile(linear_combination, 50)).astype(int)\n",
        "\n",
        "# Introduce some missing values for demonstration of preprocessing\n",
        "missing_indices_num1 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.05), replace=False)\n",
        "missing_indices_num2 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.03), replace=False)\n",
        "feature_num1[missing_indices_num1] = np.nan\n",
        "feature_num2[missing_indices_num2] = np.nan\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Feature_Num1': feature_num1,\n",
        "    'Feature_Num2': feature_num2,\n",
        "    'Feature_Num3': feature_num3,\n",
        "    'Gender': gender,\n",
        "    'City': city,\n",
        "    'Target': binary_target # Our binary target\n",
        "})\n",
        "\n",
        "print(\"--- Dummy Dataset Head ---\")\n",
        "print(data.head())\n",
        "print(\"\\n--- Dummy Dataset Info ---\")\n",
        "data.info()\n",
        "print(\"\\n--- Target Variable Distribution ---\")\n",
        "print(data['Target'].value_counts())\n",
        "\n",
        "# --- 2. Separate features (X) and target (y) ---\n",
        "X = data.drop('Target', axis=1)\n",
        "y = data['Target'] # Use the binary target\n",
        "\n",
        "# --- 3. Identify numerical and categorical columns for preprocessing ---\n",
        "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# --- 4. Create a preprocessing pipeline ---\n",
        "# This pipeline will handle missing values, scaling, and one-hot encoding.\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')), # Impute missing numerical values with mean\n",
        "    ('scaler', StandardScaler())                  # Scale numerical features\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), # Impute missing categorical values with mode\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encode categorical features\n",
        "])\n",
        "\n",
        "# Create a preprocessor using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# --- 5. Split the dataset into training and testing sets ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "print(f\"Training target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Testing target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
        "\n",
        "# --- Practical Question 19: Evaluate performance using Cohen's Kappa Score ---\n",
        "print(\"\\n--- Question 19: Evaluate Performance using Cohen's Kappa Score ---\")\n",
        "\n",
        "# Create a pipeline for the Logistic Regression model\n",
        "model_q19 = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                            ('classifier', LogisticRegression(random_state=42, max_iter=500))])\n",
        "\n",
        "# Train the model\n",
        "print(\"Training Logistic Regression model...\")\n",
        "model_q19.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_q19 = model_q19.predict(X_test)\n",
        "\n",
        "# --- Calculate and Print Evaluation Metrics ---\n",
        "\n",
        "# Accuracy (for general context)\n",
        "accuracy = accuracy_score(y_test, y_pred_q19)\n",
        "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Confusion Matrix (useful for understanding Kappa)\n",
        "cm = confusion_matrix(y_test, y_pred_q19)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# --- Calculate Cohen's Kappa Score ---\n",
        "# Cohen's Kappa measures the agreement between two raters (or a classifier and true labels)\n",
        "# while accounting for the possibility of agreement occurring by chance.\n",
        "# A score of 1 indicates perfect agreement, 0 indicates agreement equivalent to chance,\n",
        "# and negative values indicate agreement worse than chance.\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred_q19)\n",
        "print(f\"\\nCohen's Kappa Score: {kappa_score:.4f}\")\n",
        "\n",
        "# --- Interpretation of Cohen's Kappa ---\n",
        "print(\"\\n--- Interpretation of Cohen's Kappa Score ---\")\n",
        "if kappa_score > 0.80:\n",
        "    print(\"Interpretation: Very good agreement (almost perfect).\")\n",
        "elif kappa_score > 0.60:\n",
        "    print(\"Interpretation: Substantial agreement.\")\n",
        "elif kappa_score > 0.40:\n",
        "    print(\"Interpretation: Moderate agreement.\")\n",
        "elif kappa_score > 0.20:\n",
        "    print(\"Interpretation: Fair agreement.\")\n",
        "elif kappa_score >= 0.0:\n",
        "    print(\"Interpretation: Slight agreement (or no agreement beyond chance).\")\n",
        "else:\n",
        "    print(\"Interpretation: Poor agreement (worse than chance).\")\n",
        "\n",
        "print(\"\\nCohen's Kappa is particularly useful for imbalanced datasets, where accuracy alone can be misleading.\")\n",
        "print(\"It provides a more robust measure of agreement than simple accuracy.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYA2rirHxtYY",
        "outputId": "703874c3-405e-47d5-eb59-073627cbf70d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dummy Dataset Head ---\n",
            "   Feature_Num1  Feature_Num2  Feature_Num3  Gender      City  Target\n",
            "0     37.454012     55.126340             0  Female  New York       0\n",
            "1     95.071431     78.142563             5  Female     Paris       1\n",
            "2     73.199394     64.256358             7    Male  New York       0\n",
            "3     59.865848     41.346445             2    Male  New York       0\n",
            "4     15.601864     36.523780             7  Female     Paris       0\n",
            "\n",
            "--- Dummy Dataset Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 6 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Feature_Num1  475 non-null    float64\n",
            " 1   Feature_Num2  485 non-null    float64\n",
            " 2   Feature_Num3  500 non-null    int64  \n",
            " 3   Gender        500 non-null    object \n",
            " 4   City          500 non-null    object \n",
            " 5   Target        500 non-null    int64  \n",
            "dtypes: float64(2), int64(2), object(2)\n",
            "memory usage: 23.6+ KB\n",
            "\n",
            "--- Target Variable Distribution ---\n",
            "Target\n",
            "0    250\n",
            "1    250\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training set size: 400 samples\n",
            "Testing set size: 100 samples\n",
            "Training target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "Testing target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "--- Question 19: Evaluate Performance using Cohen's Kappa Score ---\n",
            "Training Logistic Regression model...\n",
            "Model training complete.\n",
            "\n",
            "Model Accuracy: 0.7500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[36 14]\n",
            " [11 39]]\n",
            "\n",
            "Cohen's Kappa Score: 0.5000\n",
            "\n",
            "--- Interpretation of Cohen's Kappa Score ---\n",
            "Interpretation: Moderate agreement.\n",
            "\n",
            "Cohen's Kappa is particularly useful for imbalanced datasets, where accuracy alone can be misleading.\n",
            "It provides a more robust measure of agreement than simple accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "classificatio:"
      ],
      "metadata": {
        "id": "f5bd4XlOx5T2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns # For general plotting aesthetics\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay, average_precision_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# --- 1. Create a dummy dataset for binary classification ---\n",
        "# This dataset will be used to demonstrate the Precision-Recall Curve.\n",
        "# We'll aim for a scenario where the positive class might be less frequent or harder to predict\n",
        "# to make the curve more illustrative.\n",
        "np.random.seed(42) # for reproducibility\n",
        "N_SAMPLES = 500\n",
        "\n",
        "# Numerical features\n",
        "feature_num1 = np.random.rand(N_SAMPLES) * 100\n",
        "feature_num2 = np.random.randn(N_SAMPLES) * 15 + 50\n",
        "feature_num3 = np.random.randint(0, 10, N_SAMPLES)\n",
        "\n",
        "# Categorical features\n",
        "gender = np.random.choice(['Male', 'Female'], N_SAMPLES, p=[0.5, 0.5])\n",
        "city = np.random.choice(['New York', 'London', 'Paris'], N_SAMPLES, p=[0.4, 0.3, 0.3])\n",
        "\n",
        "# Binary Target variable: Create a target with some imbalance or difficulty\n",
        "# Let's make the positive class (1) slightly less common and harder to predict perfectly.\n",
        "linear_combination = (0.5 * feature_num1 + 0.8 * feature_num2 - 0.3 * feature_num3 +\n",
        "                      (1 if gender[0] == 'Male' else 0) * 10 +\n",
        "                      (5 if city[0] == 'New York' else 0) +\n",
        "                      np.random.randn(N_SAMPLES) * 25) # Increased noise to make it less separable\n",
        "\n",
        "# Adjust threshold to create a slightly imbalanced positive class\n",
        "binary_target = (linear_combination > np.percentile(linear_combination, 60)).astype(int) # Roughly 40% positive class\n",
        "\n",
        "# Introduce some missing values for demonstration of preprocessing\n",
        "missing_indices_num1 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.05), replace=False)\n",
        "missing_indices_num2 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.03), replace=False)\n",
        "feature_num1[missing_indices_num1] = np.nan\n",
        "feature_num2[missing_indices_num2] = np.nan\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Feature_Num1': feature_num1,\n",
        "    'Feature_Num2': feature_num2,\n",
        "    'Feature_Num3': feature_num3,\n",
        "    'Gender': gender,\n",
        "    'City': city,\n",
        "    'Target': binary_target # Our binary target\n",
        "})\n",
        "\n",
        "print(\"--- Dummy Dataset Head ---\")\n",
        "print(data.head())\n",
        "print(\"\\n--- Dummy Dataset Info ---\")\n",
        "data.info()\n",
        "print(\"\\n--- Target Variable Distribution ---\")\n",
        "print(data['Target'].value_counts())\n",
        "print(f\"Class 0: {data['Target'].value_counts()[0]} samples ({data['Target'].value_counts(normalize=True)[0]:.2%})\")\n",
        "print(f\"Class 1: {data['Target'].value_counts()[1]} samples ({data['Target'].value_counts(normalize=True)[1]:.2%})\")\n",
        "\n",
        "\n",
        "# --- 2. Separate features (X) and target (y) ---\n",
        "X = data.drop('Target', axis=1)\n",
        "y = data['Target'] # Use the binary target\n",
        "\n",
        "# --- 3. Identify numerical and categorical columns for preprocessing ---\n",
        "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# --- 4. Create a preprocessing pipeline ---\n",
        "# This pipeline will handle missing values, scaling, and one-hot encoding.\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')), # Impute missing numerical values with mean\n",
        "    ('scaler', StandardScaler())                  # Scale numerical features\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), # Impute missing categorical values with mode\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encode categorical features\n",
        "])\n",
        "\n",
        "# Create a preprocessor using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# --- 5. Split the dataset into training and testing sets ---\n",
        "# Use stratify=y to ensure class distribution is preserved in splits.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "print(f\"Training target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Testing target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
        "\n",
        "# --- Practical Question 20: Train Logistic Regression and visualize the Precision-Recall Curve ---\n",
        "print(\"\\n--- Question 20: Visualize Precision-Recall Curve for Binary Classification ---\")\n",
        "\n",
        "# Create a pipeline for the Logistic Regression model\n",
        "model_q20 = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                            ('classifier', LogisticRegression(random_state=42, max_iter=500))])\n",
        "\n",
        "# Train the model\n",
        "print(\"Training Logistic Regression model...\")\n",
        "model_q20.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# --- Predict probabilities for the positive class ---\n",
        "# The Precision-Recall curve requires probability estimates for the positive class (class 1).\n",
        "# model.predict_proba() returns probabilities for all classes,\n",
        "# y_proba[:, 1] extracts probabilities for the positive class.\n",
        "y_proba_q20 = model_q20.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# --- Calculate Precision, Recall, and Thresholds ---\n",
        "# precision_recall_curve computes precision-recall pairs for different probability thresholds.\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_proba_q20)\n",
        "\n",
        "# Calculate Average Precision (AP) score\n",
        "# AP summarizes the precision-recall curve as the weighted mean of precisions achieved at each threshold,\n",
        "# with the increase in recall from the previous threshold used as the weight.\n",
        "ap_score = average_precision_score(y_test, y_proba_q20)\n",
        "print(f\"\\nAverage Precision (AP) Score: {ap_score:.4f}\")\n",
        "\n",
        "\n",
        "# --- Visualize the Precision-Recall Curve ---\n",
        "# The Precision-Recall curve plots Precision against Recall for different probability thresholds.\n",
        "# It is particularly informative for imbalanced datasets, where ROC curves can be misleading.\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot the Precision-Recall curve\n",
        "plt.plot(recall, precision, marker='.', label=f'Logistic Regression (AP = {ap_score:.2f})')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for Binary Classification')\n",
        "plt.grid(True)\n",
        "plt.legend(loc='lower left')\n",
        "plt.ylim([0.0, 1.05]) # Set y-axis limits to clearly see the curve\n",
        "plt.xlim([0.0, 1.05]) # Set x-axis limits\n",
        "plt.show()\n",
        "\n",
        "# --- Alternative Visualization using Scikit-learn's PrecisionRecallDisplay (more modern) ---\n",
        "# This method is often preferred as it handles labels and plotting directly.\n",
        "disp = PrecisionRecallDisplay.from_estimator(\n",
        "    model_q20, X_test, y_test, name='Logistic Regression'\n",
        ")\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "disp.plot(ax=ax)\n",
        "ax.set_title('Precision-Recall Curve (using PrecisionRecallDisplay)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nPrecision-Recall curve calculation and visualization complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bzoP7_ktx60e",
        "outputId": "4d3d9d92-376d-4cb4-8380-81768a6adf48"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dummy Dataset Head ---\n",
            "   Feature_Num1  Feature_Num2  Feature_Num3  Gender      City  Target\n",
            "0     37.454012     55.126340             0  Female  New York       0\n",
            "1     95.071431     78.142563             5  Female     Paris       1\n",
            "2     73.199394     64.256358             7    Male  New York       0\n",
            "3     59.865848     41.346445             2    Male  New York       0\n",
            "4     15.601864     36.523780             7  Female     Paris       0\n",
            "\n",
            "--- Dummy Dataset Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 6 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Feature_Num1  475 non-null    float64\n",
            " 1   Feature_Num2  485 non-null    float64\n",
            " 2   Feature_Num3  500 non-null    int64  \n",
            " 3   Gender        500 non-null    object \n",
            " 4   City          500 non-null    object \n",
            " 5   Target        500 non-null    int64  \n",
            "dtypes: float64(2), int64(2), object(2)\n",
            "memory usage: 23.6+ KB\n",
            "\n",
            "--- Target Variable Distribution ---\n",
            "Target\n",
            "0    300\n",
            "1    200\n",
            "Name: count, dtype: int64\n",
            "Class 0: 300 samples (60.00%)\n",
            "Class 1: 200 samples (40.00%)\n",
            "\n",
            "Training set size: 400 samples\n",
            "Testing set size: 100 samples\n",
            "Training target distribution:\n",
            "Target\n",
            "0    0.6\n",
            "1    0.4\n",
            "Name: proportion, dtype: float64\n",
            "Testing target distribution:\n",
            "Target\n",
            "0    0.6\n",
            "1    0.4\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "--- Question 20: Visualize Precision-Recall Curve for Binary Classification ---\n",
            "Training Logistic Regression model...\n",
            "Model training complete.\n",
            "\n",
            "Average Precision (AP) Score: 0.6946\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhsAAAIjCAYAAACqDtl9AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAcHlJREFUeJzt3XdYk9fbB/BvwCSEKYqAA8VRRa2iQrG40AqiWHfroorWum2t1PnTimjVaqt11F1HVax71Y046mrrbq1KHbgFt4AgCeS8f/iSElkJ5gnD7+e6cknOs+7cZtx5zjlPZEIIASIiIiKJWOR3AERERFS0sdggIiIiSbHYICIiIkmx2CAiIiJJsdggIiIiSbHYICIiIkmx2CAiIiJJsdggIiIiSbHYICIiIkmx2CikevXqBXd3d6O2OXToEGQyGQ4dOiRJTIVd06ZN0bRpU939GzduQCaTYcWKFfkWU0Fw+/ZtWFlZ4dixY5Lsf8WKFZDJZLhx44Yk+y9MXn8OGoL5e0Wq129e3muNkZf/c2NcvHgRxYoVw4ULFyQ7hiFYbBgo/QWdfrOyskLVqlUxZMgQxMXF5Xd4BV76Cz/9ZmFhgRIlSqBVq1Y4ceJEfodnEnFxcRg+fDg8PDxgbW0NGxsbeHl54ZtvvsGzZ8/yO7w8mzhxIurXr4+GDRvmdygmN2HCBL3npbW1NWrUqIFx48YhPj4+v8MrMIrK6zer/+/y5cujTZs2WL58OVJSUvI7RJOrUaMGWrdujfHjx+drHMXy9eiF0MSJE1GxYkW8fPkSR48exYIFC7Br1y5cuHAB1tbWZotjyZIl0Gq1Rm3TpEkTJCcnQ6FQSBRV7rp164agoCCkpaXh33//xfz589GsWTOcPHkStWrVyre43tTJkycRFBSExMREfPLJJ/Dy8gIAnDp1Ct9++y1+++037Nu3L5+jNN7Dhw/x888/4+eff5bsGD169EDXrl2hVColO0ZuFixYAFtbWyQmJmLfvn2YPHkyDhw4gGPHjkEmk5ktjrw8R8yZv6Ly+k3//05JScHdu3exd+9efPrpp5g1axZ27NgBNzc33bp5ea8taAYMGICgoCBcu3YNlStXzp8gBBlk+fLlAoA4efKkXntoaKgAINasWZPttomJiVKHV+DFxMQIAOK7777Ta9+9e7cAIAYOHJhPkf3Hz89P+Pn56e6nx7x8+fIct3v69KkoW7ascHFxEZcuXcq0PDY2VkyaNMkkMZr7uTRz5kyhUqlEQkKCWY9rLmFhYQKAePjwoV57x44dBQBx/PjxbLd98eKF1OEVGEXl9Zvd/7cQQqxevVpYWFiI+vXrmyHa/7wetxTUarVwdHQUX3/9taTHyQm7Ud7QBx98AACIiYkB8Kp/z9bWFteuXUNQUBDs7OwQHBwMANBqtZg1axZq1qwJKysruLi4oH///nj69Gmm/e7evRt+fn6ws7ODvb093nvvPaxZs0a3PKt+xLVr18LLy0u3Ta1atTB79mzd8uzGbGzYsAFeXl5QqVRwcnLCJ598grt37+qtk/647t69i/bt28PW1halSpXC8OHDkZaWluf8NW7cGABw7do1vfZnz57hyy+/hJubG5RKJapUqYJp06Zl+oah1Woxe/Zs1KpVC1ZWVihVqhRatmyJU6dO6dZZvnw5PvjgAzg7O0OpVKJGjRpYsGBBnmN+3aJFi3D37l3MnDkTHh4emZa7uLhg3LhxuvsymQwTJkzItJ67uzt69eqlu5/edXf48GEMGjQIzs7OKFeuHDZu3KhrzyoWmUym1z97+fJlfPTRRyhRogSsrKzg7e2N7du3G/TYtm7divr168PW1jbHWNNl1f88d+5c1KxZE9bW1nB0dIS3t7feczmrMQfu7u748MMPcfToUfj4+MDKygqVKlXCypUrMx3zr7/+gp+fH1QqFcqVK4dvvvkGy5cvf6NxDK+/rps2bYp3330Xp0+fRpMmTWBtbY3//e9/AICUlBSEhYWhSpUqUCqVcHNzw8iRI7M8Jb969Wr4+PjoctGkSRO9sxmmyh8AzJ8/HzVr1oRSqUSZMmUwePDgTN156Y/r4sWLaNasGaytrVG2bFlMnz7doDwVhddvuuDgYHz22Wf4448/EBkZqWvPy3tt+v/Jb7/9hv79+6NkyZKwt7dHz549s3y/z0itVmP8+PHw8vKCg4MDbGxs0LhxYxw8eFC3jhAC7u7uaNeuXabtX758CQcHB/Tv31/XJpfL0bRpU2zbts3YtJgMi403lP4iK1mypK4tNTUVgYGBcHZ2xvfff49OnToBAPr3748RI0agYcOGmD17Nnr37o2IiAgEBgZCo9Hotl+xYgVat26NJ0+eYMyYMfj2229Rp04d7NmzJ9s4IiMj0a1bNzg6OmLatGn49ttv0bRp01wH9a1YsQKdO3eGpaUlpk6dir59+2Lz5s1o1KhRpjemtLQ0BAYGomTJkvj+++/h5+eHGTNmYPHixcamTSf9DdLR0VHXlpSUBD8/P6xevRo9e/bEnDlz0LBhQ4wZMwahoaF62/fp00f3pjZt2jSMHj0aVlZW+P3333XrLFiwABUqVMD//vc/zJgxA25ubhg0aBDmzZuX57gz2r59O1QqFT766COT7O91gwYNwsWLFzF+/HiMHj0arVu3hq2tLdavX59p3XXr1qFmzZp49913AQD//PMP3n//fVy6dAmjR4/GjBkzYGNjg/bt22PLli05Hlej0eDkyZOoV69enmNfsmQJvvjiC9SoUQOzZs1CeHg46tSpgz/++CPXba9evYqPPvoIAQEBmDFjBhwdHdGrVy/8888/unXu3r2LZs2a4Z9//sGYMWMwbNgwRERE6L3x50VWr+vHjx+jVatWqFOnDmbNmoVmzZpBq9Wibdu2+P7779GmTRvMnTsX7du3xw8//IAuXbro7TM8PBw9evSAXC7HxIkTER4eDjc3Nxw4cCDbOPKavwkTJmDw4MEoU6YMZsyYgU6dOmHRokVo0aKF3nsNADx9+hQtW7aEp6cnZsyYAQ8PD4waNQq7d+/ONU9F4fWbUY8ePQDk3J1lzHvtkCFDcOnSJUyYMAE9e/ZEREQE2rdvDyFEtvuPj4/HTz/9hKZNm2LatGmYMGECHj58iMDAQJw7dw7Aqy8sn3zyCXbv3o0nT57obf/rr78iPj4en3zyiV67l5cXLly4kH9jkfLtnEohk96Nsn//fvHw4UNx+/ZtsXbtWlGyZEmhUqnEnTt3hBBChISECABi9OjRetsfOXJEABARERF67Xv27NFrf/bsmbCzsxP169cXycnJeutqtVrd3yEhIaJChQq6+0OHDhX29vYiNTU128dw8OBBAUAcPHhQCPHq1Jqzs7N499139Y61Y8cOAUCMHz9e73gAxMSJE/X2WbduXeHl5ZXtMdOln9IMDw8XDx8+FLGxseLIkSPivffeEwDEhg0bdOtOmjRJ2NjYiH///VdvH6NHjxaWlpbi1q1bQgghDhw4IACIL774ItPxMuYqKSkp0/LAwEBRqVIlvba8dqM4OjoKT0/PHNfJCIAICwvL1F6hQgUREhKiu5/+nGvUqFGm/9du3boJZ2dnvfb79+8LCwsLvf+j5s2bi1q1aomXL1/q2rRarWjQoIF45513cozz6tWrAoCYO3durrGmez2H7dq1EzVr1szxOOmPMyYmRm//AMRvv/2ma3vw4IFQKpXiq6++0rV9/vnnQiaTibNnz+raHj9+LEqUKJFpn1lJP60eHR0tHj58KGJiYsSiRYuEUqkULi4uuq4SPz8/AUAsXLhQb/tVq1YJCwsLceTIEb32hQsXCgDi2LFjQgghrly5IiwsLESHDh1EWlqa3roZn6umyN+DBw+EQqEQLVq00DvWjz/+KACIZcuW6R0PgFi5cqWuLSUlRbi6uopOnTrp2orK6zenbhQhXnWJAhAdOnTQteXlvTb9/8TLy0uo1Wpd+/Tp0wUAsW3btmzjTk1NFSkpKZnicnFxEZ9++qmuLTo6WgAQCxYs0Fu3bdu2wt3dXS+HQgixZs0aAUD88ccf2cYtJZ7ZMJK/vz9KlSoFNzc3dO3aFba2ttiyZQvKli2rt97AgQP17m/YsAEODg4ICAjAo0ePdDcvLy/Y2trqTpFFRkYiISFBV+FnlNNAteLFi+PFixd6p/9yc+rUKTx48ACDBg3SO1br1q3h4eGBnTt3ZtpmwIABevcbN26M69evG3zMsLAwlCpVCq6urmjcuDEuXbqEGTNm6J0V2LBhAxo3bgxHR0e9XPn7+yMtLQ2//fYbAGDTpk2QyWQICwvLdJyMuVKpVLq/nz9/jkePHsHPzw/Xr1/H8+fPDY49O/Hx8bCzs3vj/WSnb9++sLS01Gvr0qULHjx4oNcltnHjRmi1Wt036idPnuDAgQPo3LkzEhISdHl8/PgxAgMDceXKlUzdZRk9fvwYgP63VmMVL14cd+7cwcmTJ43etkaNGrrT9ABQqlQpVKtWTe/5tmfPHvj6+qJOnTq6thIlSui6Lg1VrVo1lCpVChUrVkT//v1RpUoV7Ny5U2/Qt1KpRO/evfW227BhA6pXrw4PDw+952p6N0z663rr1q3QarUYP348LCz033Zze10bm7/9+/dDrVbjyy+/1DtW3759YW9vn+l1bWtrq/ctWKFQwMfHJ8vXdVF8/WaU3l2YkJCQ7TrGvNf269cPcrlcd3/gwIEoVqwYdu3ale02lpaWukH8Wq0WT548QWpqKry9vXHmzBndelWrVkX9+vURERGha3vy5Al2796N4ODgTM+r9Nfxo0ePco1bCpyNYqR58+ahatWqKFasGFxcXFCtWrVMbx7FihVDuXLl9NquXLmC58+fw9nZOcv9PnjwAMB/p2/TT4MbatCgQVi/fj1atWqFsmXLokWLFujcuTNatmyZ7TY3b94E8OqN9nUeHh44evSoXlt6n2pGjo6Oen2QDx8+1BvDYWtrq9ff369fP3z88cd4+fIlDhw4gDlz5mQa83HlyhX89ddfmY6VLmOuypQpgxIlSmT7GAHg2LFjCAsLw4kTJ5CUlKS37Pnz53BwcMhx+9zY29vn+Ob0pipWrJiprWXLlnBwcMC6devQvHlzAK+6UOrUqYOqVasCeNUNIYTA119/ja+//jrLfT948CBTofw6kcMp39yMGjUK+/fvh4+PD6pUqYIWLVqge/fuBk2jLV++fKa2159vN2/ehK+vb6b1qlSpYlScmzZtgr29PeRyOcqVK5fliP2yZctmmsl15coVXLp0yaDnqoWFBWrUqGFUXHnJX3ava4VCgUqVKumWpytXrlyWH0x//fVXpn0XxddvRomJiQCQ45cHY95r33nnHb37tra2KF26dK5jiX7++WfMmDEDly9f1uv2ev29oGfPnhgyZAhu3ryJChUqYMOGDdBoNLruoIzSX8fmnF2VEYsNI/n4+MDb2zvHdZRKZaYCRKvVwtnZWa8KzSi7F6ahnJ2dce7cOezduxe7d+/G7t27sXz5cvTs2dNk0xZf/3adlffee0/vzSwsLExvMOQ777wDf39/AMCHH34IS0tLjB49Gs2aNdPlVavVIiAgACNHjszyGOkfpoa4du0amjdvDg8PD8ycORNubm5QKBTYtWsXfvjhB5NMafPw8MC5c+egVqvfaFpxdgNtM36zS6dUKnXjLubPn4+4uDgcO3YMU6ZM0a2T/tiGDx+OwMDALPed04dy+niFrAa0ZfeGlZaWpvc8qV69OqKjo7Fjxw7s2bMHmzZtwvz58zF+/HiEh4dne2wg++fbmxQ/2WnSpAmcnJxyXCer/wetVotatWph5syZWW6TcQplXrxJ/gxlTJ6L4us3o/SB1Tm9LqR+r129ejV69eqF9u3bY8SIEXB2dtaNqXt9IG7Xrl1145T+97//YfXq1fD29s7yC2T66zi357lUWGyYSeXKlbF//340bNgwyzetjOsBr570xn47UygUaNOmDdq0aQOtVotBgwZh0aJF+Prrr7PcV4UKFQAA0dHRutO+6aKjo3XLjREREYHk5GTd/UqVKuW4/tixY7FkyRKMGzdONwC2cuXKSExM1L2pZady5crYu3cvnjx5ku23o19//RUpKSnYvn273jfljCO731SbNm1w4sQJbNq0Cd26dct1fUdHx0yDb9VqNe7fv2/Ucbt06YKff/4ZUVFRuHTpEoQQeoMS03Mvl8tzzWVWypcvD5VKpZuRkdtjAF59q379/9zGxgZdunRBly5doFar0bFjR0yePBljxozJ1FVorAoVKuDq1auZ2rNqk0LlypVx/vx5NG/ePMdvjJUrV4ZWq8XFixf1unwMYWz+Mr6uM/5fqNVqxMTE5Om5kJ2i8PrNaNWqVQCQbXGeztD32itXrqBZs2a6+4mJibh//z6CgoKy3ffGjRtRqVIlbN68We85lVV3U4kSJdC6dWtEREQgODgYx44dw6xZs7Lcb0xMDCwsLIwq9kyJYzbMpHPnzkhLS8OkSZMyLUtNTdW9cbdo0QJ2dnaYOnUqXr58qbdeTt/o0vvX01lYWKB27doAkO1V8by9veHs7IyFCxfqrbN7925cunQJrVu3NuixZdSwYUP4+/vrbrkVG8WLF0f//v2xd+9e3Ujrzp0748SJE9i7d2+m9Z89e4bU1FQAQKdOnSCEyPIbXnqu0r+1Zczd8+fPsXz5cqMfW3YGDBiA0qVL46uvvsK///6bafmDBw/wzTff6O5XrlxZ12+dbvHixUZPIfb390eJEiWwbt06rFu3Dj4+PnqnWZ2dndG0aVMsWrQoy0Lm4cOHOe5fLpfD29tbbxpixsfw+++/Q61W69p27NiB27dv6633+vNSoVCgRo0aEEJkmhWRF4GBgThx4oTuuQO86rfO7gyiqXXu3Bl3797FkiVLMi1LTk7GixcvAADt27eHhYUFJk6cmOnbuDGva0Py5+/vD4VCgTlz5ujte+nSpXj+/HmeXtfZKQqv33Rr1qzBTz/9BF9fX13XZFaMea9dvHix3v/TggULkJqailatWmW7/6we8x9//JHtlVp79OiBixcvYsSIEbC0tETXrl2zXO/06dOoWbOmSbudjMEzG2bi5+eH/v37Y+rUqTh37hxatGgBuVyOK1euYMOGDZg9ezY++ugj2Nvb44cffsBnn32G9957D927d4ejoyPOnz+PpKSkbE/TffbZZ3jy5Ak++OADlCtXDjdv3sTcuXNRp04dVK9ePctt5HI5pk2bht69e8PPzw/dunVDXFwcZs+eDXd3dwwbNkzKlOgMHToUs2bNwrfffou1a9dixIgR2L59Oz788EP06tULXl5eePHiBf7++29s3LgRN27cgJOTE5o1a4YePXpgzpw5uHLlClq2bAmtVosjR46gWbNmGDJkCFq0aKH7FtK/f38kJiZiyZIlcHZ2NvpMQnYcHR2xZcsWBAUFoU6dOnpXED1z5gx++eUXvXEFn332GQYMGIBOnTohICAA58+fx969e40+vSmXy9GxY0esXbsWL168wPfff59pnXnz5qFRo0aoVasW+vbti0qVKiEuLg4nTpzAnTt3cP78+RyP0a5dO4wdOxbx8fGwt7fXewwbN25Ey5Yt0blzZ1y7dg2rV6/ONNahRYsWcHV1RcOGDeHi4oJLly7hxx9/ROvWrU0yqHbkyJFYvXo1AgIC8Pnnn8PGxgY//fQTypcvjydPnkjeP92jRw+sX78eAwYMwMGDB9GwYUOkpaXh8uXLWL9+Pfbu3Qtvb29UqVIFY8eOxaRJk9C4cWN07NgRSqUSJ0+eRJkyZTB16tQs95+X/JUqVQpjxoxBeHg4WrZsibZt2yI6Ohrz58/He++9l2lK5JsqjK/fjRs3wtbWFmq1WncF0WPHjsHT0xMbNmzIcVtj3mvVajWaN2+Ozp076/4PGjVqhLZt22a7/w8//BCbN29Ghw4d0Lp1a8TExGDhwoWoUaOGbkxJRq1bt0bJkiWxYcMGtGrVKstxgRqNRne9nnxj7ukvhVV2VxB9XUhIiLCxscl2+eLFi4WXl5dQqVTCzs5O1KpVS4wcOVLcu3dPb73t27eLBg0aCJVKJezt7YWPj4/45Zdf9I6TcTrWxo0bRYsWLYSzs7NQKBSifPnyon///uL+/fu6dV6f+ppu3bp1om7dukKpVIoSJUqI4OBg3VTe3B5X+lSy3GR3BcJ0vXr1EpaWluLq1atCCCESEhLEmDFjRJUqVYRCoRBOTk6iQYMG4vvvv9ebSpaamiq+++474eHhIRQKhShVqpRo1aqVOH36tF4ua9euLaysrIS7u7uYNm2aWLZsWaapkXmd+pru3r17YtiwYaJq1arCyspKWFtbCy8vLzF58mTx/Plz3XppaWli1KhRwsnJSVhbW4vAwEBx9erVbKe+5vSci4yMFACETCYTt2/fznKda9euiZ49ewpXV1chl8tF2bJlxYcffig2btyY62OKi4sTxYoVE6tWrcq0bMaMGaJs2bJCqVSKhg0bilOnTmXK4aJFi0STJk1EyZIlhVKpFJUrVxYjRozQy0d2U19bt26d6ZhZXW3x7NmzonHjxkKpVIpy5cqJqVOnijlz5ggAIjY2NsfHl9tUyIzHzW4KqlqtFtOmTRM1a9YUSqVSODo6Ci8vLxEeHq73OIUQYtmyZbrXmqOjo/Dz8xORkZHZPr685k+IV1NdPTw8hFwuFy4uLmLgwIHi6dOnBj2u199fisrrN/3/O/1mZWUlypUrJz788EOxbNkyvSni2eXCkPfa9P+Tw4cPi379+glHR0dha2srgoODxePHjzP9H2SMW6vViilTpogKFSoIpVIp6tatK3bs2JEpjowGDRqU45Ws06/0euXKlSyXm4NMCAlGWxFRkdGnTx/8+++/OHLkSH6HYrAvv/wSixYtQmJiokEDm4lMacWKFejduzdOnjyZ64QCUxg2bBiWLl2K2NjYLH+jq3379pDJZLleyE9K7EYhohyFhYWhatWqOHbsWIH85dfk5GS9QdePHz/GqlWr0KhRIxYaVOS9fPkSq1evRqdOnbIsNC5duoQdO3bojWvKDyw2iChH5cuXzzRYuSDx9fVF06ZNUb16dcTFxWHp0qWIj4/P9toiREXBgwcPsH//fmzcuBGPHz/G0KFDs1yvevXqukG5+YnFBhEVakFBQdi4cSMWL14MmUyGevXqYenSpWjSpEl+h0YkmYsXLyI4OBjOzs6YM2eO0VOqzY1jNoiIiEhSvM4GERERSYrFBhEREUnqrRuzodVqce/ePdjZ2eXbD9IQEREVFkIIJCQkoEyZMpl+98uYneSbw4cPiw8//FCULl1aABBbtmzJdZuDBw+KunXrCoVCISpXrmzwBZfS3b59W++iLrzxxhtvvPHGW+637C4caIh8PbPx4sULeHp64tNPP0XHjh1zXT8mJgatW7fGgAEDEBERgaioKHz22WcoXbp0rj+cky79Er+3b9/WXX5Zo9Fg3759ukuIkzSYZ/Nhrs2HuTYP5tl8Xs91fHw83Nzc3ugnBvK12GjVqlWOP0jzuoULF6JixYqYMWMGgFfzh48ePYoffvjB4GIjvevE3t4e9vb2EEIgPuklLJXWKGZljWJ8EhtNJbc0qEtKo9HA2toa9vb2fLOQGHNtPsy1eTDP5pNdrt9k6EGhGrNx4sSJTD9bHBgYiC+//DLbbVJSUvR+iS8+Ph7Aq2RqNBokqVPhOekAgGIY+ecBKcIu8rzKF8cvn72X6xMx/dcPTfFrn5Qz5tp8mGvzYJ7N5/VcmyLnharYiI2NhYuLi16bi4sL4uPjM12yON3UqVOz/Anjffv2wdraGilpQCFLQ4Fz+tYzbN2xG0oDrwwdGRkpbUCkw1ybD3NtHsyz+aTnOikp6Y33VeQ/ZceMGYPQ0FDd/fS+pxYtWui6UT74IAUHDhzABx98ALm8yKfEZJLVaXh/2mEAQGBgC1grcs6dRqNBZGQkAgICeBpUYsy1+TDX5sE8m8/ruU7vEXgTheqT1dXVFXFxcXptcXFxsLe3z/KsBgAolUoolcpM7XK5XPeEdZDJoLQEHGys+CQ2glyemuFvucGFWsbck7SYa/Nhrs2DeTaf9FybIt+F6qJevr6+iIqK0muLjIyEr69vPkVEREREucnXYiMxMRHnzp3T/fRtTEwMzp07h1u3bgF41QXSs2dP3foDBgzA9evXMXLkSFy+fBnz58/H+vXrMWzYsPwIn4iIiAyQr8XGqVOnULduXdStWxcAEBoairp162L8+PEAgPv37+sKDwCoWLEidu7cicjISHh6emLGjBn46aefDJ72SkREROaXr2M2mjZtCpHDj86uWLEiy23Onj0rYVRERERkSoVqzAYREREVPiw2iIiISFIsNoiIiEhSLDaIiIhIUiw2iIiISFIsNoiIiEhSLDaIiIhIUiw2iIiISFIsNoiIiEhSLDaIiIhIUiw2iIiISFIsNoiIiEhSLDaIiIhIUiw2iIiISFIsNoiIiEhSLDaIiIhIUiw2iIiISFIsNoiIiEhSLDaIiIhIUiw2iIiISFIsNoiIiEhSLDaIiIhIUiw2iIiISFIsNoiIiEhSLDaIiIhIUiw2iIiISFIsNoiIiEhSLDaIiIhIUiw2iIiISFIsNoiIiEhSLDaIiIhIUiw2iIiISFIsNoiIiEhSLDaIiIhIUiw2iIiISFIsNoiIiEhSLDaIiIhIUiw2iIiISFIsNoiIiEhSLDaIiIhIUiw2iIiISFIsNoiIiEhSxfI7ACoaktRpua6j0aRCCDMEQ0REBQqLDcqzjIWD9zf7Ddqmop0lgoJYcRARvU3YjUJ5lqzJ/WzG62ISZHnajoiICi+e2SCTODKyGUraKrJdnqROM/jsBxERFS0sNsgkVApLWCv4dCIioszYjUJERESSYrFBREREkmKxQURERJJisUF5VsJakeXfREREGXFEH+WZhYUM16cE6f4mIiLKCosNeiMsMoiIKDfsRiEiIiJJsdggIiIiSbHYICIiIkmx2CAiIiJJsdggIiIiSbHYICIiIkmx2CAiIiJJsdggIiIiSbHYICIiIkmx2CAiIiJJsdggIiIiSbHYICIiIkmx2CAiIiJJsdggIiIiSfEn5qnAEkIgWZNm8PoquSVkMv7kPRFRQcNigwokIQQ+WngCp28+NXgb7wqO2DDAlwUHEVEBw24UKpCSNWlGFRoAcOrmU6POhBARkXnwzAYVeKfG+cNaYZnt8iR1Gry/2W/GiIiIyBgsNqjAs1ZYwlrBpyoRUWHFbhQiIiKSFIsNIiIikhTPTZPZJavTIJen5rhOkpoDPYmIigoWG2QWQvz39/vTDudfIEREZHbsRiGzyOuUVO8KjlDJs5+JQkREBR/PbJDZHQhtBNfiNgaty6uCEhEVfiw2yOys5ZzKSkT0NmE3ChEREUkq34uNefPmwd3dHVZWVqhfvz7+/PPPHNefNWsWqlWrBpVKBTc3NwwbNgwvX740U7RERERkrHwtNtatW4fQ0FCEhYXhzJkz8PT0RGBgIB48eJDl+mvWrMHo0aMRFhaGS5cuYenSpVi3bh3+97//mTlyIiIiMlS+FhszZ85E37590bt3b9SoUQMLFy6EtbU1li1bluX6x48fR8OGDdG9e3e4u7ujRYsW6NatW65nQ4iIiCj/5NsoPbVajdOnT2PMmDG6NgsLC/j7++PEiRNZbtOgQQOsXr0af/75J3x8fHD9+nXs2rULPXr0yPY4KSkpSElJ0d2Pj48HAGg0Gmg0Gt3fGf8l00vNkFtNaqpJc63RpGb4WwONTOSw9tuBz2nzYa7Ng3k2Hyk+G/Ot2Hj06BHS0tLg4uKi1+7i4oLLly9nuU337t3x6NEjNGrUCEIIpKamYsCAATl2o0ydOhXh4eGZ2vft2wdra2u9tsjIyDw8EjKEVgDpT7dTxw7DwoSzWVPS/tv33r37oORlOXT4nDYf5to8mGfzSc91UlLSG++rUM0/PHToEKZMmYL58+ejfv36uHr1KoYOHYpJkybh66+/znKbMWPGIDQ0VHc/Pj4ebm5uaNGiBezt7QG8qtoiIyMREBAAuVxulsfyNgoIUCMycj8CW5g2z0nqVIz88wAAIDCwBafVgs9pc2KuzYN5Np/Xc53eI/Am8u1d2cnJCZaWloiLi9Nrj4uLg6ura5bbfP311+jRowc+++wzAECtWrXw4sUL9OvXD2PHjoWFReYhKEqlEkqlMlO7XC7P9ITNqo1My0Jm+jzLxX+nSV7tm8VGOj6nzYe5Ng/m2XzSc22KfOfbAFGFQgEvLy9ERUXp2rRaLaKiouDr65vlNklJSZkKCkvLV+fMhWA/PRERUUGUr18BQ0NDERISAm9vb/j4+GDWrFl48eIFevfuDQDo2bMnypYti6lTpwIA2rRpg5kzZ6Ju3bq6bpSvv/4abdq00RUdREREVLDka7HRpUsXPHz4EOPHj0dsbCzq1KmDPXv26AaN3rp1S+9Mxrhx4yCTyTBu3DjcvXsXpUqVQps2bTB58uT8eghERESUi3zv3B4yZAiGDBmS5bJDhw7p3S9WrBjCwsIQFhZmhsiIiIjIFPL9cuVERERUtLHYICIiIkmx2CAiIiJJsdggIiIiSbHYICIiIkmx2CAiIiJJsdggIiIiSbHYICIiIkmx2CAiIiJJsdggIiIiSbHYICIiIkmx2CAiIiJJsdggIiIiSbHYICIiIkmx2CAiIiJJsdggIiIiSbHYICIiIkmx2CAiIiJJFcvvAIjI9IQQSEkDktSpkAtZruur5JaQyXJfj4goL1hsEBUxQgh0/ekkztwqhpF/HjBoG+8KjtgwwJcFBxFJgt0oREVMsiYNZ249M2qbUzefIlmTJk1ARPTW45kNKlKS1IZ9YL4t3Qa/j/KDvY1VtsuT1Gnw/ma/GSMiorcRiw0q9IT4729DPzjflm4DlcIS1gq+zIkof7EbhQq9vJz+Z7cBEZH58CsPFSlHRjZDSVtFtssLc7eBEMKgAsnQriQiInNhsUFFSlHtNhBC4KOFJ3D65tP8DoWIyGjsRiEqBJI1aUYXGhXtBFRyS4kiIiIyXNH7CkhUxJ0a5w9rRc5FhEajwcHIfUYNgDWm++Vtmc1DRKbBYoOokLE2oKtIIxMwpBbIy0we4O2ZzUNEpsFuFKK3WF5n5HA2DxEZg2c2iPKJobNLAPPMMMltJk96HIV1Ng8R5R8WG0T5oCDOLimqM3mIKP+xG4UKvRLWiiz/LsjyMrsEeDVWgjNMiKiw4dcYKvQsLGS4PiVI97ehCsrsC0Nml5gjDiIiqbDYoCLB0CKjIM6+MGR2iVQK41khIip8WGzQW+VNZ18UtTENeT0rRERkjKL1zklkBClmXxTG3y9hkUFEUmOxQW8tU8++KIgzTIiICgKj32lTUlLwxx9/4ObNm0hKSkKpUqVQt25dVKxYUYr4iAqNvMww4ewSInobGFxsHDt2DLNnz8avv/4KjUYDBwcHqFQqPHnyBCkpKahUqRL69euHAQMGwM7OTsqYiQo8Q2eYcHYJEb0NDLrORtu2bdGlSxe4u7tj3759SEhIwOPHj3Hnzh0kJSXhypUrGDduHKKiolC1alVERkZKHTdRgZY+wyS3GwsNInobGHRmo3Xr1ti0aRPkcnmWyytVqoRKlSohJCQEFy9exP37900aJBERERVeBhUb/fv3N3iHNWrUQI0aNfIcEBERERUtnI1CZKDcpqsWpOmsREQFicmKjfPnz6NevXpIS+MbLhVcxl4xM69XHCUiov+Y9MyGyPjOTFQAGXvFzLxccZTTWYmI9BlcbHTs2DHH5c+fP+fIeioU8nrFTEOuOAq8PdNZDe02elvyQUTZM7jY+PXXXxEQEAAXF5csl7P7hIo6U19xtDDKS7eSlD9iR0SFg8HvnNWrV0enTp3Qp0+fLJefO3cOO3bsMFlgRFTw5KVbqaj+iB0RGc7gV7+XlxfOnDmTbbGhVCpRvnx5kwVGRAVbbt1Kxv6IHREVXQYXGwsXLsyxq6R69eqIiYkxSVBEVPCxW4mIDGXwO4VSqZQyDiIqwjiYlOjtxq8lRCQJDiYlonQG/RAb0dvK2IuAFXXG5ONNBpMSUdHCMxtEOTD2ImBFXV7zwcGkRG83FhtEuWCRoS8v+eBgUqK3G7tRiIiISFJ5KjZWrlyJbdu26bVt27YNK1euNElQREREVHTkqdjo1asXxowZo9c2atQo9O7d2yRBEVHhx8G1RJQuT52oWq02U9vly5ffOBgiKjo4uJaI0nHEFhFJhkUGEQEGFhvx8fEG79De3j7PwRAREVHRY1CxUbx48Vyv6CeEgEwm40/NExERkR6Dio2DBw9KHQcREREVUQYVG35+flLHQUREREVUnqa+HjlyBJ988gkaNGiAu3fvAgBWrVqFo0ePmjQ4IiIiKvyMLjY2bdqEwMBAqFQqnDlzBikpKQCA58+fY8qUKSYPkIiIiAo3o4uNb775BgsXLsSSJUsgl8t17Q0bNsSZM2dMGhwREREVfkYXG9HR0WjSpEmmdgcHBzx79swUMREREVERYnSx4erqiqtXr2ZqP3r0KCpVqmSSoIiIiKjoMLrY6Nu3L4YOHYo//vgDMpkM9+7dQ0REBIYPH46BAwdKESMREREVYkZfrnz06NHQarVo3rw5kpKS0KRJEyiVSgwfPhyff/65FDESEVEuhBBI1hh+UUWV3DLXizUSmYrRxYZMJsPYsWMxYsQIXL16FYmJiahRowZsbW2liI+IiHIhhMBHC0/g9M2nBm/jXcERGwb4suAgs8jzD7EpFArY2dnBzs6OhQYRUT5K1qQZVWgAwKmbT5GsSYO1gr/HSdIz+lmWmpqK8PBwzJkzB4mJiQAAW1tbfP755wgLC9ObDktEVNgY2x2h0aRCCAkDMtKpcf6wVlhmuzxJnQbvb/abMSKiPBQbn3/+OTZv3ozp06fD19cXAHDixAlMmDABjx8/xoIFC0weJBGROeSlOwIAKtpZIijItBWHMUVPkvq/9awVljxbQQWO0c/INWvWYO3atWjVqpWurXbt2nBzc0O3bt1YbBBRoZWX7ggAiEmQIVmTBoXCNHHkteghKqiMLjaUSiXc3d0ztVesWBEKU73SiIjyWW7dEYB+l0SyOg1yeWqu+zVkFkheix7vCo5QyXOOmSg/GF1sDBkyBJMmTcLy5cuhVCoBACkpKZg8eTKGDBli8gCJiPKDId0RGcdqvD/tsEH7NXYWiCFFTzpOZ6WCyqBio2PHjnr39+/fj3LlysHT0xMAcP78eajVajRv3tz0ERIRvSFDxz9kHPtgCGMGkqY7dfMpHr9Q5zqIMx3HYFBRYNAz2MHBQe9+p06d9O67ubnlOYB58+bhu+++Q2xsLDw9PTF37lz4+Phku/6zZ88wduxYbN68GU+ePEGFChUwa9YsBAUF5TkGIiq6zDX+4UBoI7gWt8l2+YuUNLw3+VWXS0GZDWJoccUzJvSmDCo2li9fLsnB161bh9DQUCxcuBD169fHrFmzEBgYiOjoaDg7O2daX61WIyAgAM7Ozti4cSPKli2Lmzdvonjx4pLER0TmZ8zZBanGP+Rl7IO1POczEMaeNclrHLnJ2PVjaNHDC4DRm8rXc3MzZ85E37590bt3bwDAwoULsXPnTixbtgyjR4/OtP6yZcvw5MkTHD9+XHc9j6wGq2aUkpKClJQU3f34+HgAgEajgUaj0f2d8V+SBvNsPoUt12r1fwMrjfnW71W+OH757L0cPwQ1mv/2/fsoP6gMGP+gklsiNTX3wZ6pGfKrSU3NMd8Z1z0Q2gglbXIfUG9oHMZISErJfaXXnLr5FPFJL/O1O6ewPacLMyk+G2VCGH85mo0bN2L9+vW4desW1Gq13rIzZ84YtA+1Wg1ra2ts3LgR7du317WHhITg2bNn2LZtW6ZtgoKCUKJECVhbW2Pbtm0oVaoUunfvjlGjRsHSMus3kAkTJiA8PDxT+5o1a2BtbW1QrEQkrQQNMO5U3j7IpvukQplD/ZCSBoz8s5hB6xpLK4Bhv7/a9w/vp8Iihy/+GR/jN96psMun6x9mjGN83VTY5hCHWvvfuqbOHRUeSUlJ6N69O54/fw57e/s87cPoV/ecOXMwduxY9OrVC9u2bUPv3r1x7do1nDx5EoMHDzZ4P48ePUJaWhpcXFz02l1cXHD58uUst7l+/ToOHDiA4OBg7Nq1C1evXsWgQYOg0WgQFhaW5TZjxoxBaGio7n58fDzc3NzQokULXdI0Gg0iIyMREBDAK6BKiHk2n8KW68eJKRh36tVsDkO+9Ser03SzP5o0a57j2YpkdRrw56t1AwNbmPzbeUCAGpGR+xHYIudcZ3yM/s2bo6St0qRxGCpjHEEtco4jSZ2KcacOAJAmd8YobM/pwuz1XKf3CLwJo5858+fPx+LFi9GtWzesWLECI0eORKVKlTB+/Hg8efLkjQPKiVarhbOzMxYvXgxLS0t4eXnh7t27+O6777ItNpRKpW6KbkZyuTzTEzarNjI95tl8Ckuui8m1ur/tra3gYJPzB3GxYv91LRg65RRIz4fpPzAtZLnnOuNjLJaP/y/ODsUy/G0DixxOx8jFf8ukyp2xCstzuihIz7Up8m30M+fWrVto0KABAEClUiEhIQEA0KNHD7z//vv48ccfDdqPk5MTLC0tERcXp9ceFxcHV1fXLLcpXbo05HK5XpdJ9erVERsbC7VazYuKEb0l8jLlNL8veFXCWpHl3+ZmYSHD9SlBur+JzMHoYsPV1VU35bR8+fL4/fff4enpiZiYGBgz/EOhUMDLywtRUVG6MRtarRZRUVHZXhysYcOGWLNmDbRaLSwsLAAA//77L0qXLs1Cg+gtdWRkM5S0NWywZX7OpihIH/L5fXx6+1gYu8EHH3yA7du3AwB69+6NYcOGISAgAF26dEGHDh2M2ldoaCiWLFmCn3/+GZcuXcLAgQPx4sUL3eyUnj17YsyYMbr1Bw4ciCdPnmDo0KH4999/sXPnTkyZMsWosSJEVLSo/v+iV7ndCsK0TQsLGT/o6a1k9JmNxYsXQ6t91fc4ePBglCxZEsePH0fbtm3Rv39/o/bVpUsXPHz4EOPHj0dsbCzq1KmDPXv26AaN3rp1S3cGA3h18bC9e/di2LBhqF27NsqWLYuhQ4di1KhRxj4MIipACkoXAxFJw+hiw8LCQq8A6Nq1K7p27ZrnAIYMGZJtt8mhQ4cytfn6+uL333/P8/GIqOApSF0MRGR6BhUbf/31l8E7rF27dp6DIaK3lzFFBs+EEBUuBhUbderUgUwmy3UAqEwmQ1qa8aPEiYiMwTMhRIWLQcVGTEyM1HEQERmFRQZR4WFQsVGhQgWp4yAiIqIiyuipr0RERETGYLFBREREkmKxQURERJJisUFERESSylOx8ezZM/z0008YM2aM7pdez5w5g7t375o0OCIiIir8jL6C6F9//QV/f384ODjgxo0b6Nu3L0qUKIHNmzfj1q1bWLlypRRxEhERUSFl9JmN0NBQ9OrVC1euXIGVlZWuPSgoCL/99ptJgyMiIqLCz+hi4+TJk1n+4FrZsmURGxtrkqCIiIio6DC62FAqlYiPj8/U/u+//6JUqVImCYqIiIiKDqOLjbZt22LixInQaDQAXv0eyq1btzBq1Ch06tTJ5AESERFR4WZ0sTFjxgwkJibC2dkZycnJ8PPzQ5UqVWBnZ4fJkydLESMREREVYkbPRnFwcEBkZCSOHj2Kv/76C4mJiahXrx78/f2liI+IiAqAJLXhv+itkltCJiuaP5QnhECyhrkwltHFxu3bt+Hm5oZGjRqhUaNGUsREREQFgBD//e39zX6Dt6tR2h4bBvjCkM/YwvRhLITARwtP4PTNpwZvY0wugMKVD2MYXWy4u7ujUaNG+OSTT/DRRx/B0dFRiriIiCifGfMNPqOL9+NRM2yvQesa+mGs0aTqFT/5IVmTZlShARiXCwDwruD4//koWgWH0cXGqVOnsGbNGkycOBGff/45WrZsiU8++QRt2rSBUqmUIkYiIspnR0Y2Q0lbRY7rPE5Uo/H0g0bt15gP44p2lggKMn3FYWjXSMaupFPj/GGtsMx23bzkAgBO3XyKZE0arBVGfzwXaEY/mrp166Ju3bqYPn06Dh06hDVr1qBfv37QarXo2LEjli1bJkWcRESUj1QKy1w/AJMU/30Y51ac5OXDOCZBhmRNGhQ51zxGyUvXCABY55IPY3IBvCpkjOmqKmzyXDrJZDI0a9YMzZo1w8CBA9GnTx/8/PPPLDaIiCjX4sSYD2MpP4jz0jXiXcERKnn2ZzVeZ0ihVtTl+dHfuXMHa9aswZo1a3DhwgX4+vpi3rx5poyNiIjyUQlrRZZ/m5oxH8bJ6jTI5am57zMPAy1z6xoxZt/myl1hYXSxsWjRIqxZswbHjh2Dh4cHgoODsW3bNlSoUEGK+IiIKJ9YWMhwfUqQ7u/cGPMBa8y6GQeGvj/tcK5xAIYPPM04DiO3rhFjGJu7os7orH7zzTfo1q0b5syZA09PTyliIiKiAsKYD0pjPmCNWTcvs2KMnQUiBRYZ/zG62Lh161aRm5JDRESmYWxxYqwDoY3gWtwm2+V5nQVi7DgMMo5BxcZff/2Fd999FxYWFvj7779zXLd27domCYyIiOh11nLTzgJJV1QvplVQGFRs1KlTB7GxsXB2dkadOnUgk8kgMnSipd+XyWRIS8vbRWCIiIhMibNACg6D/hdiYmJ0Px8fExMjaUBEREQZZRxA6mjCgadkPgYVGxlnmty8eRMNGjRAsWL6m6ampuL48eOclUJERCZlYSFDdHgAdu3ebdKBp2Q+Rv/EfLNmzfDkyZNM7c+fP0ezZs1MEhQREVFGFhYyGFo7WFjIWGgUMEYXG+ljM173+PFj2NhkP0KYiIiI3k4Gj5zp2LEjgFeDQXv16qX3o2tpaWn466+/0KBBA9NHSERERIWawcWGg4MDgFdnNuzs7KBSqXTLFAoF3n//ffTt29f0ERIREVGhZnCxsXz5cgCAu7s7hg8fzi4TIiIiMojRE5DDwsKkiIOIiIiKKIOKjXr16iEqKgqOjo6oW7dujldZO3PmjMmCIyIiosLPoGKjXbt2ugGh7du3lzIeIiIiKmIMKjYydp2wG4WIiIiMYfR1Nm7fvo07d+7o7v/555/48ssvsXjxYpMGRkREREWD0cVG9+7dcfDgq5/vjY2Nhb+/P/7880+MHTsWEydONHmAREREVLgZXWxcuHABPj4+AID169ejVq1aOH78OCIiIrBixQpTx0dERESFnNHFhkaj0Q0W3b9/P9q2bQsA8PDwwP37900bHRERERV6RhcbNWvWxMKFC3HkyBFERkaiZcuWAIB79+6hZMmSJg+QiIiICjeji41p06Zh0aJFaNq0Kbp16wZPT08AwPbt23XdK0RERETpjL6CaNOmTfHo0SPEx8fD0dFR196vXz9YW1ubNDgiIiIq/IwuNgDA0tISqampOHr0KACgWrVqcHd3N2VcREREVEQY3Y3y4sULfPrppyhdujSaNGmCJk2aoEyZMujTpw+SkpKkiJGIiIgKMaOLjdDQUBw+fBi//vornj17hmfPnmHbtm04fPgwvvrqKyliJCIiemskqdOQpE7N8fYi5dUtt/XSb0KIfH1MRnejbNq0CRs3bkTTpk11bUFBQVCpVOjcuTMWLFhgyviIiIiKvIy1gPc3+02+f+8KjtgwwDfHH1KVktFnNpKSkuDi4pKp3dnZmd0oREREeZCsSZN0/6duPpX8GDkx+syGr68vwsLCsHLlSlhZWQEAkpOTER4eDl9fX5MHSERE9DY5MrIZStoqsl3+OFGNxtMPGrRukjpNkjMlxjK62Jg1axYCAwNRrlw53TU2zp8/DysrK+zdu9fkARIREb1NVApLWCuy/3hOUqQZvG5BYXSEtWrVwtWrV7FmzRpcunQJANCtWzcEBwdDpVKZPEAiIqKiroS1Isu/33TdgsKoYuP333/Hr7/+CrVajQ8++ACfffaZVHERERG9NSwsZLg+JUj3t6nWLSgMLjY2btyILl26QKVSQS6XY+bMmZg2bRqGDx8uZXxERERvBWMKh8JSZKQzeDbK1KlT0bdvXzx//hxPnz7FN998gylTpkgZGxERERUBBhcb0dHRGD58OCwtLQEAX331FRISEvDgwQPJgiMiIqLCz+BiIykpCfb29rr7CoUCVlZWSExMlCQwIiIiKhqMGiD6008/wdbWVnc/NTUVK1asgJOTk67tiy++MF10REREVOgZXGyUL18eS5Ys0WtzdXXFqlWrdPdlMhmLDSIiItJjcLFx48YNCcMgIiKiosro30YhIiIiMoZBxcbatWsN3uHt27dx7NixPAdERERERYtBxcaCBQtQvXp1TJ8+XXeJ8oyeP3+OXbt2oXv37qhXrx4eP35s8kCJiIiocDJozMbhw4exfft2zJ07F2PGjIGNjQ1cXFxgZWWFp0+fIjY2Fk5OTujVqxcuXLiQ5U/QExER0dvJ4AGibdu2Rdu2bfHo0SMcPXoUN2/eRHJyMpycnFC3bl3UrVsXFhYcAkJERET6jP7VVycnJ7Rv316CUIiIiKgoMrrYICIiosInSZ1m0HoaTSqEMO2xWWwQEREVURmLBu9v9hu8XUU7SwQFma7i4CALIiKiIipZY9jZjNfFJMjyvG1WeGaDiIjoLXBkZDOUtFXkuE6SOs2oMyCGYrFBRET0FlApLGGtyJ+PfaOPmpaWhhUrViAqKgoPHjyAVqvVW37gwAGTBUdERER5V8JakeXf5mZ0sTF06FCsWLECrVu3xrvvvguZTCZFXERERPSGLCxkuD4lSPd3fjG62Fi7di3Wr1+PoKAgKeIhIiIiE8rPIkMXg7EbKBQKVKlSRYpYiIiIqAgyutj46quvMHv2bAhTX/GDiIiIiiSju1GOHj2KgwcPYvfu3ahZsybkcrne8s2bN5ssOCIiIir8jC42ihcvjg4dOkgRCxERERVBRhcby5cvN3kQ8+bNw3fffYfY2Fh4enpi7ty58PHxyXW7tWvXolu3bmjXrh22bt1q8riIiIjozeX5cuUPHz7E0aNHcfToUTx8+DDPAaxbtw6hoaEICwvDmTNn4OnpicDAQDx48CDH7W7cuIHhw4ejcePGeT42ERERSc/oYuPFixf49NNPUbp0aTRp0gRNmjRBmTJl0KdPHyQlJRkdwMyZM9G3b1/07t0bNWrUwMKFC2FtbY1ly5Zlu01aWhqCg4MRHh6OSpUqGX1MIiIiMh+ju1FCQ0Nx+PBh/Prrr2jYsCGAV4NGv/jiC3z11VdYsGCBwftSq9U4ffo0xowZo2uzsLCAv78/Tpw4ke12EydOhLOzM/r06YMjR47keIyUlBSkpKTo7sfHxwMANBoNNBqN7u+M/5I0mGfzYa7Nh7k2D+bZPDSaVL2/M35Wvgmji41NmzZh48aNaNq0qa4tKCgIKpUKnTt3NqrYePToEdLS0uDi4qLX7uLigsuXL2e5zdGjR7F06VKcO3fOoGNMnToV4eHhmdr37dsHa2trvbbIyEjDAqc3wjybD3NtPsy1eTDP0kpJA9JLgwMHDkBpiTz1WrzO6GIjKSkpU3EAAM7OziYJKCcJCQno0aMHlixZAicnJ4O2GTNmDEJDQ3X34+Pj4ebmhhYtWsDe3h7Aq0o5MjISAQEBmabykukwz+bDXJsPc20ezLN5CCHwwQcpOHDgAFoH+kOhUOh6BN6E0cWGr68vwsLCsHLlSlhZWQEAkpOTER4eDl9fX6P25eTkBEtLS8TFxem1x8XFwdXVNdP6165dw40bN9CmTRtdW/oPwRUrVgzR0dGoXLmy3jZKpRJKpTLTvuRyeaYnbFZtZHrMs/kw1+bDXJsH8yw9B5kMSstXVww3Vb6NLjZmz56NwMBAlCtXDp6engCA8+fPw8rKCnv37jVqXwqFAl5eXoiKikL79u0BvCoeoqKiMGTIkEzre3h44O+//9ZrGzduHBISEjB79my4ubkZ+3CIiIhIYkYXG++++y6uXLmCiIgI3biKbt26ITg4GCqVyugAQkNDERISAm9vb/j4+GDWrFl48eIFevfuDQDo2bMnypYti6lTp8LKygrvvvuu3vbFixfXxUVEREQFj9HFBgBYW1ujb9++JgmgS5cuePjwIcaPH4/Y2FjUqVMHe/bs0Y0LuXXrFiws8nw5ECIiIspnBhUb27dvR6tWrSCXy7F9+/Yc123btq3RQQwZMiTLbhMAOHToUI7brlixwujjERERkfkYVGy0b98esbGxcHZ21o2tyIpMJkNaWpqpYiMiIqIiwKBiI33Gx+t/ExEREeXGJIMhnj17ZordEBERURFkdLExbdo0rFu3Tnf/448/RokSJVC2bFmcP3/epMERERFR4Wd0sbFw4ULd9SwiIyOxf/9+7NmzB61atcKIESNMHiAREREVbkZPfY2NjdUVGzt27EDnzp3RokULuLu7o379+iYPkIiIiAo3o89sODo64vbt2wCAPXv2wN/fH8Cr66lzJgoRERG9zugzGx07dkT37t3xzjvv4PHjx2jVqhUA4OzZs6hSpYrJAyQiIqLCzehi44cffoC7uztu376N6dOnw9bWFgBw//59DBo0yOQBEhERUeFmdLEhl8sxfPjwTO3Dhg0zSUBERERUtBSIy5UTERFR0cXLlRMREZGkeLlyIiIikhR/u52IiIgkZXSx8cUXX2DOnDmZ2n/88Ud8+eWXpoiJiIiIihCji41NmzahYcOGmdobNGiAjRs3miQoIiIiKjqMLjYeP34MBweHTO329vZ49OiRSYIiIiKiosPoYqNKlSrYs2dPpvbdu3ejUqVKJgmKiIiIig6jL+oVGhqKIUOG4OHDh/jggw8AAFFRUZgxYwZmzZpl6viIiIiokDO62Pj000+RkpKCyZMnY9KkSQAAd3d3LFiwAD179jR5gERERFS4GV1sAMDAgQMxcOBAPHz4ECqVSvf7KERERESvy9N1NlJTU7F//35s3rwZQggAwL1795CYmGjS4IiIiKjwM/rMxs2bN9GyZUvcunULKSkpCAgIgJ2dHaZNm4aUlBQsXLhQijiJiIiokDL6zMbQoUPh7e2Np0+fQqVS6do7dOiAqKgokwZHREREhZ/RZzaOHDmC48ePQ6FQ6LW7u7vj7t27JguMiIiIigajz2xotdosf9n1zp07sLOzM0lQREREVHQYXWy0aNFC73oaMpkMiYmJCAsLQ1BQkCljIyIioiLA6G6U77//Hi1btkSNGjXw8uVLdO/eHVeuXIGTkxN++eUXKWIkIiKiQszoYsPNzQ3nz5/HunXrcP78eSQmJqJPnz4IDg7WGzBKREREBBhZbGg0Gnh4eGDHjh0IDg5GcHCwVHERERFREWHUmA25XI6XL19KFQsREREVQUYPEB08eDCmTZuG1NRUKeIhIiKiIsboMRsnT55EVFQU9u3bh1q1asHGxkZv+ebNm00WHBERERV+RhcbxYsXR6dOnaSIhYiIiIogo4uN5cuXSxEHERERFVEGj9nQarWYNm0aGjZsiPfeew+jR49GcnKylLERERFREWBwsTF58mT873//g62tLcqWLYvZs2dj8ODBUsZGRERERYDBxcbKlSsxf/587N27F1u3bsWvv/6KiIgIaLVaKeMjIiKiQs7gYuPWrVt6v33i7+8PmUyGe/fuSRIYERERFQ0GFxupqamwsrLSa5PL5dBoNCYPioiIiIoOg2ejCCHQq1cvKJVKXdvLly8xYMAAvWtt8DobRERElJHBxUZISEimtk8++cSkwRAREVHRY3CxwetrEBERUV4Y/dsoRERERMZgsUFERESSYrFBREREkmKxQURERJJisUFERESSYrFBREREkmKxQURERJJisUFERESSYrFBREREkmKxQURERJJisUFERESSYrFBREREkmKxQURERJJisUFERESSYrFBREREkmKxQURERJJisUFERESSYrFBREREkmKxQURERJJisUFERESSYrFBREREkmKxQURERJJisUFERESSYrFBREREkmKxQURERJJisUFERESSYrFBREREkmKxQURERJJisUFERESSYrFBREREkmKxQURERJJisUFERESSYrFBREREkmKxQURERJJisUFERESSYrFBREREkmKxQURERJIqEMXGvHnz4O7uDisrK9SvXx9//vlntusuWbIEjRs3hqOjIxwdHeHv75/j+kRERJS/8r3YWLduHUJDQxEWFoYzZ87A09MTgYGBePDgQZbrHzp0CN26dcPBgwdx4sQJuLm5oUWLFrh7966ZIyciIiJD5HuxMXPmTPTt2xe9e/dGjRo1sHDhQlhbW2PZsmVZrh8REYFBgwahTp068PDwwE8//QStVouoqCgzR05ERESGKJafB1er1Th9+jTGjBmja7OwsIC/vz9OnDhh0D6SkpKg0WhQokSJLJenpKQgJSVFdz8+Ph4AoNFooNFodH9n/JekwTybD3NtPsy1eTDP5iPFZ2O+FhuPHj1CWloaXFxc9NpdXFxw+fJlg/YxatQolClTBv7+/lkunzp1KsLDwzO179u3D9bW1nptkZGRBkZOb4J5Nh/m2nyYa/Ngns0nPddJSUlvvK98LTbe1Lfffou1a9fi0KFDsLKyynKdMWPGIDQ0VHc/Pj5eN87D3t4ewKuqLTIyEgEBAZDL5WaJ/W3EPJsPc20+zLV5MM/m83qu03sE3kS+FhtOTk6wtLREXFycXntcXBxcXV1z3Pb777/Ht99+i/3796N27drZrqdUKqFUKjO1y+XyTE/YrNrI9Jhn82GuzYe5Ng/m2XzSc22KfOfrAFGFQgEvLy+9wZ3pgz19fX2z3W769OmYNGkS9uzZA29vb3OESkRERHmU790ooaGhCAkJgbe3N3x8fDBr1iy8ePECvXv3BgD07NkTZcuWxdSpUwEA06ZNw/jx47FmzRq4u7sjNjYWAGBrawtbW9t8exxERESUtXwvNrp06YKHDx9i/PjxiI2NRZ06dbBnzx7doNFbt27BwuK/EzALFiyAWq3GRx99pLefsLAwTJgwwZyhExERkQHyvdgAgCFDhmDIkCFZLjt06JDe/Rs3bkgfEBEREZlMvl/Ui4iIiIo2FhtEREQkKRYbREREJCkWG0RERCQpFhtEREQkKRYbREREJCkWG0RERCQpFhtEREQkKRYbREREJCkWG0RERCQpFhtEREQkKRYbREREJCkWG0RERCQpFhtEREQkKRYbREREJCkWG0RERCQpFhtEREQkKRYbREREJCkWG0RERCQpFhtEREQkKRYbREREJCkWG0RERCQpFhtEREQkKRYbREREJCkWG0RERCQpFhtEREQkKRYbREREJCkWG0RERCQpFhtEREQkKRYbREREJCkWG0RERCQpFhtEREQkKRYbREREJCkWG0RERCQpFhtEREQkKRYbREREJCkWG0RERCQpFhtEREQkKRYbREREJCkWG0RERCQpFhtEREQkKRYbREREJCkWG0RERCQpFhtEREQkKRYbREREJCkWG0RERCQpFhtEREQkKRYbREREJCkWG0RERCQpFhtEREQkKRYbREREJCkWG0RERCQpFhtEREQkKRYbREREJCkWG0RERCSpYvkdQEEkhEBqairS0tLyO5QiRaPRoFixYnj58iVzKzHmWlqWlpYoVqwYZDJZfodCVCiw2HiNWq3G/fv3kZSUlN+hFDlCCLi6uuL27dt8k5YYcy09a2trlC5dmvklMgCLjQy0Wi1u3LgBS0tLlClTBgqFgm8kJqTVapGYmAhbW1tYWLAHT0rMtXSEEFCr1Xj48CFiYmLg7u6e3yERFXgsNjLQaDTQarVwc3ODtbV1fodT5Gi1WqjValhZWfEDUGLMtbRUKhXkcjlu3rwJjUaT3+EQFXh8F8pACAEAfHMmolylv0+kv28QUfb4qUpERESSYrFBREREkmKxQQZzd3fHrFmz8rz9ihUrUKFCBdMFVIS8aW6N0aNHD0yZMsUsxyqsunbtihkzZuR3GERFBouNIqJXr15o3769pMc4efIk+vXrZ9C6WX14dunSBadOncrz8VesWAGZTAaZTAYLCwuULl0aXbp0wa1bt/K8z4LCmNy+ifPnz2PXrl344osvMi375ZdfYGlpicGDB2dadujQIV3uZTIZXFxc0KlTJ1y/fl3SeDds2AAPDw9YWVmhVq1a2LVrV67bpKSkYOzYsahQoQKUSiXc3d2xbNky3XKNRoOJEyeicuXKsLKygqenJ/bs2aO3j3HjxmHy5Ml4/vy5yR8T0duIxQYZrFSpUm80S0elUqFUqVJvFIO9vT3u37+Pu3fvYtOmTYiOjsbHH3/8Rvs0hNQzDt40t4aaO3cuPv74Y9ja2mZatnTpUowcORK//PILXr58meX20dHRuHfvHjZs2IB//vkHbdq0keyiYcePH0e3bt3Qp08fnD17Fu3bt0f79u1x4cKFHLfr3LkzoqKisHTpUkRHR+OXX35BtWrVdMvHjRuHRYsWYe7cubh48SIGDBiADh064OzZs7p13n33XVSuXBmrV6+W5LERvXXEW+b58+cCgHj+/LmuTa1Wi61bt4r4+Hhx8eJFkZycrFum1WrFixRNvty0Wq3BjyskJES0a9cu2+WHDh0S7733nlAoFMLV1VWMGjVKaDQa3fL4+HjRvXt3YW1tLVxdXcXMmTOFn5+fGDp0qG6dChUqiB9++EGXl7CwMOHm5iYUCoUoXbq0+Pzzz4UQQvj5+QkAejchhFi6dKmwt7cXaWlpun1u375deHt7C6VSKUqWLCnat2+f7WNYvny5cHBw0GubM2dOpv/PrVu3irp16wqlUikqVqwoJkyYoPdYL126JBo2bCiUSqWoXr26iIyMFADEli1bhBBCxMTECABi7dq1okmTJkKpVIrly5cLIYRYsmSJ8PDwEEqlUlSrVk3MmzdPt9+UlBQxePBg4erqKpRKpShfvryYMmVKrvl6PbdCCHHz5k3Rtm1bYWNjI+zs7MTHH38sYmNjdcvDwsKEp6enWLlypahQoYKwt7cXXbp0EfHx8UIIIdLS0sTTp0/1cp2amiocHBzEjh07MuX2+vXrQqVSiWfPnon69euLiIgIveUHDx4UAMTTp091bREREQKAuHz5clb/XW+sc+fOonXr1npt9evXF/379892m927dwsHBwfx+PHjbNcpXbq0+PHHH/XaOnbsKIKDg/XawsPDRaNGjbLdT3Jysrh48aKIj48XW7duFWq1OqeHQ28o/X2aeZbe67nO6nPTWLzORi6SNWmoMX5vvhz74sRAWCve/L/o7t27CAoKQq9evbBy5UpcvnwZffv2hZWVFSZMmAAACA0NxbFjx7B9+3a4uLhg/PjxOHPmDOrUqZPlPjdt2oQffvgBa9euRc2aNREbG4vz588DADZv3gxPT0/069cPffv2zTaunTt3okOHDhg7dixWrlwJtVpt0GnydA8ePMCWLVtgaWkJS0tLAMCRI0fQs2dPzJkzB40bN8a1a9d03RNhYWFIS0tD+/btUb58efzxxx9ISEjAV199leX+R48ejRkzZqBu3bqwsrJCREQExo8fjx9//BF169bF2bNn0bdvX9jY2CAkJARz5szB9u3bsX79epQvXx63b9/G7du3c83X67RaLdq1awdbW1scPnwYqampGDx4MLp06YJDhw7p1rt27Rq2bt2KHTt24OnTp+jcuTO+/fZbTJ48Ocv9/vXXX3j+/Dm8vb0zLVu+fDlat24NBwcHfPLJJ1i6dCm6d++eY/5VKhWAV1fdzUpERAT69++f4z52796Nxo0bZ7nsxIkTCA0N1WsLDAzE1q1bs93f9u3b4e3tjenTp2PVqlWwsbFB27ZtMWnSJF28KSkpsLKyyvRYjh49qtfm4+ODyZMnIyUlBUqlMsfHQUQ5Y7HxFpg/fz7c3Nzw448/QiaTwcPDA/fu3cOoUaMwfvx4vHjxAj///DPWrFmD5s2bA3j14VOmTJls93nr1i24urrC398fcrkc5cuXh4+PDwCgRIkSsLS0hJ2dHVxdXbPdx+TJk9G1a1eEh4fr2jw9PXN8LM+fP4etrS2EELpLyn/xxRewsbEBAISHh2P06NEICQkBAFSqVAmTJk3CyJEjERYWhsjISFy7dg2HDh3SxTZ58mQEBARkOtaXX36Jjh076u6HhYVhxowZuraKFSvi4sWLWLRoEUJCQnDr1i288847aNSoEWQymd5g2Jzy9bqoqCj8/fffiImJgZubGwBg5cqVqFmzJk6ePIn33nsPwKuiZMWKFbCzswPwauBnVFRUtsXGzZs3YWlpCWdnZ7329P3MnTsXwKvBkV999RViYmJQsWLFLPd1//59fP/99yhbtqxeF0VGbdu2Rf369bNclq5s2bLZLouNjYWLi4tem4uLC2JjY7Pd5vr16zh69CisrKywZcsWPHr0CIMGDcLjx4+xfPlyAK8KlpkzZ6JJkyaoXLkyoqKisHnz5kzdQWXKlIFarUZsbCwHNhO9IRYbuVDJLXFxYmC+HdsULl26BF9fX71Lrzds2BCJiYm4c+cOnj59Co1Go/fh5+DgkO2HCAB8/PHHmDVrFipVqoSWLVsiKCgIbdq0QbFihj+lzp07l+OZj6zY2dnhzJkz0Gg02L17NyIiIvQ+XM+fP49jx47ptaWlpeHly5dISkpCdHQ03Nzc9Iqg7D70M54BePHiBa5du4Y+ffroxZyamgoHBwcArwbpBgQEoFq1amjZsiU+/PBDtGjRAoBx+bp06RLc3Nx0hQYA1KhRA8WLF8elS5d0xYa7u7uu0ACA0qVL48GDB9nmLjk5GUqlMtMl+CMjI/HixQsEBQUBAJycnBAQEIBly5Zh0qRJeuuWK1dOV+h5enpi06ZNUCgUWR7Pzs5OLz5z0Gq1kMlkiIiI0P2/zJw5Ex999BHmz58PlUqF2bNno2/fvvDw8IBMJkPlypXRu3dvvUGkwH9nbvg7SURvjsVGLmQymUm6MooaNzc3REdHY//+/YiMjMSgQYPw3Xff4fDhw5DL5QbtI/3N3BgWFhaoUqUKAKB69eq4du0aBg4ciFWrVgEAEhMTER4erndGIt3rp85zk362JH2/ALBkyZJM39bTu3Dq1auHmJgY7N69G/v370fnzp3h7++PjRs3miRfr3t9O5lMBq1Wm+36Tk5OSEpKglqt1isQli5diidPnuj9f2i1Wvz1118IDw/Xu6LukSNHYG9vD2dn51wLiTftRnF1dUVcXJxeW1xcXI5ny0qXLo2yZcvqCg3g1fNECIE7d+7gnXfeQalSpbB161a8fPkSjx8/RpkyZTB69GhUqlRJb19PnjwBgDce1ExELDbeCtWrV8emTZsghNB9qz127Bjs7OxQrlw5ODo6Qi6X4+TJkyhfvjyAV90V//77L5o0aZLtflUqFdq0aYM2bdpg8ODB8PDwwN9//4169epBoVDkOkuhdu3aiIqKQu/evfP82EaPHo3KlStj2LBhqFevHurVq4fo6GhdQfK6atWq4fbt24iLi9Odoj958mSux3FxcUGZMmVw/fp1BAcHZ7uevb09unTpgi5duuCjjz5Cy5Yt8eTJE5QoUSLHfGVUvXp13XiP9LMbFy9exLNnz1CjRg1DU5NJ+vibixcv6v5+/Pgxtm3bphtLki4tLQ2NGjXCvn370LJlS117xYoVUbx4cYOO96bdKL6+voiKisKXX36pa4uMjISvr2+22zRs2BAbNmzQ/QgdAPz777+wsLBAuXLl9Na1srJC2bJlodFosGnTJnTu3Flv+YULF1CuXDk4OTnl+BiIKHcsNoqQ58+f49y5c3ptJUuWxKBBgzBr1ix8/vnnGDJkCKKjoxEWFobQ0FBYWFjAzs4OISEhGDFiBEqUKAFnZ2eEhYXBwsIi21+9XbFiBdLS0lC/fn1YW1tj9erVUKlUur5td3d3/Pbbb+jatSuUSmWWb9hhYWFo3rw5KleujK5duyI1NRW7du3CqFGjDH7Mbm5u6NChA8aPH48dO3Zg/Pjx+PDDD1G+fHl89NFHsLCwwPnz53HhwgV88803CAgIQOXKlRESEoLp06cjISEB48aNA4Bcf+E3PDwcX3zxBRwcHNCyZUukpKTg1KlTePr0KUJDQzFz5kyULl0adevWhYWFBTZs2ABXV1cUL14813xl5O/vj1q1aiE4OBizZs1CamoqBg0aBD8/vywHdxqqVKlSqFevHo4ePaorNlatWoWSJUuic+fOmR5/UFAQli5dqldsGONNu1GGDh0KPz8/zJgxA61bt8batWtx6tQpLF68WLfOmDFjcPfuXaxcuRIA0L17d0yaNAm9e/dGeHg4Hj16hBEjRuDTTz/Vnbn5448/cPfuXdSpUwd3797FhAkToNVqMXLkSL3jHzlyRNcNRkRvyBTTZAoTY6e+FhYhISGZppsCEH369BFC5G3qq4+Pjxg9erRunYzTM7ds2SLq168v7O3thY2NjXj//ffF/v37deueOHFC1K5dWyiVyhynvm7atEnUqVNHKBQK4eTkJDp27JjtY8xq6mv6sQCIP/74QwghxJ49e0SDBg2ESqUS9vb2wsfHRyxevFi3fvrUV4VCITw8PMSvv/4qAIg9e/YIIf6b+nr27NlMx4qIiNDF6+joKJo0aSI2b94shBBi8eLFok6dOsLGxkbY29uL5s2bizNnzhiUr7xOfc3ohx9+EBUqVBBCZD31VQgh5s+fL95//33d/Vq1aolBgwZlme9169YJhUIhHj58mOXUV3NYv369qFq1qlAoFKJmzZpi586destDQkKEn5+fXtulS5eEv7+/UKlUoly5ciI0NFQkJSXplh86dEhUr15dN926R48e4u7du3r7SE5OFg4ODuLEiRPZxsapr+bFqa/mI8XUVxYbomgUG6aWmJgoHBwcxE8//WSyfWb3AZjfjh49KgCIq1ev5ncoJpNdrpOSkoSbm5s4fvx4PkVWOMyfP18EBATkuA6LDfNisWE+vM4GSebs2bO4fPkyfHx88Pz5c0ycOBEA0K5du3yOzPS2bNkCW1tbvPPOO7h69SqGDh2Khg0bonLlyvkdmuRUKhVWrlyJR48e5XcoBZpcLtdNBSaiN8dig3S+//57REdHQ6FQwMvLC0eOHCmSg+MSEhIwatQo3Lp1C05OTvD393+rfnSradOm+R1CgffZZ5/ldwhERQqLDQIA1K1bF6dPn87vMMyiZ8+e6NmzZ36HQUT01uAPsREREZGkWGxkkD71TwiRz5EQUUGX/j6R25RpIiogxca8efPg7u4OKysr1K9fH3/++WeO62/YsAEeHh6wsrJCrVq1jPrxrpykXzqalycmotykv08Yc4l+ordVvr9K1q1bh9DQUCxcuBD169fHrFmzEBgYiOjo6Ew/GAUAx48fR7du3TB16lR8+OGHWLNmDdq3b48zZ87g3XfffaNYLC0tUbx4cd3vS1hbW/NbiwlptVqo1Wq8fPlS7xLYZHrMtXTE//82zIMHD1C8eHHd5eqJKHv5XmzMnDkTffv21V2yeuHChdi5cyeWLVuG0aNHZ1p/9uzZaNmyJUaMGAEAmDRpEiIjI/Hjjz9i4cKFbxxP+u8u5PSDVpQ3QggkJydDpVKxiJMYcy294sWLw9XVFampqfkdClGBl6/FhlqtxunTpzFmzBhdm4WFBfz9/XHixIkstzlx4gRCQ0P12gIDA7F169Ys109JSUFKSorufnx8PABAo9FAo9Ho/s74r5OTExwdHZGamsrxGyaUmpqK48ePo0GDBjz1LDHmWjoymQzFihWDpaUlUlNTM71/kDSYZ/PJ7rPxTeTru9CjR4+Qlpam+0GsdC4uLrh8+XKW28TGxma5fmxsbJbrT506FeHh4Zna9+3bB2tra722yMhIY8KnPPrtt9/yO4S3BnNtPnz/MA/m2XzSc22KcYxF/ivPmDFj9M6ExMfHw83NDS1atIC9vT2AV1VbZGQkAgIC8vxz35Q75tl8mGvzYa7Ng3k2n9dznd4j8CbytdhwcnKCpaUl4uLi9Nrj4uJ0Yyde5+rqatT6SqUSSqUyU7tcLs/0hM2qjUyPeTYf5tp8mGvzYJ7NJz3Xpsh3vg5TT78sdlRUlK5Nq9UiKioKvr6+WW7j6+urtz7w6lRPdusTERFR/sr3bpTQ0FCEhITA29sbPj4+mDVrFl68eKGbndKzZ0+ULVsWU6dOBQAMHToUfn5+mDFjBlq3bo21a9fi1KlTWLx4sUHHSx/wmfG0kEajQVJSEuLj41kxS4h5Nh/m2nyYa/Ngns3n9Vynf16+0YSJN/0pWlOYO3euKF++vFAoFMLHx0f8/vvvumV+fn4iJCREb/3169eLqlWrCoVCIWrWrCl27txp8LFu374tAPDGG2+88cYbb0bcbt++nefPeZkQb9fcTq1Wi3v37sHOzk53/YH0QaO3b9/WDRol02OezYe5Nh/m2jyYZ/N5PddCCCQkJKBMmTJ5vkhgvnejmJuFhQXKlSuX5TJ7e3s+ic2AeTYf5tp8mGvzYJ7NJ2OuHRwc3mhfvI4xERERSYrFBhEREUmKxQZeXYsjLCwsy+txkOkwz+bDXJsPc20ezLP5SJHrt26AKBEREZkXz2wQERGRpFhsEBERkaRYbBAREZGkWGwQERGRpN6aYmPevHlwd3eHlZUV6tevjz///DPH9Tds2AAPDw9YWVmhVq1a2LVrl5kiLdyMyfOSJUvQuHFjODo6wtHREf7+/rn+v9B/jH1Op1u7di1kMhnat28vbYBFhLF5fvbsGQYPHozSpUtDqVSiatWqfP8wkLG5njVrFqpVqwaVSgU3NzcMGzYML1++NFO0hdNvv/2GNm3aoEyZMpDJZNi6dWuu2xw6dAj16tWDUqlElSpVsGLFCuMPnOcLnRcia9euFQqFQixbtkz8888/om/fvqJ48eIiLi4uy/WPHTsmLC0txfTp08XFixfFuHHjhFwuF3///beZIy9cjM1z9+7dxbx588TZs2fFpUuXRK9evYSDg4O4c+eOmSMvfIzNdbqYmBhRtmxZ0bhxY9GuXTvzBFuIGZvnlJQU4e3tLYKCgsTRo0dFTEyMOHTokDh37pyZIy98jM11RESEUCqVIiIiQsTExIi9e/eK0qVLi2HDhpk58sJl165dYuzYsWLz5s0CgNiyZUuO61+/fl1YW1uL0NBQcfHiRTF37lxhaWkp9uzZY9Rx34piw8fHRwwePFh3Py0tTZQpU0ZMnTo1y/U7d+4sWrdurddWv3590b9/f0njLOyMzfPrUlNThZ2dnfj555+lCrHIyEuuU1NTRYMGDcRPP/0kQkJCWGwYwNg8L1iwQFSqVEmo1WpzhVhkGJvrwYMHiw8++ECvLTQ0VDRs2FDSOIsSQ4qNkSNHipo1a+q1denSRQQGBhp1rCLfjaJWq3H69Gn4+/vr2iwsLODv748TJ05kuc2JEyf01geAwMDAbNenvOX5dUlJSdBoNChRooRUYRYJec31xIkT4ezsjD59+pgjzEIvL3nevn07fH19MXjwYLi4uODdd9/FlClTkJaWZq6wC6W85LpBgwY4ffq0rqvl+vXr2LVrF4KCgswS89vCVJ+HRf6H2B49eoS0tDS4uLjotbu4uODy5ctZbhMbG5vl+rGxsZLFWdjlJc+vGzVqFMqUKZPpiU368pLro0ePYunSpTh37pwZIiwa8pLn69ev48CBAwgODsauXbtw9epVDBo0CBqNBmFhYeYIu1DKS667d++OR48eoVGjRhBCIDU1FQMGDMD//vc/c4T81sju8zA+Ph7JyclQqVQG7afIn9mgwuHbb7/F2rVrsWXLFlhZWeV3OEVKQkICevTogSVLlsDJySm/wynStFotnJ2dsXjxYnh5eaFLly4YO3YsFi5cmN+hFTmHDh3ClClTMH/+fJw5cwabN2/Gzp07MWnSpPwOjbJQ5M9sODk5wdLSEnFxcXrtcXFxcHV1zXIbV1dXo9anvOU53ffff49vv/0W+/fvR+3ataUMs0gwNtfXrl3DjRs30KZNG12bVqsFABQrVgzR0dGoXLmytEEXQnl5TpcuXRpyuRyWlpa6turVqyM2NhZqtRoKhULSmAurvOT666+/Ro8ePfDZZ58BAGrVqoUXL16gX79+GDt2LCws+F3aFLL7PLS3tzf4rAbwFpzZUCgU8PLyQlRUlK5Nq9UiKioKvr6+WW7j6+urtz4AREZGZrs+5S3PADB9+nRMmjQJe/bsgbe3tzlCLfSMzbWHhwf+/vtvnDt3Tndr27YtmjVrhnPnzsHNzc2c4RcaeXlON2zYEFevXtUVcwDw77//onTp0iw0cpCXXCclJWUqKNKLPMGf/DIZk30eGjd2tXBau3atUCqVYsWKFeLixYuiX79+onjx4iI2NlYIIUSPHj3E6NGjdesfO3ZMFCtWTHz//ffi0qVLIiwsjFNfDWBsnr/99luhUCjExo0bxf3793W3hISE/HoIhYaxuX4dZ6MYxtg837p1S9jZ2YkhQ4aI6OhosWPHDuHs7Cy++eab/HoIhYaxuQ4LCxN2dnbil19+EdevXxf79u0TlStXFp07d86vh1AoJCQkiLNnz4qzZ88KAGLmzJni7Nmz4ubNm0IIIUaPHi169OihWz996uuIESPEpUuXxLx58zj1NSdz584V5cuXFwqFQvj4+Ijff/9dt8zPz0+EhITorb9+/XpRtWpVoVAoRM2aNcXOnTvNHHHhZEyeK1SoIABkuoWFhZk/8ELI2Od0Riw2DGdsno8fPy7q168vlEqlqFSpkpg8ebJITU01c9SFkzG51mg0YsKECaJy5crCyspKuLm5iUGDBomnT5+aP/BC5ODBg1m+76bnNiQkRPj5+WXapk6dOkKhUIhKlSqJ5cuXG31c/sQ8ERERSarIj9kgIiKi/MVig4iIiCTFYoOIiIgkxWKDiIiIJMVig4iIiCTFYoOIiIgkxWKDiIiIJMVig4iIiCTFYoOoCJHJZNi6dSsA4MaNG5DJZLn+rHx0dDRcXV2RkJAgfYAA3N3dMWvWrBzXmTBhAurUqSNpHHk5Rsb85lWvXr3Qvn37N9pHVt5//31s2rTJ5PslMgUWG0Qm0KtXL8hkMshkMsjlclSsWBEjR47Ey5cv8zu0XI0ZMwaff/457OzsALz66e70xyKTyeDi4oJOnTrh+vXrJjneyZMn0a9fP939rD7Ahw8fnunHn95mv/32G9q0aYMyZcpkW/CMGzcOo0eP1vsROKKCgsUGkYm0bNkS9+/fx/Xr1/HDDz9g0aJFCAsLy++wcnTr1i3s2LEDvXr1yrQsOjoa9+7dw4YNG/DPP/+gTZs2SEtLe+NjlipVCtbW1jmuY2tri5IlS77xsYqKFy9ewNPTE/Pmzct2nVatWiEhIQG7d+82Y2REhmGxQWQiSqUSrq6ucHNzQ/v27eHv74/IyEjdcq1Wi6lTp6JixYpQqVTw9PTExo0b9fbxzz//4MMPP4S9vT3s7OzQuHFjXLt2DcCrMwIBAQFwcnKCg4MD/Pz8cObMmTeKef369fD09ETZsmUzLXN2dkbp0qXRpEkTjB8/HhcvXsTVq1cBAAsWLEDlypWhUChQrVo1rFq1SredEAITJkxA+fLloVQqUaZMGXzxxRe65Rm7Udzd3QEAHTp0gEwm093P2MWxb98+WFlZ4dmzZ3rxDR06FB988IHu/tGjR9G4cWOoVCq4ubnhiy++wIsXLwzOhaH5vX//Plq1agWVSoVKlSpl+j+8ffs2OnfujOLFi6NEiRJo164dbty4YXAcWWnVqhW++eYbdOjQIdt1LC0tERQUhLVr177RsYikwGKDSAIXLlzA8ePHoVAodG1Tp07FypUrsXDhQvzzzz8YNmwYPvnkExw+fBgAcPfuXTRp0gRKpRIHDhzA6dOn8emnnyI1NRUAkJCQgJCQEBw9ehS///473nnnHQQFBb3RWIsjR47A29s71/VUKhUAQK1WY8uWLRg6dCi++uorXLhwAf3790fv3r1x8OBBAMCmTZt0Z3auXLmCrVu3olatWlnu9+TJkwCA5cuX4/79+7r7GTVv3hzFixfXG4+QlpaGdevWITg4GABw7do1tGzZEp06dcJff/2FdevW4ejRoxgyZIjBuTA0v19//TU6deqE8+fPIzg4GF27dsWlS5cAABqNBoGBgbCzs8ORI0dw7Ngx2NraomXLllCr1Vked8WKFZDJZAbHmRMfHx8cOXLEJPsiMqk3/LVaIhKvfpbZ0tJS2NjYCKVSKQAICwsLsXHjRiGEEC9fvhTW1tbi+PHjetv16dNHdOvWTQghxJgxY0TFihWFWq026JhpaWnCzs5O/Prrr7o2AGLLli1CCCFiYmIEAHH27Nls9+Hp6SkmTpyo15b+E9TpP9V979490aBBA1G2bFmRkpIiGjRoIPr27au3zccffyyCgoKEEELMmDFDVK1aNdvHUaFCBfHDDz9kGXO6sLAw4enpqbs/dOhQ8cEHH+ju7927VyiVSl2Mffr0Ef369dPbx5EjR4SFhYVITk7OMo7Xj/G67PI7YMAAvfXq168vBg4cKIQQYtWqVaJatWpCq9XqlqekpAiVSiX27t0rhHj1XGnXrp1u+ebNm0W1atWyjeN1WeUr3bZt24SFhYVIS0szeH9E5sAzG0Qm0qxZM5w7dw5//PEHQkJC0Lt3b3Tq1AkAcPXqVSQlJSEgIAC2tra628qVK3XdJOfOnUPjxo0hl8uz3H9cXBz69u2Ld955Bw4ODrC3t0diYiJu3bqV55iTk5NhZWWV5bJy5crBxsYGZcqUwYsXL7Bp0yYoFApcunQJDRs21Fu3YcOGum/3H3/8MZKTk1GpUiX07dsXW7Zs0Z2dyavg4GAcOnQI9+7dAwBERESgdevWKF68OADg/PnzWLFihV5uAwMDodVqERMTY9AxDM2vr69vpvvpj/38+fO4evUq7OzsdHGUKFECL1++1P0/v65Dhw64fPmyMenIlkqlglarRUpKikn2R2QqxfI7AKKiwsbGBlWqVAEALFu2DJ6enli6dCn69OmDxMREAMDOnTszjY9QKpUA/uuqyE5ISAgeP36M2bNno0KFClAqlfD19c329LwhnJyc8PTp0yyXHTlyBPb29nB2dtbNVDGEm5sboqOjsX//fkRGRmLQoEH47rvvcPjw4WwLqdy89957qFy5MtauXYuBAwdiy5YtWLFihW55YmIi+vfvrzc2JF358uUNOoYp8puYmAgvLy9ERERkWlaqVCmD95NXT548gY2NTa7PJSJzY7FBJAELCwv873//Q2hoKLp3744aNWpAqVTi1q1b8PPzy3Kb2rVr4+eff4ZGo8nyQ/nYsWOYP38+goKCALwaiPjo0aM3irNu3bq4ePFilssqVqyoO3OQUfXq1XHs2DGEhIToxVajRg3dfZVKhTZt2qBNmzYYPHgwPDw88Pfff6NevXqZ9ieXyw2a5RIcHIyIiAiUK1cOFhYWaN26tW5ZvXr1cPHiRV2xlxeG5vf3339Hz5499e7XrVtXF8e6devg7OwMe3v7PMeSVxcuXNDFQlSQsBuFSCIff/wxLC0tMW/ePNjZ2WH48OEYNmwYfv75Z1y7dg1nzpzB3Llz8fPPPwMAhgwZgvj4eHTt2hWnTp3ClStXsGrVKkRHRwMA3nnnHaxatQqXLl3CH3/8geDg4Df+BhsYGIgTJ04YNaV1xIgRWLFiBRYsWIArV65g5syZ2Lx5M4YPHw7g1YDHpUuX4sKFC7h+/TpWr14NlUqFChUqZLk/d3d3REVFITY2NtuzLMCrYuPMmTOYPHkyPvroI90ZIQAYNWoUjh8/jiFDhuDcuXO4cuUKtm3bZtQAUUPzu2HDBixbtgz//vsvwsLC8Oeff+qOExwcDCcnJ7Rr1w5HjhxBTEwMDh06hC+++AJ37tzJ8rhbtmyBh4dHjrElJibi3Llzugu0xcTE4Ny5c5m6eI4cOYIWLVoY/JiJzCa/B40QFQWvD/pLN3XqVFGqVCmRmJgotFqtmDVrlqhWrZqQy+WiVKlSIjAwUBw+fFi3/vnz50WLFi2EtbW1sLOzE40bNxbXrl0TQghx5swZ4e3tLaysrMQ777wjNmzYkONgS0MGiGo0GlGmTBmxZ88eXdvrA0SzMn/+fFGpUiUhl8tF1apVxcqVK3XLtmzZIurXry/s7e2FjY2NeP/998X+/ft1y1+Pefv27aJKlSqiWLFiokKFCkKI7Adv+vj4CADiwIEDmZb9+eefIiAgQNja2gobGxtRu3ZtMXny5Gwfw+vHMDS/8+bNEwEBAUKpVAp3d3exbt06vf3ev39f9OzZUzg5OQmlUikqVaok+vbtK54/fy6EyPxcWb58ucjtrTj9/+T1W0hIiG6dO3fuCLlcLm7fvp3jvojyg0wIIfKpziGiAmDevHnYvn079u7dm9+h0BsYNWoUnj59isWLF+d3KESZcMwG0Vuuf//+ePbsGRISEowaCEoFi7OzM0JDQ/M7DKIs8cwGERERSYoDRImIiEhSLDaIiIhIUiw2iIiISFIsNoiIiEhSLDaIiIhIUiw2iIiISFIsNoiIiEhSLDaIiIhIUiw2iIiISFL/B4XLxu9p3wRcAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Precision-Recall curve calculation and visualization complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "their accuracy."
      ],
      "metadata": {
        "id": "epzSNZbuyEex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# --- 1. Create a dummy dataset for binary classification ---\n",
        "# This dataset will be used to compare different Logistic Regression solvers.\n",
        "np.random.seed(42) # for reproducibility\n",
        "N_SAMPLES = 700 # Slightly larger sample to potentially see solver differences\n",
        "\n",
        "# Numerical features\n",
        "feature_num1 = np.random.rand(N_SAMPLES) * 100\n",
        "feature_num2 = np.random.randn(N_SAMPLES) * 15 + 50\n",
        "feature_num3 = np.random.randint(0, 10, N_SAMPLES)\n",
        "\n",
        "# Categorical features\n",
        "gender = np.random.choice(['Male', 'Female'], N_SAMPLES, p=[0.5, 0.5])\n",
        "city = np.random.choice(['New York', 'London', 'Paris', 'Tokyo'], N_SAMPLES, p=[0.4, 0.3, 0.2, 0.1])\n",
        "\n",
        "# Binary Target variable:\n",
        "linear_combination = (0.5 * feature_num1 + 0.8 * feature_num2 - 0.3 * feature_num3 +\n",
        "                      (1 if gender[0] == 'Male' else 0) * 10 +\n",
        "                      (5 if city[0] == 'New York' else 0) +\n",
        "                      np.random.randn(N_SAMPLES) * 20)\n",
        "\n",
        "binary_target = (linear_combination > np.percentile(linear_combination, 50)).astype(int)\n",
        "\n",
        "# Introduce some missing values for demonstration of preprocessing\n",
        "missing_indices_num1 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.05), replace=False)\n",
        "missing_indices_num2 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.03), replace=False)\n",
        "feature_num1[missing_indices_num1] = np.nan\n",
        "feature_num2[missing_indices_num2] = np.nan\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Feature_Num1': feature_num1,\n",
        "    'Feature_Num2': feature_num2,\n",
        "    'Feature_Num3': feature_num3,\n",
        "    'Gender': gender,\n",
        "    'City': city,\n",
        "    'Target': binary_target # Our binary target\n",
        "})\n",
        "\n",
        "print(\"--- Dummy Dataset Head ---\")\n",
        "print(data.head())\n",
        "print(\"\\n--- Dummy Dataset Info ---\")\n",
        "data.info()\n",
        "print(\"\\n--- Target Variable Distribution ---\")\n",
        "print(data['Target'].value_counts())\n",
        "\n",
        "# --- 2. Separate features (X) and target (y) ---\n",
        "X = data.drop('Target', axis=1)\n",
        "y = data['Target'] # Use the binary target\n",
        "\n",
        "# --- 3. Identify numerical and categorical columns for preprocessing ---\n",
        "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# --- 4. Create a preprocessing pipeline ---\n",
        "# This pipeline will handle missing values, scaling, and one-hot encoding.\n",
        "# Scaling is particularly important for solvers like 'lbfgs' and 'saga'.\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')), # Impute missing numerical values with mean\n",
        "    ('scaler', StandardScaler())                  # Scale numerical features\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), # Impute missing categorical values with mode\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encode categorical features\n",
        "])\n",
        "\n",
        "# Create a preprocessor using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# --- 5. Split the dataset into training and testing sets ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "print(f\"Training target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Testing target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
        "\n",
        "# --- Practical Question 21: Train Logistic Regression with different solvers and compare accuracy ---\n",
        "print(\"\\n--- Question 21: Comparing Different Logistic Regression Solvers ---\")\n",
        "\n",
        "# Define the solvers to compare\n",
        "# Note: Each solver has specific penalty compatibility.\n",
        "# 'lbfgs': Supports L2 (default). Good general-purpose.\n",
        "# 'liblinear': Supports L1 and L2. Good for small datasets.\n",
        "# 'saga': Supports L1, L2, and Elastic Net. Good for large datasets.\n",
        "solvers_to_compare = ['lbfgs', 'liblinear', 'saga']\n",
        "solver_accuracies = {} # Dictionary to store accuracies for each solver\n",
        "\n",
        "for solver_name in solvers_to_compare:\n",
        "    print(f\"\\n--- Training with solver: '{solver_name}' ---\")\n",
        "\n",
        "    # Determine the penalty based on solver compatibility\n",
        "    # For simplicity, we'll use 'l2' for 'lbfgs', 'l1' for 'liblinear', and 'elasticnet' for 'saga'.\n",
        "    # In a real scenario, you might tune penalty along with the solver.\n",
        "    penalty = 'l2'\n",
        "    l1_ratio = None # Only relevant for 'elasticnet'\n",
        "\n",
        "    if solver_name == 'liblinear':\n",
        "        penalty = 'l1' # Choosing L1 for liblinear to demonstrate its capability\n",
        "    elif solver_name == 'saga':\n",
        "        penalty = 'elasticnet' # Choosing Elastic Net for saga\n",
        "        l1_ratio = 0.5 # A typical mix for elasticnet\n",
        "\n",
        "    try:\n",
        "        # Create a pipeline for the current solver\n",
        "        model_solver = Pipeline(steps=[\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('classifier', LogisticRegression(\n",
        "                solver=solver_name,\n",
        "                penalty=penalty,\n",
        "                l1_ratio=l1_ratio, # This parameter is ignored if penalty is not 'elasticnet'\n",
        "                random_state=42,\n",
        "                max_iter=1000 # Increased max_iter to help convergence for lbfgs/saga\n",
        "            ))\n",
        "        ])\n",
        "\n",
        "        # Train the model\n",
        "        print(f\"  Training model with penalty='{penalty}'...\")\n",
        "        model_solver.fit(X_train, y_train)\n",
        "        print(\"  Model training complete.\")\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred_solver = model_solver.predict(X_test)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy_solver = accuracy_score(y_test, y_pred_solver)\n",
        "        solver_accuracies[solver_name] = accuracy_solver\n",
        "        print(f\"  Accuracy with '{solver_name}' solver: {accuracy_solver:.4f}\")\n",
        "\n",
        "        # Optional: Print classification report for more details\n",
        "        print(\"\\n  Classification Report:\")\n",
        "        print(classification_report(y_test, y_pred_solver, target_names=['Class 0', 'Class 1']))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  Error training with '{solver_name}' solver: {e}\")\n",
        "        solver_accuracies[solver_name] = f\"Error: {e}\"\n",
        "\n",
        "# --- Summary Comparison ---\n",
        "print(\"\\n--- Summary of Solver Accuracies ---\")\n",
        "for solver, accuracy in solver_accuracies.items():\n",
        "    if isinstance(accuracy, float):\n",
        "        print(f\"Solver '{solver}': Accuracy = {accuracy:.4f}\")\n",
        "    else:\n",
        "        print(f\"Solver '{solver}': {accuracy}\")\n",
        "\n",
        "# Find the best performing solver\n",
        "best_solver = None\n",
        "max_accuracy = -1.0\n",
        "for solver, accuracy in solver_accuracies.items():\n",
        "    if isinstance(accuracy, float) and accuracy > max_accuracy:\n",
        "        max_accuracy = accuracy\n",
        "        best_solver = solver\n",
        "\n",
        "if best_solver:\n",
        "    print(f\"\\nConclusion: The best performing solver in this comparison is '{best_solver}' with an accuracy of {max_accuracy:.4f}.\")\n",
        "else:\n",
        "    print(\"\\nConclusion: Could not determine a best solver due to errors in training.\")\n",
        "\n",
        "print(\"\\nNote: The 'best' solver can depend on the specific dataset, its size, and the chosen regularization penalty.\")\n",
        "print(\"It's often recommended to try a few and select based on performance and computational efficiency.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHfXFHD5yFvj",
        "outputId": "1728b8b0-5500-42f6-ab47-4c69e8ff0c42"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dummy Dataset Head ---\n",
            "   Feature_Num1  Feature_Num2  Feature_Num3  Gender      City  Target\n",
            "0     37.454012     40.222458             1    Male  New York       1\n",
            "1     95.071431     50.710980             2  Female    London       0\n",
            "2     73.199394           NaN             6    Male     Paris       0\n",
            "3     59.865848     44.231667             4  Female  New York       0\n",
            "4           NaN     65.094392             1  Female  New York       0\n",
            "\n",
            "--- Dummy Dataset Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 700 entries, 0 to 699\n",
            "Data columns (total 6 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Feature_Num1  665 non-null    float64\n",
            " 1   Feature_Num2  679 non-null    float64\n",
            " 2   Feature_Num3  700 non-null    int64  \n",
            " 3   Gender        700 non-null    object \n",
            " 4   City          700 non-null    object \n",
            " 5   Target        700 non-null    int64  \n",
            "dtypes: float64(2), int64(2), object(2)\n",
            "memory usage: 32.9+ KB\n",
            "\n",
            "--- Target Variable Distribution ---\n",
            "Target\n",
            "1    350\n",
            "0    350\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training set size: 560 samples\n",
            "Testing set size: 140 samples\n",
            "Training target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "Testing target distribution:\n",
            "Target\n",
            "0    0.5\n",
            "1    0.5\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "--- Question 21: Comparing Different Logistic Regression Solvers ---\n",
            "\n",
            "--- Training with solver: 'lbfgs' ---\n",
            "  Training model with penalty='l2'...\n",
            "  Model training complete.\n",
            "  Accuracy with 'lbfgs' solver: 0.7286\n",
            "\n",
            "  Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.72      0.74      0.73        70\n",
            "     Class 1       0.74      0.71      0.72        70\n",
            "\n",
            "    accuracy                           0.73       140\n",
            "   macro avg       0.73      0.73      0.73       140\n",
            "weighted avg       0.73      0.73      0.73       140\n",
            "\n",
            "\n",
            "--- Training with solver: 'liblinear' ---\n",
            "  Training model with penalty='l1'...\n",
            "  Model training complete.\n",
            "  Accuracy with 'liblinear' solver: 0.7286\n",
            "\n",
            "  Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.72      0.74      0.73        70\n",
            "     Class 1       0.74      0.71      0.72        70\n",
            "\n",
            "    accuracy                           0.73       140\n",
            "   macro avg       0.73      0.73      0.73       140\n",
            "weighted avg       0.73      0.73      0.73       140\n",
            "\n",
            "\n",
            "--- Training with solver: 'saga' ---\n",
            "  Training model with penalty='elasticnet'...\n",
            "  Model training complete.\n",
            "  Accuracy with 'saga' solver: 0.7286\n",
            "\n",
            "  Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.72      0.74      0.73        70\n",
            "     Class 1       0.74      0.71      0.72        70\n",
            "\n",
            "    accuracy                           0.73       140\n",
            "   macro avg       0.73      0.73      0.73       140\n",
            "weighted avg       0.73      0.73      0.73       140\n",
            "\n",
            "\n",
            "--- Summary of Solver Accuracies ---\n",
            "Solver 'lbfgs': Accuracy = 0.7286\n",
            "Solver 'liblinear': Accuracy = 0.7286\n",
            "Solver 'saga': Accuracy = 0.7286\n",
            "\n",
            "Conclusion: The best performing solver in this comparison is 'lbfgs' with an accuracy of 0.7286.\n",
            "\n",
            "Note: The 'best' solver can depend on the specific dataset, its size, and the chosen regularization penalty.\n",
            "It's often recommended to try a few and select based on performance and computational efficiency.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "Correlation Coefficient (MCC)."
      ],
      "metadata": {
        "id": "qNcorKsFyQv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, matthews_corrcoef, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# --- 1. Create a dummy dataset for binary classification ---\n",
        "# This dataset will be used to demonstrate MCC evaluation.\n",
        "# We'll create a slightly imbalanced dataset to show MCC's strength.\n",
        "np.random.seed(42) # for reproducibility\n",
        "N_SAMPLES = 500\n",
        "\n",
        "# Numerical features\n",
        "feature_num1 = np.random.rand(N_SAMPLES) * 100\n",
        "feature_num2 = np.random.randn(N_SAMPLES) * 15 + 50\n",
        "feature_num3 = np.random.randint(0, 10, N_SAMPLES)\n",
        "\n",
        "# Categorical features\n",
        "gender = np.random.choice(['Male', 'Female'], N_SAMPLES, p=[0.5, 0.5])\n",
        "city = np.random.choice(['New York', 'London', 'Paris'], N_SAMPLES, p=[0.4, 0.3, 0.3])\n",
        "\n",
        "# Binary Target variable: Create a slightly imbalanced target\n",
        "# Let's make class 1 (positive class) about 30% of the samples.\n",
        "linear_combination = (0.5 * feature_num1 + 0.8 * feature_num2 - 0.3 * feature_num3 +\n",
        "                      (1 if gender[0] == 'Male' else 0) * 10 +\n",
        "                      (5 if city[0] == 'New York' else 0) +\n",
        "                      np.random.randn(N_SAMPLES) * 20)\n",
        "\n",
        "binary_target = (linear_combination > np.percentile(linear_combination, 70)).astype(int) # Roughly 30% positive class\n",
        "\n",
        "# Introduce some missing values for demonstration of preprocessing\n",
        "missing_indices_num1 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.05), replace=False)\n",
        "missing_indices_num2 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.03), replace=False)\n",
        "feature_num1[missing_indices_num1] = np.nan\n",
        "feature_num2[missing_indices_num2] = np.nan\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Feature_Num1': feature_num1,\n",
        "    'Feature_Num2': feature_num2,\n",
        "    'Feature_Num3': feature_num3,\n",
        "    'Gender': gender,\n",
        "    'City': city,\n",
        "    'Target': binary_target # Our binary target\n",
        "})\n",
        "\n",
        "print(\"--- Dummy Dataset Head ---\")\n",
        "print(data.head())\n",
        "print(\"\\n--- Dummy Dataset Info ---\")\n",
        "data.info()\n",
        "print(\"\\n--- Target Variable Distribution ---\")\n",
        "print(data['Target'].value_counts())\n",
        "print(f\"Class 0: {data['Target'].value_counts()[0]} samples ({data['Target'].value_counts(normalize=True)[0]:.2%})\")\n",
        "print(f\"Class 1: {data['Target'].value_counts()[1]} samples ({data['Target'].value_counts(normalize=True)[1]:.2%})\")\n",
        "\n",
        "\n",
        "# --- 2. Separate features (X) and target (y) ---\n",
        "X = data.drop('Target', axis=1)\n",
        "y = data['Target'] # Use the binary target\n",
        "\n",
        "# --- 3. Identify numerical and categorical columns for preprocessing ---\n",
        "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# --- 4. Create a preprocessing pipeline ---\n",
        "# This pipeline will handle missing values, scaling, and one-hot encoding.\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')), # Impute missing numerical values with mean\n",
        "    ('scaler', StandardScaler())                  # Scale numerical features\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), # Impute missing categorical values with mode\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encode categorical features\n",
        "])\n",
        "\n",
        "# Create a preprocessor using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# --- 5. Split the dataset into training and testing sets ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "print(f\"Training target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Testing target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
        "\n",
        "# --- Practical Question 22: Evaluate performance using Matthews Correlation Coefficient (MCC) ---\n",
        "print(\"\\n--- Question 22: Evaluate Performance using Matthews Correlation Coefficient (MCC) ---\")\n",
        "\n",
        "# Create a pipeline for the Logistic Regression model\n",
        "model_q22 = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                            ('classifier', LogisticRegression(random_state=42, max_iter=500))])\n",
        "\n",
        "# Train the model\n",
        "print(\"Training Logistic Regression model...\")\n",
        "model_q22.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_q22 = model_q22.predict(X_test)\n",
        "\n",
        "# --- Calculate and Print Evaluation Metrics ---\n",
        "\n",
        "# Accuracy (for general context)\n",
        "accuracy = accuracy_score(y_test, y_pred_q22)\n",
        "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Confusion Matrix (useful for understanding MCC)\n",
        "cm = confusion_matrix(y_test, y_pred_q22)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# --- Calculate Matthews Correlation Coefficient (MCC) ---\n",
        "# MCC is a single-value metric that takes into account all four values in the confusion matrix:\n",
        "# True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
        "# It's considered a balanced measure even if the classes are of very different sizes.\n",
        "# MCC ranges from -1 to +1:\n",
        "#   +1: Perfect prediction\n",
        "#    0: Average random prediction\n",
        "#   -1: Total disagreement between prediction and observation\n",
        "mcc_score = matthews_corrcoef(y_test, y_pred_q22)\n",
        "print(f\"\\nMatthews Correlation Coefficient (MCC): {mcc_score:.4f}\")\n",
        "\n",
        "# --- Interpretation of MCC ---\n",
        "print(\"\\n--- Interpretation of Matthews Correlation Coefficient (MCC) ---\")\n",
        "if mcc_score >= 0.75:\n",
        "    print(\"Interpretation: Very strong positive correlation.\")\n",
        "elif mcc_score >= 0.50:\n",
        "    print(\"Interpretation: Strong positive correlation.\")\n",
        "elif mcc_score >= 0.25:\n",
        "    print(\"Interpretation: Moderate positive correlation.\")\n",
        "elif mcc_score > 0:\n",
        "    print(\"Interpretation: Weak positive correlation.\")\n",
        "elif mcc_score == 0:\n",
        "    print(\"Interpretation: No correlation (random prediction).\")\n",
        "elif mcc_score < 0:\n",
        "    print(\"Interpretation: Inverse correlation (worse than random).\")\n",
        "\n",
        "print(\"\\nMCC is generally preferred over F1-score or accuracy for imbalanced datasets\")\n",
        "print(\"because it provides a more truthful and balanced evaluation of the classifier's performance.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s88HiDIPyR5l",
        "outputId": "0495dc28-19e4-4703-a930-3452f14d5e92"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dummy Dataset Head ---\n",
            "   Feature_Num1  Feature_Num2  Feature_Num3  Gender      City  Target\n",
            "0     37.454012     55.126340             0  Female  New York       0\n",
            "1     95.071431     78.142563             5  Female     Paris       1\n",
            "2     73.199394     64.256358             7    Male  New York       0\n",
            "3     59.865848     41.346445             2    Male  New York       0\n",
            "4     15.601864     36.523780             7  Female     Paris       0\n",
            "\n",
            "--- Dummy Dataset Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 6 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Feature_Num1  475 non-null    float64\n",
            " 1   Feature_Num2  485 non-null    float64\n",
            " 2   Feature_Num3  500 non-null    int64  \n",
            " 3   Gender        500 non-null    object \n",
            " 4   City          500 non-null    object \n",
            " 5   Target        500 non-null    int64  \n",
            "dtypes: float64(2), int64(2), object(2)\n",
            "memory usage: 23.6+ KB\n",
            "\n",
            "--- Target Variable Distribution ---\n",
            "Target\n",
            "0    350\n",
            "1    150\n",
            "Name: count, dtype: int64\n",
            "Class 0: 350 samples (70.00%)\n",
            "Class 1: 150 samples (30.00%)\n",
            "\n",
            "Training set size: 400 samples\n",
            "Testing set size: 100 samples\n",
            "Training target distribution:\n",
            "Target\n",
            "0    0.7\n",
            "1    0.3\n",
            "Name: proportion, dtype: float64\n",
            "Testing target distribution:\n",
            "Target\n",
            "0    0.7\n",
            "1    0.3\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "--- Question 22: Evaluate Performance using Matthews Correlation Coefficient (MCC) ---\n",
            "Training Logistic Regression model...\n",
            "Model training complete.\n",
            "\n",
            "Model Accuracy: 0.7400\n",
            "\n",
            "Confusion Matrix:\n",
            "[[60 10]\n",
            " [16 14]]\n",
            "\n",
            "Matthews Correlation Coefficient (MCC): 0.3474\n",
            "\n",
            "--- Interpretation of Matthews Correlation Coefficient (MCC) ---\n",
            "Interpretation: Moderate positive correlation.\n",
            "\n",
            "MCC is generally preferred over F1-score or accuracy for imbalanced datasets\n",
            "because it provides a more truthful and balanced evaluation of the classifier's performance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "accuracy to see the impact of feature scaling."
      ],
      "metadata": {
        "id": "JriehxPJyfE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# --- 1. Create a dummy dataset with varying scales for demonstration ---\n",
        "# This dataset is designed to clearly show the impact of feature scaling.\n",
        "# We'll create numerical features with distinct value ranges.\n",
        "np.random.seed(42) # for reproducibility\n",
        "N_SAMPLES = 500\n",
        "\n",
        "# Numerical features with different scales\n",
        "feature_small_scale = np.random.rand(N_SAMPLES) * 10 # Values from 0-10\n",
        "feature_medium_scale = np.random.normal(50, 10, N_SAMPLES) # Values around 50 +/- 10\n",
        "feature_large_scale = np.random.randint(1000, 5000, N_SAMPLES) # Values from 1000-5000\n",
        "\n",
        "# Categorical features (will be one-hot encoded)\n",
        "gender = np.random.choice(['Male', 'Female'], N_SAMPLES, p=[0.5, 0.5])\n",
        "city = np.random.choice(['New York', 'London', 'Paris'], N_SAMPLES, p=[0.4, 0.3, 0.3])\n",
        "\n",
        "# Binary Target variable: A relationship that might benefit from scaling\n",
        "# The coefficients here are chosen to give a meaningful target, and the noise\n",
        "# is added to ensure it's not perfectly separable.\n",
        "linear_combination = (0.2 * feature_small_scale + 0.05 * feature_medium_scale +\n",
        "                      0.001 * feature_large_scale +\n",
        "                      (1 if gender[0] == 'Male' else 0) * 0.5 +\n",
        "                      (1 if city[0] == 'New York' else 0) * 0.8 +\n",
        "                      np.random.randn(N_SAMPLES) * 0.5) # Add some noise\n",
        "\n",
        "binary_target = (linear_combination > np.median(linear_combination)).astype(int)\n",
        "\n",
        "# Introduce some missing values for demonstration of preprocessing\n",
        "missing_indices_small = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.02), replace=False)\n",
        "missing_indices_medium = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.03), replace=False)\n",
        "feature_small_scale[missing_indices_small] = np.nan\n",
        "feature_medium_scale[missing_indices_medium] = np.nan\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Feature_Small': feature_small_scale,\n",
        "    'Feature_Medium': feature_medium_scale,\n",
        "    'Feature_Large': feature_large_scale,\n",
        "    'Gender': gender,\n",
        "    'City': city,\n",
        "    'Target': binary_target # Our binary target\n",
        "})\n",
        "\n",
        "print(\"--- Dummy Dataset Head (with varying scales) ---\")\n",
        "print(data.head())\n",
        "print(\"\\n--- Dummy Dataset Info ---\")\n",
        "data.info()\n",
        "print(\"\\n--- Target Variable Distribution ---\")\n",
        "print(data['Target'].value_counts())\n",
        "\n",
        "# --- 2. Separate features (X) and target (y) ---\n",
        "X = data.drop('Target', axis=1)\n",
        "y = data['Target']\n",
        "\n",
        "# --- 3. Identify numerical and categorical columns for preprocessing ---\n",
        "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# --- 4. Define Preprocessing Pipelines (With and Without Numerical Scaling) ---\n",
        "\n",
        "# Preprocessor for models *WITHOUT* explicit numerical scaling\n",
        "# It still handles imputation for numerical features and encoding for categorical.\n",
        "# This simulates \"raw\" numerical data, though missing values are still handled.\n",
        "numerical_transformer_no_scale = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')) # Only impute, no scaling\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor_no_scale = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer_no_scale, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ],\n",
        "    remainder='passthrough' # Keep any other columns not specified\n",
        ")\n",
        "\n",
        "# Preprocessor for models *WITH* numerical scaling (Standardization)\n",
        "# This uses StandardScaler to transform numerical features to have mean 0 and variance 1.\n",
        "numerical_transformer_with_scale = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler()) # Impute then standardize\n",
        "])\n",
        "\n",
        "preprocessor_with_scale = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer_with_scale, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# --- 5. Split the dataset into training and testing sets ---\n",
        "# Use stratify=y to ensure class distribution is preserved in splits.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "print(f\"Training target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Testing target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
        "\n",
        "\n",
        "# --- Practical Question 23: Train Logistic Regression on both raw and standardized data.\n",
        "# Compare their accuracy to see the impact of feature scaling. (Combines with Q15) ---\n",
        "print(\"\\n--- Question 23 (and 15): Feature Scaling (Standardization) Impact Comparison ---\")\n",
        "\n",
        "# --- Model 1: Logistic Regression WITHOUT Feature Scaling (on numerical features) ---\n",
        "print(\"\\n--- Training Model WITHOUT Feature Scaling (Numerical Features) ---\")\n",
        "model_no_scaling = Pipeline(steps=[('preprocessor', preprocessor_no_scale),\n",
        "                                   ('classifier', LogisticRegression(random_state=42, max_iter=1000))])\n",
        "\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "print(f\"Accuracy WITHOUT Feature Scaling: {accuracy_no_scaling:.4f}\")\n",
        "print(\"\\nClassification Report (WITHOUT Scaling):\")\n",
        "print(classification_report(y_test, y_pred_no_scaling))\n",
        "\n",
        "\n",
        "# --- Model 2: Logistic Regression WITH Feature Scaling (Standardization) ---\n",
        "print(\"\\n--- Training Model WITH Feature Scaling (Standardization) ---\")\n",
        "model_with_scaling = Pipeline(steps=[('preprocessor', preprocessor_with_scale),\n",
        "                                     ('classifier', LogisticRegression(random_state=42, max_iter=1000))])\n",
        "\n",
        "model_with_scaling.fit(X_train, y_train)\n",
        "y_pred_with_scaling = model_with_scaling.predict(X_test)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "\n",
        "print(f\"Accuracy WITH Feature Scaling (Standardization): {accuracy_with_scaling:.4f}\")\n",
        "print(\"\\nClassification Report (WITH Scaling):\")\n",
        "print(classification_report(y_test, y_pred_with_scaling))\n",
        "\n",
        "\n",
        "# --- Comparison of Results ---\n",
        "print(\"\\n--- Comparison of Model Accuracies ---\")\n",
        "print(f\"Accuracy WITHOUT scaling: {accuracy_no_scaling:.4f}\")\n",
        "print(f\"Accuracy WITH scaling:    {accuracy_with_scaling:.4f}\")\n",
        "\n",
        "if accuracy_with_scaling > accuracy_no_scaling:\n",
        "    print(\"\\nObservation: Feature scaling (Standardization) IMPROVED the model's accuracy.\")\n",
        "    print(\"This often happens when features have vastly different scales, as Logistic Regression's\")\n",
        "    print(\"optimization algorithm (like gradient descent) converges more efficiently on scaled data.\")\n",
        "elif accuracy_with_scaling < accuracy_no_scaling:\n",
        "    print(\"\\nObservation: Feature scaling (Standardization) slightly DECREASED the model's accuracy.\")\n",
        "    print(\"This can happen if the original scales were already optimal or if the dataset is very simple,\")\n",
        "    print(\"or if the scaling introduced some noise for this particular random split/dataset.\")\n",
        "else:\n",
        "    print(\"\\nObservation: Feature scaling (Standardization) had NO SIGNIFICANT IMPACT on accuracy.\")\n",
        "    print(\"This might occur if the features were already on a similar scale or if the model is robust to scale differences.\")\n",
        "\n",
        "print(\"\\nFeature scaling is generally recommended for Logistic Regression, especially when using solvers that are sensitive to feature scales (e.g., 'lbfgs', 'saga').\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KB_FnRtygZ4",
        "outputId": "2ae1869b-4231-457a-ecd2-a35df2cbd7e4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dummy Dataset Head (with varying scales) ---\n",
            "   Feature_Small  Feature_Medium  Feature_Large  Gender      City  Target\n",
            "0       3.745401       53.417560           2772    Male    London       0\n",
            "1       9.507143       68.761708           2312    Male  New York       1\n",
            "2       7.319939       59.504238           2285    Male     Paris       0\n",
            "3       5.986585       44.230963           1215  Female  New York       0\n",
            "4       1.560186       41.015853           4218  Female     Paris       1\n",
            "\n",
            "--- Dummy Dataset Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 6 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   Feature_Small   490 non-null    float64\n",
            " 1   Feature_Medium  485 non-null    float64\n",
            " 2   Feature_Large   500 non-null    int64  \n",
            " 3   Gender          500 non-null    object \n",
            " 4   City            500 non-null    object \n",
            " 5   Target          500 non-null    int64  \n",
            "dtypes: float64(2), int64(2), object(2)\n",
            "memory usage: 23.6+ KB\n",
            "\n",
            "--- Target Variable Distribution ---\n",
            "Target\n",
            "0    250\n",
            "1    250\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training set size: 400 samples\n",
            "Testing set size: 100 samples\n",
            "Training target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "Testing target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "--- Question 23 (and 15): Feature Scaling (Standardization) Impact Comparison ---\n",
            "\n",
            "--- Training Model WITHOUT Feature Scaling (Numerical Features) ---\n",
            "Accuracy WITHOUT Feature Scaling: 0.8900\n",
            "\n",
            "Classification Report (WITHOUT Scaling):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.88      0.89        50\n",
            "           1       0.88      0.90      0.89        50\n",
            "\n",
            "    accuracy                           0.89       100\n",
            "   macro avg       0.89      0.89      0.89       100\n",
            "weighted avg       0.89      0.89      0.89       100\n",
            "\n",
            "\n",
            "--- Training Model WITH Feature Scaling (Standardization) ---\n",
            "Accuracy WITH Feature Scaling (Standardization): 0.8900\n",
            "\n",
            "Classification Report (WITH Scaling):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.88      0.89        50\n",
            "           1       0.88      0.90      0.89        50\n",
            "\n",
            "    accuracy                           0.89       100\n",
            "   macro avg       0.89      0.89      0.89       100\n",
            "weighted avg       0.89      0.89      0.89       100\n",
            "\n",
            "\n",
            "--- Comparison of Model Accuracies ---\n",
            "Accuracy WITHOUT scaling: 0.8900\n",
            "Accuracy WITH scaling:    0.8900\n",
            "\n",
            "Observation: Feature scaling (Standardization) had NO SIGNIFICANT IMPACT on accuracy.\n",
            "This might occur if the features were already on a similar scale or if the model is robust to scale differences.\n",
            "\n",
            "Feature scaling is generally recommended for Logistic Regression, especially when using solvers that are sensitive to feature scales (e.g., 'lbfgs', 'saga').\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
        "cross-validation."
      ],
      "metadata": {
        "id": "v-UPYGKTywhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# --- 1. Create a dummy dataset for binary classification ---\n",
        "# This dataset will be used to find the optimal 'C' parameter.\n",
        "np.random.seed(42) # for reproducibility\n",
        "N_SAMPLES = 500\n",
        "\n",
        "# Numerical features\n",
        "feature_num1 = np.random.rand(N_SAMPLES) * 100\n",
        "feature_num2 = np.random.randn(N_SAMPLES) * 15 + 50\n",
        "feature_num3 = np.random.randint(0, 10, N_SAMPLES)\n",
        "\n",
        "# Categorical features\n",
        "gender = np.random.choice(['Male', 'Female'], N_SAMPLES, p=[0.5, 0.5])\n",
        "city = np.random.choice(['New York', 'London', 'Paris'], N_SAMPLES, p=[0.4, 0.3, 0.3])\n",
        "\n",
        "# Binary Target variable:\n",
        "linear_combination = (0.5 * feature_num1 + 0.8 * feature_num2 - 0.3 * feature_num3 +\n",
        "                      (1 if gender[0] == 'Male' else 0) * 10 +\n",
        "                      (5 if city[0] == 'New York' else 0) +\n",
        "                      np.random.randn(N_SAMPLES) * 20)\n",
        "\n",
        "binary_target = (linear_combination > np.percentile(linear_combination, 50)).astype(int)\n",
        "\n",
        "# Introduce some missing values for demonstration of preprocessing\n",
        "missing_indices_num1 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.05), replace=False)\n",
        "missing_indices_num2 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.03), replace=False)\n",
        "feature_num1[missing_indices_num1] = np.nan\n",
        "feature_num2[missing_indices_num2] = np.nan\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Feature_Num1': feature_num1,\n",
        "    'Feature_Num2': feature_num2,\n",
        "    'Feature_Num3': feature_num3,\n",
        "    'Gender': gender,\n",
        "    'City': city,\n",
        "    'Target': binary_target # Our binary target\n",
        "})\n",
        "\n",
        "print(\"--- Dummy Dataset Head ---\")\n",
        "print(data.head())\n",
        "print(\"\\n--- Dummy Dataset Info ---\")\n",
        "data.info()\n",
        "print(\"\\n--- Target Variable Distribution ---\")\n",
        "print(data['Target'].value_counts())\n",
        "\n",
        "# --- 2. Separate features (X) and target (y) ---\n",
        "X = data.drop('Target', axis=1)\n",
        "y = data['Target'] # Use the binary target\n",
        "\n",
        "# --- 3. Identify numerical and categorical columns for preprocessing ---\n",
        "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# --- 4. Create a preprocessing pipeline ---\n",
        "# This pipeline will handle missing values, scaling, and one-hot encoding.\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')), # Impute missing numerical values with mean\n",
        "    ('scaler', StandardScaler())                  # Scale numerical features\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), # Impute missing categorical values with mode\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encode categorical features\n",
        "])\n",
        "\n",
        "# Create a preprocessor using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# --- 5. Split the dataset into training and testing sets ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "print(f\"Training target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Testing target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
        "\n",
        "# --- Practical Question 24: Find the optimal C (regularization strength) using cross-validation ---\n",
        "print(\"\\n--- Question 24: Finding Optimal C (Regularization Strength) using Cross-Validation ---\")\n",
        "\n",
        "# Create a pipeline for the Logistic Regression model.\n",
        "# We will tune the 'C' parameter of the 'classifier' step.\n",
        "pipeline_c_tuning = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                     ('classifier', LogisticRegression(penalty='l2', solver='lbfgs', random_state=42, max_iter=1000))])\n",
        "# Using 'lbfgs' solver as it's robust and default for L2 penalty.\n",
        "# max_iter increased to ensure convergence.\n",
        "\n",
        "# Define the parameter grid for 'C'.\n",
        "# 'C' is the inverse of regularization strength. Smaller C means stronger regularization.\n",
        "# We'll search a range of C values, often on a logarithmic scale.\n",
        "param_grid_c = {\n",
        "    'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV.\n",
        "# - estimator: The pipeline to tune.\n",
        "# - param_grid: The grid of C values to search.\n",
        "# - cv: Cross-validation strategy. StratifiedKFold is crucial for classification.\n",
        "# - scoring: Metric to optimize (e.g., 'accuracy').\n",
        "# - n_jobs: Number of CPU cores to use (-1 for all available).\n",
        "# - verbose: Controls the verbosity of the output.\n",
        "grid_search_c = GridSearchCV(\n",
        "    estimator=pipeline_c_tuning,\n",
        "    param_grid=param_grid_c,\n",
        "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), # 5-fold stratified cross-validation\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=2 # Set to 2 for more detailed output during search\n",
        ")\n",
        "\n",
        "# Fit GridSearchCV to the training data. This performs the cross-validation search.\n",
        "print(\"\\nStarting GridSearchCV to find optimal C...\")\n",
        "grid_search_c.fit(X_train, y_train)\n",
        "print(\"GridSearchCV for C complete.\")\n",
        "\n",
        "# Print the optimal C value found\n",
        "optimal_c = grid_search_c.best_params_['classifier__C']\n",
        "print(f\"\\nOptimal C (Regularization Strength) found: {optimal_c}\")\n",
        "\n",
        "# Print the best cross-validation accuracy achieved with the optimal C\n",
        "best_cv_accuracy = grid_search_c.best_score_\n",
        "print(f\"Best cross-validation accuracy with optimal C: {best_cv_accuracy:.4f}\")\n",
        "\n",
        "# Evaluate the model with the optimal C on the unseen test set\n",
        "y_pred_optimal_c = grid_search_c.best_estimator_.predict(X_test)\n",
        "test_accuracy_optimal_c = accuracy_score(y_test, y_pred_optimal_c)\n",
        "print(f\"Test accuracy with the model using optimal C: {test_accuracy_optimal_c:.4f}\")\n",
        "\n",
        "# Optional: Print the full results of the grid search for C\n",
        "# print(\"\\n--- Full GridSearchCV Results for C ---\")\n",
        "# print(pd.DataFrame(grid_search_c.cv_results_)[['param_classifier__C', 'mean_test_score', 'std_test_score', 'rank_test_score']])\n",
        "\n",
        "print(\"\\nFinding optimal C using cross-validation complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mx3txCjPyzCe",
        "outputId": "eaf50688-742c-4fc8-f86a-65c114b4add3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dummy Dataset Head ---\n",
            "   Feature_Num1  Feature_Num2  Feature_Num3  Gender      City  Target\n",
            "0     37.454012     55.126340             0  Female  New York       0\n",
            "1     95.071431     78.142563             5  Female     Paris       1\n",
            "2     73.199394     64.256358             7    Male  New York       0\n",
            "3     59.865848     41.346445             2    Male  New York       0\n",
            "4     15.601864     36.523780             7  Female     Paris       0\n",
            "\n",
            "--- Dummy Dataset Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 6 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Feature_Num1  475 non-null    float64\n",
            " 1   Feature_Num2  485 non-null    float64\n",
            " 2   Feature_Num3  500 non-null    int64  \n",
            " 3   Gender        500 non-null    object \n",
            " 4   City          500 non-null    object \n",
            " 5   Target        500 non-null    int64  \n",
            "dtypes: float64(2), int64(2), object(2)\n",
            "memory usage: 23.6+ KB\n",
            "\n",
            "--- Target Variable Distribution ---\n",
            "Target\n",
            "0    250\n",
            "1    250\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training set size: 400 samples\n",
            "Testing set size: 100 samples\n",
            "Training target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "Testing target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "--- Question 24: Finding Optimal C (Regularization Strength) using Cross-Validation ---\n",
            "\n",
            "Starting GridSearchCV to find optimal C...\n",
            "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
            "GridSearchCV for C complete.\n",
            "\n",
            "Optimal C (Regularization Strength) found: 10\n",
            "Best cross-validation accuracy with optimal C: 0.7200\n",
            "Test accuracy with the model using optimal C: 0.7500\n",
            "\n",
            "Finding optimal C using cross-validation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "make predictions."
      ],
      "metadata": {
        "id": "64NJ3yAZy7oh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib # Import joblib for saving and loading models\n",
        "import os     # For removing the saved file\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# --- 1. Create a dummy dataset for binary classification ---\n",
        "# This dataset will be used to train and then save/load a model.\n",
        "np.random.seed(42) # for reproducibility\n",
        "N_SAMPLES = 500\n",
        "\n",
        "# Numerical features\n",
        "feature_num1 = np.random.rand(N_SAMPLES) * 100\n",
        "feature_num2 = np.random.randn(N_SAMPLES) * 15 + 50\n",
        "feature_num3 = np.random.randint(0, 10, N_SAMPLES)\n",
        "\n",
        "# Categorical features\n",
        "gender = np.random.choice(['Male', 'Female'], N_SAMPLES, p=[0.5, 0.5])\n",
        "city = np.random.choice(['New York', 'London', 'Paris'], N_SAMPLES, p=[0.4, 0.3, 0.3])\n",
        "\n",
        "# Binary Target variable:\n",
        "linear_combination = (0.5 * feature_num1 + 0.8 * feature_num2 - 0.3 * feature_num3 +\n",
        "                      (1 if gender[0] == 'Male' else 0) * 10 +\n",
        "                      (5 if city[0] == 'New York' else 0) +\n",
        "                      np.random.randn(N_SAMPLES) * 20)\n",
        "\n",
        "binary_target = (linear_combination > np.percentile(linear_combination, 50)).astype(int)\n",
        "\n",
        "# Introduce some missing values for demonstration of preprocessing\n",
        "missing_indices_num1 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.05), replace=False)\n",
        "missing_indices_num2 = np.random.choice(N_SAMPLES, int(N_SAMPLES * 0.03), replace=False)\n",
        "feature_num1[missing_indices_num1] = np.nan\n",
        "feature_num2[missing_indices_num2] = np.nan\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Feature_Num1': feature_num1,\n",
        "    'Feature_Num2': feature_num2,\n",
        "    'Feature_Num3': feature_num3,\n",
        "    'Gender': gender,\n",
        "    'City': city,\n",
        "    'Target': binary_target # Our binary target\n",
        "})\n",
        "\n",
        "print(\"--- Dummy Dataset Head ---\")\n",
        "print(data.head())\n",
        "print(\"\\n--- Dummy Dataset Info ---\")\n",
        "data.info()\n",
        "print(\"\\n--- Target Variable Distribution ---\")\n",
        "print(data['Target'].value_counts())\n",
        "\n",
        "# --- 2. Separate features (X) and target (y) ---\n",
        "X = data.drop('Target', axis=1)\n",
        "y = data['Target'] # Use the binary target\n",
        "\n",
        "# --- 3. Identify numerical and categorical columns for preprocessing ---\n",
        "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# --- 4. Create a preprocessing pipeline ---\n",
        "# This pipeline will handle missing values, scaling, and one-hot encoding.\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')), # Impute missing numerical values with mean\n",
        "    ('scaler', StandardScaler())                  # Scale numerical features\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), # Impute missing categorical values with mode\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encode categorical features\n",
        "])\n",
        "\n",
        "# Create a preprocessor using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# --- 5. Split the dataset into training and testing sets ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "print(f\"Training target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Testing target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
        "\n",
        "# --- Practical Question 25: Train, Save, and Load Logistic Regression Model using joblib ---\n",
        "print(\"\\n--- Question 25: Save and Load Logistic Regression Model with Joblib ---\")\n",
        "\n",
        "# --- Step 1: Train the Logistic Regression Model ---\n",
        "# Create a pipeline for the Logistic Regression model.\n",
        "# It's good practice to save the entire pipeline, as it includes the preprocessor\n",
        "# which is essential for transforming new data consistently.\n",
        "trained_model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                         ('classifier', LogisticRegression(random_state=42, max_iter=500))])\n",
        "\n",
        "print(\"\\nTraining Logistic Regression model...\")\n",
        "trained_model_pipeline.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# Make initial predictions with the trained model to compare later\n",
        "y_pred_original = trained_model_pipeline.predict(X_test)\n",
        "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
        "print(f\"Accuracy of the originally trained model: {accuracy_original:.4f}\")\n",
        "\n",
        "# --- Step 2: Save the trained model using joblib ---\n",
        "# Define the filename for the saved model.\n",
        "model_filename = 'logistic_regression_pipeline.joblib'\n",
        "\n",
        "# Use joblib.dump() to save the entire pipeline object to a file.\n",
        "try:\n",
        "    joblib.dump(trained_model_pipeline, model_filename)\n",
        "    print(f\"\\nModel successfully saved to '{model_filename}'\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving model: {e}\")\n",
        "\n",
        "# --- Step 3: Load the model back using joblib ---\n",
        "# Use joblib.load() to load the model object from the file.\n",
        "loaded_model_pipeline = None\n",
        "try:\n",
        "    loaded_model_pipeline = joblib.load(model_filename)\n",
        "    print(f\"Model successfully loaded from '{model_filename}'\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "\n",
        "# --- Step 4: Make predictions using the loaded model ---\n",
        "if loaded_model_pipeline:\n",
        "    print(\"\\nMaking predictions using the loaded model...\")\n",
        "    y_pred_loaded = loaded_model_pipeline.predict(X_test)\n",
        "\n",
        "    # Evaluate the accuracy of the loaded model\n",
        "    accuracy_loaded = accuracy_score(y_test, y_pred_loaded)\n",
        "    print(f\"Accuracy of the loaded model: {accuracy_loaded:.4f}\")\n",
        "\n",
        "    # --- Compare predictions from original and loaded models ---\n",
        "    # This confirms that the loaded model is identical and works as expected.\n",
        "    if np.array_equal(y_pred_original, y_pred_loaded):\n",
        "        print(\"\\nVerification: Predictions from loaded model are IDENTICAL to the original model's predictions.\")\n",
        "    else:\n",
        "        print(\"\\nVerification: Predictions from loaded model DIFFER from the original model's predictions. (This might indicate an issue)\")\n",
        "\n",
        "    print(\"\\nClassification Report (from Loaded Model):\")\n",
        "    print(classification_report(y_test, y_pred_loaded))\n",
        "else:\n",
        "    print(\"\\nCannot make predictions as the model failed to load.\")\n",
        "\n",
        "# --- Optional: Clean up the saved model file ---\n",
        "try:\n",
        "    if os.path.exists(model_filename):\n",
        "        os.remove(model_filename)\n",
        "        print(f\"\\nCleaned up: Removed saved model file '{model_filename}'.\")\n",
        "except OSError as e:\n",
        "    print(f\"Error removing file '{model_filename}': {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJLjSnN1y8vO",
        "outputId": "36e0919a-8c06-49d7-fbb2-88f6a3e58231"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dummy Dataset Head ---\n",
            "   Feature_Num1  Feature_Num2  Feature_Num3  Gender      City  Target\n",
            "0     37.454012     55.126340             0  Female  New York       0\n",
            "1     95.071431     78.142563             5  Female     Paris       1\n",
            "2     73.199394     64.256358             7    Male  New York       0\n",
            "3     59.865848     41.346445             2    Male  New York       0\n",
            "4     15.601864     36.523780             7  Female     Paris       0\n",
            "\n",
            "--- Dummy Dataset Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 6 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Feature_Num1  475 non-null    float64\n",
            " 1   Feature_Num2  485 non-null    float64\n",
            " 2   Feature_Num3  500 non-null    int64  \n",
            " 3   Gender        500 non-null    object \n",
            " 4   City          500 non-null    object \n",
            " 5   Target        500 non-null    int64  \n",
            "dtypes: float64(2), int64(2), object(2)\n",
            "memory usage: 23.6+ KB\n",
            "\n",
            "--- Target Variable Distribution ---\n",
            "Target\n",
            "0    250\n",
            "1    250\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training set size: 400 samples\n",
            "Testing set size: 100 samples\n",
            "Training target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "Testing target distribution:\n",
            "Target\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "--- Question 25: Save and Load Logistic Regression Model with Joblib ---\n",
            "\n",
            "Training Logistic Regression model...\n",
            "Model training complete.\n",
            "Accuracy of the originally trained model: 0.7500\n",
            "\n",
            "Model successfully saved to 'logistic_regression_pipeline.joblib'\n",
            "Model successfully loaded from 'logistic_regression_pipeline.joblib'\n",
            "\n",
            "Making predictions using the loaded model...\n",
            "Accuracy of the loaded model: 0.7500\n",
            "\n",
            "Verification: Predictions from loaded model are IDENTICAL to the original model's predictions.\n",
            "\n",
            "Classification Report (from Loaded Model):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.72      0.74        50\n",
            "           1       0.74      0.78      0.76        50\n",
            "\n",
            "    accuracy                           0.75       100\n",
            "   macro avg       0.75      0.75      0.75       100\n",
            "weighted avg       0.75      0.75      0.75       100\n",
            "\n",
            "\n",
            "Cleaned up: Removed saved model file 'logistic_regression_pipeline.joblib'.\n"
          ]
        }
      ]
    }
  ]
}